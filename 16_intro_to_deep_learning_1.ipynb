{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db8d391-3fcd-4bdd-8ca6-fda22de08bc9",
   "metadata": {},
   "source": [
    "# Module 5 Deep Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ac414-21e0-40ae-94a0-519fe73fdaf2",
   "metadata": {},
   "source": [
    "## Introduction to deep learning\n",
    " \n",
    "Deep learning is a subset of machine learning, where the fundamental model structure is a network of inputs, hidden layers, and outputs, as shown below.\n",
    "\n",
    "![](images/neural_network01.jpg)\n",
    "\n",
    "A network can have one or more hidden layers. \n",
    "\n",
    "The original motivation of deep learning was to create models inspired by how the human brain learns: **through interconnected cells called neurons**.\n",
    "This is the reason why we continue to call deep learning models as **neural networks**. There are a number of libraries and frameworks for deep learning in python.\n",
    "\n",
    "These are:\n",
    "\n",
    "*   `Tensorflow`: an open-source library for numerical computation and machine learning.\n",
    "*   `Keras`: a high-level neural network API that runs on top of TensorFlow, mxnet, or Theano.\n",
    "*   `Apache mxnet`: a scalable and flexible deep learning framework that supports multiple languages and devices.\n",
    "*   `Caffe`: a fast and modular framework for deep learning, mainly used for computer vision.\n",
    "*   `Theano`: a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays.\n",
    "*   `sklearn-theano`: a library that wraps some of the scikit-learn API with Theano expressions.\n",
    "*   `Chainer`: a Python-based framework for deep learning that supports dynamic computation graphs.\n",
    "*   `Lasagne`: a lightweight library that builds on top of Theano to provide neural network layers and utilities.\n",
    "*   `PyTorch`: a Python package that provides tensor computation and dynamic neural networks.\n",
    "\n",
    "In this course we'll focus on PyTorch. It is being used by deep learning engineers in industry and it is a favored tool amongst researchers. \n",
    "Many deep learning papers are published using PyTorch. PyTorch is designed to be intuitive and user-friendly, sharing a lot of common ground with the Python library NumPy.\n",
    "\n",
    "**Comparing machine learning and deep learning**\n",
    "    \n",
    "*  Machine learning and deep learning can both be described as artificial intelligence. \n",
    "*  In both cases, a computer system learns from data to make intelligent decisions. \n",
    "*  However, deep learning diverges from machine learning in many different aspects.\n",
    "\n",
    "![](images/ML_vs_DL.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7977577-e76e-4107-bd2e-04bb92cff50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f5a4c-ab8f-4956-8e09-549389b25ab0",
   "metadata": {},
   "source": [
    "## 5.1 Tensors in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c6f9e7-2c24-4a57-9d98-e779168bc913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# tensors in pytorch\n",
    "array = [[1, 2, 3], [4, 5, 6]]\n",
    "tensor_input = torch.tensor(array)\n",
    "print(tensor_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53ca839-5ebc-427b-a5c2-d38642108e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_input.data: \n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor_input.shape:  torch.Size([2, 3])\n",
      "tensor_input.device:  cpu\n",
      "tensor.dtype:  torch.int64\n"
     ]
    }
   ],
   "source": [
    "# some of the tensor attributes and functions\n",
    "print(\"tensor_input.data: \\n\", tensor_input.data)\n",
    "print(\"tensor_input.shape: \", tensor_input.shape)\n",
    "print(\"tensor_input.device: \",tensor_input.device)\n",
    "print(\"tensor.dtype: \",tensor_input.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ddd5900-7f5a-4cce-bf9c-8b9be78ec5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check that your GPU is working in Pytorch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f62cfd-86fa-4a30-8e6a-687b45602607",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6529885e-3022-4483-895b-e306338fe7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "transpose = tensor_input.t().to(dev)\n",
    "print(transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171df93e-c6e5-48ca-9682-cd09a213db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(tensor_input.device)\n",
    "print(transpose.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7088e87d-a42f-4bdb-8d5a-1f90db239339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# creating a tensor of rank 3\n",
    "torch.manual_seed(0)\n",
    "tensor3R = torch.randn(3,128,128, device = \"cuda\")\n",
    "print(tensor3R.device)\n",
    "print(tensor3R.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b80775-964d-48ed-9692-21e33a0284ab",
   "metadata": {},
   "source": [
    "### 5.1.1 Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c438a0c-655e-4275-a9e6-2124bf8f57ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor addition: \n",
      " tensor([[3, 3],\n",
      "        [5, 5]]) \n",
      "\n",
      "tensor subtraction: \n",
      " tensor([[-1, -1],\n",
      "        [-1, -1]])\n"
     ]
    }
   ],
   "source": [
    "# compatible shapes\n",
    "a = torch.tensor([[1, 1], [2, 2]])\n",
    "b = torch.tensor([[2, 2], [3, 3]])\n",
    "print(\"tensor addition: \\n\",a + b, \"\\n\")    \n",
    "print(\"tensor subtraction: \\n\",a - b)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "366d0d37-91c1-46e0-9baf-eb1773a0d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incompatible shapes\n",
    "a = torch.tensor([[1, 1], [2, 2]])\n",
    "c = torch.tensor([[2, 2, 4], [3, 3, 5]])\n",
    "\n",
    "# print(a + c)\n",
    "## RuntimeError                              Traceback (most recent call last)\n",
    "## Cell In[10], line 5\n",
    "##      2 a = torch.tensor([[1, 1], [2, 2]])\n",
    "##      3 c = torch.tensor([[2, 2, 4], [3, 3, 5]])\n",
    "##----> 5 print(a + c)\n",
    "\n",
    "## RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e49d1c-10d2-43b3-8ad5-ee444f9d61f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "d = torch.tensor(5)\n",
    "print(d)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c328ee6-46b6-435b-8bc3-08561c36afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [2, 2]])\n",
      "tensor([[6, 6],\n",
      "        [7, 7]])\n",
      "tensor([[6, 6],\n",
      "        [7, 7]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a + 5)\n",
    "print(a + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd10faea-6186-4502-a44f-bf354ae89b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 == torch.tensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98cb68d1-996d-43eb-bc23-945f79279c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [2, 2]])\n",
      "tensor([[2, 2],\n",
      "        [3, 3]])\n",
      "tensor([[2, 2],\n",
      "        [6, 6]])\n"
     ]
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "print(a)\n",
    "print(b)\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95a2b83e-4559-456a-8c29-5aef25baaa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5,  5],\n",
       "        [10, 10]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8265050d-b34b-4c31-bf86-be5454fd1274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 220.,  235.,  250.,  265.],\n",
      "         [ 580.,  631.,  682.,  733.],\n",
      "         [ 940., 1027., 1114., 1201.],\n",
      "         [1300., 1423., 1546., 1669.]],\n",
      "\n",
      "        [[1660., 1819., 1978., 2137.],\n",
      "         [2020., 2215., 2410., 2605.],\n",
      "         [2380., 2611., 2842., 3073.],\n",
      "         [2740., 3007., 3274., 3541.]],\n",
      "\n",
      "        [[3100., 3403., 3706., 4009.],\n",
      "         [3460., 3799., 4138., 4477.],\n",
      "         [3820., 4195., 4570., 4945.],\n",
      "         [4180., 4591., 5002., 5413.]],\n",
      "\n",
      "        [[4540., 4987., 5434., 5881.],\n",
      "         [4900., 5383., 5866., 6349.],\n",
      "         [5260., 5779., 6298., 6817.],\n",
      "         [5620., 6175., 6730., 7285.]],\n",
      "\n",
      "        [[5980., 6571., 7162., 7753.],\n",
      "         [6340., 6967., 7594., 8221.],\n",
      "         [6700., 7363., 8026., 8689.],\n",
      "         [7060., 7759., 8458., 9157.]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "a = torch.arange(120.).reshape(5, 4, 6)\n",
    "b = torch.arange(24.).reshape(6, 4)\n",
    "# c = (a @ b).to(dev)\n",
    "c = a @ b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7797877-18a9-41b7-b28f-17d98d4d6a4a",
   "metadata": {},
   "source": [
    "## 5.2 Simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc827dff-408c-4ce3-9745-67761ddf34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035d1b0-72b7-4b9a-adbf-ad18e5f01d8c",
   "metadata": {},
   "source": [
    "### 5.2.1 My first neural network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2cd34ac-63ba-4514-867f-df86e6101e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3555]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "input_tensor = torch.Tensor([[2, 3, 6]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(nn.Linear(3,1))\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a62cb02e-8f06-4a87-b539-80d3b2d9af65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor =\n",
      " tensor([[2., 3., 6.]])\n",
      "\n",
      "H0_weights =\n",
      " Parameter containing:\n",
      "tensor([[-0.0043,  0.3097, -0.4752]], requires_grad=True)\n",
      "\n",
      "H0_bias =\n",
      " Parameter containing:\n",
      "tensor([-0.4249], requires_grad=True)\n",
      "\n",
      "Output: tensor([[-2.3555]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# weights and biases \n",
    "print(\"input_tensor =\\n\", input_tensor)\n",
    "print(\"\\nH0_weights =\\n\", model[0].weight)\n",
    "print(\"\\nH0_bias =\\n\", model[0].bias)\n",
    "\n",
    "# linear combination of weights and bias\n",
    "print(\"\\nOutput:\", input_tensor @ model[0].weight.t() + model[0].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1c375-ddf2-43e5-89a7-3a9e46e3a8d5",
   "metadata": {},
   "source": [
    "![\"First Neural Network\"](images/simple_neural_network_01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d362f4b7-b9d4-4f53-b7a2-39fad120dd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of parameters in the one hidden layer network\n",
    "model[0].weight.numel() + model[0].bias.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7e42f-1ae4-4454-9ae2-402e6597beae",
   "metadata": {},
   "source": [
    "### 5.2.2 My second neural network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b5063a8-517e-4e55-9585-0481fd2a839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2638, 0.7427]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "nn2_input = torch.Tensor([[2, 3, 6, 4]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "nn2_model = nn.Sequential(\n",
    "    nn.Linear(4,3),\n",
    "    nn.Linear(3,2))\n",
    "nn2_output = nn2_model(nn2_input)\n",
    "print(nn2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "929d85a0-93de-4d4c-9f11-7ceb6c669c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6215,  1.2123, -1.1977]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the output of the first layer\n",
    "H0_input = nn2_input\n",
    "H0_output = H0_input @ nn2_model[0].weight.t() + nn2_model[0].bias\n",
    "H0_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a121003-8e39-4e8e-84c4-9cac3982ee38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2638, 0.7427]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the output of the 2nd layer\n",
    "H1_input = H0_output\n",
    "H1_output = H1_input @ nn2_model[1].weight.t() + nn2_model[1].bias\n",
    "H1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6089de4f-9ef1-4712-93f1-ff70297dc441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total weights = 18\n",
      "total biases = 5\n",
      "total trainable parameters = 23\n"
     ]
    }
   ],
   "source": [
    "#nn2_output == H1_output\n",
    "total_weights = sum([nn2_model[i].weight.numel() for i in range(len(list(nn2_model.children())))])\n",
    "total_biases = sum([nn2_model[i].bias.numel() for i in range(len(list(nn2_model.children())))])\n",
    "\n",
    "print(\"total weights =\", total_weights)\n",
    "print(\"total biases =\", total_biases)\n",
    "print(\"total trainable parameters =\", total_weights + total_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1d83f-f41c-4e73-b667-8d1d554a4e7f",
   "metadata": {},
   "source": [
    "![\"Second Neural Network\"](images/simple_neural_network_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ad33d-8653-4bf7-801e-ec71fbfeddb4",
   "metadata": {},
   "source": [
    "## 5.3 Activation function\n",
    "\n",
    "**Linear layers** are not the only layer type we can add to a network. We can use **activation functions** to add non-linearity to a network.\n",
    "\n",
    "This non-linearity grants networks the ability to learn more complex interactions between inputs `X` and targets `y` than only linear relationships. \n",
    "\n",
    " The output will no longer be a linear function of the input. \n",
    " \n",
    " We'll call the output of the last linear layer the **\"pre-activation\" output**, which we'll pass to activation functions to obtain transformed output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bda2fa-c128-4fe3-af0b-f4d6fe872da1",
   "metadata": {},
   "source": [
    "![](images/neural_network_with_activation_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b608374-a0db-46d9-91ab-ff5b896f28f5",
   "metadata": {},
   "source": [
    "### 5.3.2 Sigmoid function\n",
    "\n",
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve.\n",
    "\n",
    "$$\\sigma(x)=\\dfrac{1}{1+e^{-x}} = \\dfrac{e^{x}}{1+ e^{x}} = 1 - \\sigma(-x)$$\n",
    "\n",
    "Its derivative is\n",
    "\n",
    "$$\\sigma'(x)= \\sigma(x)\\sigma(-x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78e751d8-4e63-4ef6-84b3-603b1e160346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "# Generate input values\n",
    "x = torch.linspace(-10, 10, 100)  # Values from -10 to 10\n",
    "\n",
    "# Calculate sigmoid output values\n",
    "y = sigmoid(x)\n",
    "\n",
    "# gradient of x\n",
    "y_prime = sigmoid(x) * sigmoid(-x)\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label='Sigmoid Function')\n",
    "plt.plot(x, y_prime, label = \"Derivative of Sigmoid Function\")\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Sigmoid Output')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f46d1b-a313-44b8-8c51-af9f98f38011",
   "metadata": {},
   "source": [
    "In binary classification task, to predict whether an animal is `1` (a mammal) or `0` (not a mammal), we do the following steps.\n",
    "\n",
    "1) we take the pre-activation output,\n",
    "2) pass it to the sigmoid, and\n",
    "3) obtain a value between 0 and 1,\n",
    "\n",
    "Using the common threshold of 0.5\n",
    "* if output > 0.5, then class label = 1 (a mammal)\n",
    "* if output <= 0.5, then class label = 0 (not a mammal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9eac16d5-c553-4545-a2ae-5c9f370c2c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9933, 0.9820, 0.1192, 0.5744, 0.2689]])\n"
     ]
    }
   ],
   "source": [
    "# implementing sigmoid in pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "pre_activation = torch.tensor([[5, 4, -2, .3, -1]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(pre_activation)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "533de432-4f34-48f1-9fda-ef0099f2516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False,  True, False]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "601ebfab-3803-4129-b2ed-39e146fd6df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = (output > 0.5).float()\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c689a09-0aaa-49be-b490-84db6954a429",
   "metadata": {},
   "source": [
    "**The sigmoid function as the last step in neural network**\n",
    "    \n",
    "The sigmoid is commonly used as the last step in a neural network when performing binary classification.\n",
    "A sigmoid as the last step in a network of only linear layers is equivalent to a logistic regression using traditional machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586faa6e-d872-4ce7-9622-a192de06015e",
   "metadata": {},
   "source": [
    "### 5.3.3  The Softmax activation function\n",
    "\n",
    "We use the `sigmoid` activation function for binary classification. \n",
    "\n",
    "For multiclass classification, involving more than two class labels, we use `softmax`, another popular activation function.\n",
    "\n",
    "The Softmax function is defined as:\n",
    "\n",
    "$$\\text{Softmax}(x_i) = \\dfrac{e^{x_i}}{\\sum{e^{x_j}}}$$\n",
    "\n",
    "Its partial derivative with respect to $x_i$ is\n",
    "$$\\dfrac{\\partial{\\,\\text{Softmax}(x_i)}}{\\partial{x_i}} = \\text{Softmax}(x_i) \\left(1 - \\text{Softmax}(x_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226bae0-3f75-4d18-b401-bd7a57eed504",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Suppose that there are $N=3$ classes. \n",
    "\n",
    "1) **Class 0** for bird,\n",
    "2) **Class 1** for mammal, and\n",
    "3) **Class 2** for reptile.\n",
    "\n",
    "So the output has 3 elements which means that Softmax has 3 elements.\n",
    "\n",
    "The output is a probability distribution since element is a probability and the sum of the output is equal to `1`.\n",
    "\n",
    "softmax can be the last layer in `nn.Sequential`.\n",
    "\n",
    "![](images/softmax_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "991ac024-d15c-4776-8795-bf0fad8c745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output =  [0.1392 0.842  0.0188]\n",
      "sum of probabilities = 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# create the preactivation output\n",
    "pre_activation = torch.tensor([[4.3, 6.1, 2.3]])\n",
    "\n",
    "# Apply softmax along the last dimension (dim = -1)\n",
    "probabilities = nn.Softmax(dim = -1)\n",
    "output = probabilities(pre_activation)\n",
    "\n",
    "print(\"output = \", np.round(output.reshape(3,).tolist(), 4))\n",
    "print(\"sum of probabilities =\", float(output.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db48a98-592c-4001-b502-d133f9bfb627",
   "metadata": {},
   "source": [
    "## 5.4 Forward Pass\n",
    "\n",
    "Generating a prediction  from the neural network models is is called **running a forward pass** through a=the network.\n",
    "\n",
    "When we pass input data through a neural network in the forward direction to generate outputs, or predictions, the input data flows through the model layers. \n",
    "\n",
    "At each layer, computations performed on the data generate intermediate representations, which are passed to each subsequent layer until the final output is generated.\n",
    "\n",
    "The purpose of the forward pass is to propagate input data through the network and produce predictions or outputs based on the model's learned parameters (**weights** and **biases**). \n",
    "\n",
    "This is used for both training and generating new predictions. \n",
    "\n",
    "The final output can be **binary classifications**, **multi-class classifications**, or **numerical predictions** (**regressions**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff00f3-f3d0-4753-b279-ae45c7909da8",
   "metadata": {},
   "source": [
    "### 5.4.1 Binary classification: forward pass\n",
    "\n",
    "* `Class = 1`, the observation observed is a mammal\n",
    "* `Class = 0`, the animal observed is not a mammal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b001897-1198-4e30-9913-f6f2308dc655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3796],\n",
      "        [0.3907],\n",
      "        [0.5054],\n",
      "        [0.3692],\n",
      "        [0.5270]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create input data of shape (5,6)\n",
    "torch.manual_seed(0)\n",
    "input_data = torch.randn(5,6) # input data has 5 observations and 6 features\n",
    "\n",
    "# Create a binary classification model (bc_model)\n",
    "bc_model = nn.Sequential(\n",
    "    nn.Linear(6, 5), # First linear layer (Hidden)\n",
    "    nn.Linear(5, 1), # Second Linear Layer (Hidden)\n",
    "    nn.Sigmoid() # Sigmoid activation function\n",
    ")\n",
    "\n",
    "# Pass the bc_input data through the bc_model\n",
    "bc_output = bc_model(input_data)\n",
    "\n",
    "print(bc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32d24c57-d8f8-41e4-96a0-2697b3051561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False,  True]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_prediction = bc_output > 0.5\n",
    "bc_prediction.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9aaf3fd7-b3c5-4883-8132-2e5767b9cde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_prediction.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb8737-44c9-488a-a451-b8bbbff3dc01",
   "metadata": {},
   "source": [
    "### 5.4.2 Multi-class classification: forward pass\n",
    "\n",
    "Suppose we are predicting three classes: `0` for **mammal**, `1` for **bird** or `2` for **reptile**. \n",
    "\n",
    "We specify our model has three classes, setting this value as the last linear layer's output dimension. \n",
    "\n",
    "We use `softmax` instead of `sigmoid`, with `dim = -1` to indicate the five samples have the same last dimension as the last linear layer's output. \n",
    "\n",
    "Let's use the same input as before. \n",
    "\n",
    "*  The output shape is `(5,3)`.\n",
    "*  Each row sums to `1`.\n",
    "*  Value with the highest probability is assigned predicted label in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38f885eb-20bb-4cc0-8de1-6842d3a80b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Specify model has 3 classes\n",
    "n_classes = 3\n",
    "\n",
    "# Create a multi-class classification model (mc_model)\n",
    "mc_model = nn.Sequential(\n",
    "    nn.Linear(6, 5), # First Linear layer (hidden)\n",
    "    nn.Linear(5, n_classes), # Second linear layer (hidden)\n",
    "    nn.Softmax(dim = -1)  # Softmax activation\n",
    ")\n",
    "\n",
    "# Pass the mc_input through the mc_model\n",
    "mc_output = mc_model(input_data)\n",
    "print(mc_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2de6dd15-fc7e-4c4d-99c4-8332eb6bcb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3898, 0.3416, 0.2686],\n",
      "        [0.4191, 0.3383, 0.2426],\n",
      "        [0.1878, 0.4429, 0.3692],\n",
      "        [0.2340, 0.3831, 0.3829],\n",
      "        [0.2728, 0.4086, 0.3186]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee0cb14d-f289-4313-afdc-0f9466c16647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "mc_prediction = torch.argmax(mc_output, dim=1).view(-1,1)\n",
    "print(mc_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff773b2-391b-41cc-942f-6a5f8d95c701",
   "metadata": {},
   "source": [
    "### 5.4.3 Regression: forward pass\n",
    "\n",
    "The regressio model is used to predict continuous numerical values. \n",
    "\n",
    "Let's say we use the same data on five animals as before, but this time are predicting weights of animals based on their properties.\n",
    "\n",
    "There is no activation function at the end; and the last linear layer's last dimension returns an output with one feature. \n",
    "\n",
    "Output dimensions are five by one: five continuous values, one for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdbc917d-bed2-4b03-a14d-8c4843970c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3884],\n",
      "        [0.4115],\n",
      "        [0.6828],\n",
      "        [0.8111],\n",
      "        [0.3276]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create regression model\n",
    "torch.manual_seed(1)\n",
    "\n",
    "reg_model = nn.Sequential(\n",
    "    nn.Linear(6, 4), # First linear layer\n",
    "    nn.Linear(4,1) # Second linear layer\n",
    ")\n",
    "\n",
    "# Pass input data through the reg_model\n",
    "reg_output = reg_model(input_data)\n",
    "\n",
    "# Return reg_output\n",
    "print(reg_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e8d54-5298-455a-9cf2-5779e92687a0",
   "metadata": {},
   "source": [
    "*   For regression problems, where the output can be any real value, a common choice is to use a linear activation function in the output layer. \n",
    "    *   This means that the output is simply the weighted sum of the input without any transformation. \n",
    "    *   This allows the network to predict any value without any restriction.\n",
    " \n",
    "*   However, some regression problems may have a limited range of output values, such as `[0, 1]` or `[-1, 1]`.\n",
    "    *   In such cases, it may be beneficial to use a nonlinear activation function in the output layer that can map the output to the desired range.\n",
    "    *   For example, a `sigmoid` function can map any real value to `[0, 1]`, and a `tanh` function can map any real value to `[-1, 1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11dd031-bd66-4e72-b8f6-7e77e3981ba6",
   "metadata": {},
   "source": [
    "*   **Linear**: $f(x) = x$ and its derivative is $f'(x)=1$.\n",
    "\n",
    "<br> \n",
    "\n",
    "*   **Sigmoid**: $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$ and its derivative is $\\sigma'(x) = \\sigma(x) \\sigma(-x)$.\n",
    "\n",
    "<br>\n",
    "\n",
    "*   **Tanh**: $\\tanh(x) = \\dfrac{e^x - e^{-x}}{e^x+ e^{-x}}$  and its derivative is   $\\tanh'(x) = \\dfrac{4}{(e^x + e^{-x})^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7498d-f403-47cf-a0e0-357c36b7ee03",
   "metadata": {},
   "source": [
    "### 5.4.4 Back Propagation\n",
    "\n",
    "A **backward pass**, or **backpropagation**, is the process by which layer weights and biases are updated during training. \n",
    "\n",
    "All this is part of something called a **\"training loop\"**. \n",
    "\n",
    "This involves propagating data forward, comparing outputs to true values, then propagating backwards to improve each layer's weights and biases using some handy math.\n",
    "\n",
    "We repeat several times until the model is tuned with meaningful weights and biases. \n",
    "\n",
    "In short, during training, the backward pass is the complementary step to the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2a233a-bddc-490c-8b40-05b3ffb50ca7",
   "metadata": {},
   "source": [
    "## 5.5 Loss functions to evaluate model predictions\n",
    "\n",
    "* Created neural networks that\n",
    "    * take inputs and\n",
    "    * return predictions as outputs. \n",
    "\n",
    "<br>\n",
    "\n",
    "*  We'll now assess the differences between actual values and those predicted by the network using the loss function\n",
    "    * tells us how good our model is at making predictions during training. \n",
    "    * input of the loss function:  a model prediction, `y-hat`, and true label, or ground truth, `y`\n",
    "    * output a floating number.\n",
    " \n",
    "<br>\n",
    "\n",
    "* Our goal is to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c3dfd-bc17-4d30-a74b-a1a0fe987abb",
   "metadata": {},
   "source": [
    "### 5.5.1  Hot-one encoding to transform the values of the true label, `y`\n",
    "\n",
    "If the number of  classes is `num_classes = 3`, then\n",
    "1)  `[1, 0, 0]`, if `y = 0`, or\n",
    "2)  `[0, 1, 0]`, if `y = 1`, or\n",
    "2)  `[0, 0, 1]`, if `y = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2fcda2c-a3d9-4d51-ae3f-8eca27e8c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding for y=0 : tensor([1, 0, 0])\n",
      "one-hot encoding for y=1 : tensor([0, 1, 0])\n",
      "one-hot encoding for y=2 : tensor([0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"one-hot encoding for y=0 :\", F.one_hot(torch.tensor(0), num_classes = 3))\n",
    "print(\"one-hot encoding for y=1 :\", F.one_hot(torch.tensor(1), num_classes = 3))\n",
    "print(\"one-hot encoding for y=2 :\", F.one_hot(torch.tensor(2), num_classes = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b44cc4a-f4dc-49b7-990d-be0d9dcd53aa",
   "metadata": {},
   "source": [
    "### 5.5.2 Parameters and output for the loss function\n",
    "\n",
    "The parameters of the loss function are:\n",
    "\n",
    "* `scores`: model prediction before the final softmax activation function\n",
    "* `one_hot_target`: one-hot encoded ground truth label\n",
    "\n",
    "Its output is `loss` a single `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5abee34-6117-43f6-acc1-c0ce8876bc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Calculating cross entropy loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = [1]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
    "\n",
    "# Create the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccab0c-e323-4810-99f5-fc643c528401",
   "metadata": {},
   "source": [
    "## 5.6 Updating the model's parameters\n",
    "\n",
    "A model is mispredicting when loss is high, and predicting correctly when loss is low.\n",
    "\n",
    "So we have to minimize the loss by finding the derivative of the loss function.\n",
    "\n",
    "![](images/nn2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18508886-99db-47e6-9822-7495553a5b44",
   "metadata": {},
   "source": [
    "### 5.6.1  Gradients of the weights and biases\n",
    "\n",
    "Since **loss** is dependent on the model's parameters: **weights** and **biases**, we can minimize the loss by updating the values of these parameters using their respective gradients.\n",
    "\n",
    "$$\\dfrac{\\partial{\\text{loss}}}{\\partial{w}} = \\left(\\dfrac{\\partial{\\text{loss}}}{\\partial{\\text{y\\_hat}}}\\right)\\left( \\dfrac{\\partial{\\text{y\\_hat}}}{\\partial{\\text{z}}}\\right)\\left(\\dfrac{\\partial{\\text{z}}}{\\partial{\\text{w}}}\\right) = \\left(\\dfrac{\\partial{\\text{loss}}}{\\partial{\\text{y\\_hat}}}\\right)\\left(\\dfrac{\\partial{\\text{y\\_hat}}}{\\partial{\\text{z}}}\\right)\\left( X \\right)$$\n",
    "\n",
    "$$\\dfrac{\\partial{\\text{loss}}}{\\partial{b}} = \\left(\\dfrac{\\partial{\\text{loss}}}{\\partial{\\text{y\\_hat}}}\\right)\\left( \\dfrac{\\partial{\\text{y\\_hat}}}{\\partial{\\text{z}}}\\right)\\left(\\dfrac{\\partial{\\text{z}}}{\\partial{\\text{w}}}\\right) = \\left(\\dfrac{\\partial{\\text{loss}}}{\\partial{\\text{y\\_hat}}}\\right)\\left(\\dfrac{\\partial{\\text{y\\_hat}}}{\\partial{\\text{z}}}\\right)$$\n",
    "\n",
    "\n",
    "*  *Updating the weight* : `weight = weight - lr * weight_grad`\n",
    "*  *Updating thew bias* :   `bias = bias - lr * bias_grad`\n",
    "\n",
    "\n",
    " where\n",
    " * $\\text{weight\\_grad} = \\dfrac{\\partial\\,{\\text{loss}}}{\\partial{w}}$\n",
    " * $\\text{bias\\_grad} = \\dfrac{\\partial\\,{\\text{loss}}}{\\partial{b}}$\n",
    " * `lr` is the **learning rate**.\n",
    "\n",
    " In deep learning the learning  rate is a hyperparameter that determines the step size at which a neural network's model parameters are updated during training. \n",
    "It controls how much the model learns from each new data point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb71094-6f3f-4ea1-89cc-3e31ffee1441",
   "metadata": {},
   "source": [
    "### 5.6.2 Loss functions for classification and regression\n",
    "\n",
    "**Binary cross entropy loss function (BCE)** for binary classification problem. In PyTorch we use (`nn.BCELoss()`)\n",
    "\n",
    "*  it measures the difference between the predicted probabilities, `y_hat` and the true labels `y`. \n",
    "\n",
    "*  The formula for binary cross entropy loss is:\n",
    "    \n",
    "    $$L(y,p) = - [y \\cdot \\log{p} + (1-y)\\log(1-p)]$$\n",
    "    \n",
    "where $L(y, p)$ is the binary cross-entropy loss, \n",
    "\n",
    "$\n",
    "$ y is the true binary label $($0 or$ $1, and \n",
    "\n",
    "$\r",
    "$\n",
    "p is the predicted probability that the instance belongs to clas$s$ with  `p = y_hat` if `y_hat > 0.5` and `1-p = y_hat` if `y_hat <= 0.5`.    \n",
    "    1\n",
    "Its derivative with respect to $p$ is\n",
    "\n",
    "$$\\frac{dL}{dp} = -\\dfrac{y}{p} + \\dfrac{1 - y}{1 - p}$$\n",
    "\n",
    "\n",
    "The **BCE** is usually combined with **sigmoid** activation function.e:.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65039df-2226-466b-9f35-a39ed4ad6bb8",
   "metadata": {},
   "source": [
    "In PyTorch, the **cross-entropy loss function** is commonly used for multi-class classification problems and is typically referred to as \"CrossEntropyLoss.\" (`nn.CrossEntropyLoss()`)\n",
    "\n",
    "This loss function combines the `softmax function` (to convert raw scores into class probabilities) and the negative log-likelihood loss.\n",
    "\n",
    "\n",
    "The formula forthe  **cross-entropy loss function**  in one observation is given by:\n",
    "\n",
    "$$L(y, p) = - \\sum_{i}^{n}  y_i  \\cdot \\log{p_i}$$\n",
    "\n",
    "where $L(y, p)$ is the cross-entropy loss function,\n",
    "\n",
    "$n$ is the number of classes\n",
    "\n",
    "$y$  is the true multiclass label, which is a vector of one-hot encoded labels, where only the true class is $1$, and all other entries are $0$.\n",
    "\n",
    "$p$  represents the predicted probability distribution over classes.\n",
    "\n",
    "The `nn.CrossEntropyLoss` function automatically applies the **softmax** function to the output tensor and computes the **cross-entropy loss** between the predicted class probabilities and the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf047a-b56b-4826-af5e-2d54c82d4a70",
   "metadata": {},
   "source": [
    "In PyTorch, for regression tasks where we want to predict continuous values rather than class labels, we typically use **mean squared error (MSE)** or **mean absolute error (MAE) loss functions**. \n",
    "\n",
    "*   **Mean Squared Error (MSE) Loss (L2 Loss)**:  `nn.MSELoss()`.\n",
    "\n",
    "    *   This loss function measures the average of the squared differences between the predicted values and the true target values. \n",
    "\n",
    "*   **Mean Absolute Error (MAE) Loss (L1 Loss)**: ` nn.L1Loss()`.\r\n",
    "    *    \r\n",
    "This loss function measures the average of the absolute differences between the predicted values and the true target value\n",
    "    *    . It is also suitable for regression tasks, particularly iweou want your model to be less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af74e9f1-2484-49cb-a621-a65696acea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(12345)\n",
    "input_tensor = torch.randn(1,9)\n",
    "target = torch.tensor([[1., 0.]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(9,2),\n",
    "    nn.Sigmoid() # Sigmoid activation function                 \n",
    ")\n",
    "\n",
    "prediction = model(input_tensor)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(prediction, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "weight0 = model[0].weight\n",
    "weight0_grads = model[0].weight.grad\n",
    "bias0 = model[0].bias\n",
    "bias0_grads = model[0].bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6dfa0f4c-9c0c-4cbe-bc68-86886d4792b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight0 =  Parameter containing:\n",
      "tensor([[ 0.0149, -0.2350, -0.1835, -0.1942,  0.1139, -0.1986, -0.0073,  0.0140,\n",
      "          0.2149],\n",
      "        [-0.2520, -0.2288, -0.1936,  0.2333, -0.1198,  0.2812,  0.1205,  0.0422,\n",
      "         -0.0025]], requires_grad=True)\n",
      "\n",
      "weight0_grads = \n",
      " tensor([[ 0.3549, -0.1187, -0.1154, -0.5068, -0.0791, -0.1292,  0.1526, -0.0933,\n",
      "         -0.2466],\n",
      "        [-0.3767,  0.1260,  0.1224,  0.5378,  0.0840,  0.1371, -0.1620,  0.0991,\n",
      "          0.2618]])\n"
     ]
    }
   ],
   "source": [
    "print(\"weight0 = \", weight0) \n",
    "print(\"\\nweight0_grads = \\n\", weight0_grads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a8e344a-762d-49ac-a30a-026a182fc468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias0 =  Parameter containing:\n",
      "tensor([-0.0659,  0.0418], requires_grad=True)\n",
      "\n",
      "bias0_grads = \n",
      " tensor([-0.3042,  0.3229])\n"
     ]
    }
   ],
   "source": [
    "print(\"bias0 = \", bias0) \n",
    "print(\"\\nbias0_grads = \\n\", bias0_grads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8878f91-c829-453f-9df7-903230b495ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight0 = \n",
      " tensor([[ 0.0145, -0.2349, -0.1834, -0.1937,  0.1140, -0.1985, -0.0074,  0.0141,\n",
      "          0.2151],\n",
      "        [-0.2516, -0.2290, -0.1937,  0.2328, -0.1199,  0.2810,  0.1207,  0.0421,\n",
      "         -0.0027]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Updating the weights manually\n",
    "lr = torch.tensor([0.001])\n",
    "print(\"weight0 = \\n\", weight0 - lr * weight0_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c61be5-f163-419b-a88f-7f92e0b20a3f",
   "metadata": {},
   "source": [
    "### 5.6.3 The need for an optimizer\n",
    "\n",
    "*  Some functions have one minimum and one only, called the \"global\" minimum. These functions are \"convex\". \n",
    "*  Some \"non-convex\" functions have more than one \"local\" minimum.\n",
    "\n",
    "![](images/convex_and_non_convex_functions.png)\n",
    "\n",
    "At a local minimum, the function value is lowest compared to nearby points, but points further away may be even lower. \n",
    "\n",
    "When minimizing loss functions, our goal is to find the global minimum of the non-convex function, here, when x is approximately one.\n",
    "\n",
    "Loss functions used in deep learning are not convex! T\n",
    "\n",
    "To find global minima of non-convex functions, we use a mechanism called \"gradient descent\" which is called \"optimizers\" in PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca198a-ec33-40e5-a75a-6bdee97ed593",
   "metadata": {},
   "source": [
    "**The need for momentum**\n",
    "\n",
    "![](images/momentum_low.png)\n",
    "\n",
    "![](images/momentum_high.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3f10ce24-4df0-4eff-b0f9-e238811ee23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first prediction =  tensor([[0.3548, 0.5141]], grad_fn=<SigmoidBackward0>)\n",
      "final prediction =  tensor([[0.7355, 0.2253]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# torch.manual_seed(908) # for 01\n",
    "# torch.manual_seed(1) # for 11\n",
    "torch.manual_seed(2)  # for 10\n",
    "#torch.manual_seed(3)  # for 11\n",
    "#torch.manual_seed(1000)  # for 00\n",
    "\n",
    "input_tensor = torch.randn(1,9)\n",
    "target = torch.tensor([[1., 0.]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(9,2),\n",
    "    nn.Sigmoid() # Sigmoid activation function                 \n",
    ")\n",
    "\n",
    "first_prediction = model(input_tensor)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(first_prediction, target)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1, momentum=1)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "#loss.backward(retain_graph=True)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "final_prediction = model(input_tensor)\n",
    "\n",
    "print(\"first prediction = \", first_prediction)\n",
    "print(\"final prediction = \", final_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b786b5-4258-4119-8a14-a4910c08a1fa",
   "metadata": {},
   "source": [
    "## 5.7 Using the MSELoss\n",
    "\n",
    "We cannot use cross-entropy loss for regression problems. The mean squared error loss (MSELoss) is a common loss function for regression problems.\n",
    "\n",
    "```\n",
    "def mean_squared_loss(prediction, target):\n",
    "   return np.mean((prediction - target)**2)\n",
    "```\n",
    "\n",
    "\n",
    "In PyTorch,\n",
    "\n",
    "```\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Prediction and target are float tensors\n",
    "loss = criterion(prediction, target)\n",
    "```\n",
    "\n",
    "This loss is used for regression problems (e.g., when trying to fit a linear regression model).\r\n",
    "\n",
    "                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e1fdf0e-1d5e-4b74-a525-82e1267180cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat - y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat, dtype=torch.float32), \n",
    "                        torch.tensor(y, dtype=torch.float32))\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26db9a76-01c8-4325-b65c-5d9fc00ae83b",
   "metadata": {},
   "source": [
    "## 5.8 Writing a training loop\n",
    "\n",
    "In scikit-learn, the whole training loop is contained in the `.fit()` method. \n",
    "\n",
    "In PyTorch, however, we have to implement the loop manually.arameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe7b59-fa59-4cfa-8117-a020d6d0214f",
   "metadata": {},
   "source": [
    "### 5.8.1 Training a neural network\n",
    "\n",
    "1)  Create a model\n",
    "    \n",
    "2)  Choose a loss function\n",
    "\n",
    "3)  Create a dataset\n",
    "\n",
    "4)  Define an optimizer\n",
    "\n",
    "5)  Run a training loop, where for each sample of the dataset, we repeat:\n",
    "\n",
    "    *   Calculating loss (forward pass)\n",
    "     \n",
    "    *   Calculating local gradients\n",
    "     \n",
    "    *   Updating model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbb41b3d-1e43-4dad-ab51-24f2eb39c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6ff71-3d4c-47d7-b2cd-9bb2c9f32ee0",
   "metadata": {},
   "source": [
    "### 5.8.2 Before the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ddcf0-ddf4-4d11-84dc-f8cbf0d4f14d",
   "metadata": {},
   "source": [
    "*   experience_level: The experience level in the job during the year with the following possible values: **EN** Entry-level / Junior, **MI** Mid-level / Intermediate,  **SE** Senior-level / Expert, **EX** Executive-level / Director.\n",
    "*   employment_type: The type of employement for the role: **PT** Part-time, *FT** Full-time,  **CT** Contract, **FL** Freelance.\n",
    "*   company_size: The average number of people that worked for the company during the year: **S** less than 50 employees (small), **M** 50 to 250 employees (medium), **L** more than 250 employees (large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf6d28b1-6734-45e2-9dc6-3618e1ca9a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "salaries = pd.read_csv(\"datasets/ds_salaries.csv\")\n",
    "target_df = salaries[\"salary_in_usd\"]\n",
    "features_df = salaries[[\"experience_level\", \"employment_type\", \"remote_ratio\", \"company_size\"]]\n",
    "features_df.loc[:, \"company_size\"] = features_df[\"company_size\"].astype(\"category\").cat.codes\n",
    "features_df.loc[:, \"experience_level\"] = features_df[\"experience_level\"].astype(\"category\").cat.codes\n",
    "features_df.loc[:, \"employment_type\"] = features_df[\"employment_type\"].astype(\"category\").cat.codes\n",
    "features = features_df.to_numpy(dtype='float32')\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e8925ed-0f62-4db1-b565-51dc8c7555bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "206d5ab6-dc71-40c9-acf4-6b48717d459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "target = scaler.fit_transform(target_df.values.reshape(-1, 1))\n",
    "target = target.astype(np.float32)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6865529e-9908-40b8-94bf-a85680007886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features), torch.tensor(target))\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = nn.Sequential(nn.Linear(4, 2),                       \n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "# Create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf792a-ff5b-4db8-9a33-2c3284d98bb3",
   "metadata": {},
   "source": [
    "### 5,8,3 The training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "daf00b91-dfb3-4c4c-b043-aedce57aba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            feature, target = data\n",
    "            pred = model(feature)\n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item() * feature.size(0)\n",
    "            total_samples += feature.size(0)\n",
    "            \n",
    "            # Print ground truth and predicted salaries\n",
    "            for i in range(len(target)):\n",
    "                print(\"Ground truth salary: {:.3f}. Predicted salary: {:.3f}\".format(target[i].item(), pred[i].item()))\n",
    "    \n",
    "    average_loss = total_loss / total_samples\n",
    "    print(\"Average Loss: {:.4f}\".format(average_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21d90dc5-4496-4659-9f97-a81f76af58a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth salary: 0.148. Predicted salary: 0.088\n",
      "Ground truth salary: 0.253. Predicted salary: 0.208\n",
      "Ground truth salary: 0.280. Predicted salary: 0.153\n",
      "Ground truth salary: 0.205. Predicted salary: 0.154\n",
      "Ground truth salary: 0.179. Predicted salary: 0.153\n",
      "Ground truth salary: 0.213. Predicted salary: 0.165\n",
      "Ground truth salary: 0.093. Predicted salary: 0.175\n",
      "Ground truth salary: 0.213. Predicted salary: 0.208\n",
      "Ground truth salary: 0.288. Predicted salary: 0.208\n",
      "Ground truth salary: 0.096. Predicted salary: 0.262\n",
      "Ground truth salary: 0.145. Predicted salary: 0.175\n",
      "Ground truth salary: 0.330. Predicted salary: 0.153\n",
      "Ground truth salary: 0.224. Predicted salary: 0.208\n",
      "Ground truth salary: 0.167. Predicted salary: 0.185\n",
      "Ground truth salary: 0.364. Predicted salary: 0.229\n",
      "Ground truth salary: 0.087. Predicted salary: 0.219\n",
      "Ground truth salary: 0.104. Predicted salary: 0.133\n",
      "Ground truth salary: 0.339. Predicted salary: 0.251\n",
      "Ground truth salary: 0.330. Predicted salary: 0.153\n",
      "Ground truth salary: 0.117. Predicted salary: 0.197\n",
      "Ground truth salary: 0.132. Predicted salary: 0.208\n",
      "Ground truth salary: 0.208. Predicted salary: 0.207\n",
      "Ground truth salary: 0.305. Predicted salary: 0.208\n",
      "Ground truth salary: 0.149. Predicted salary: 0.176\n",
      "Ground truth salary: 0.168. Predicted salary: 0.231\n",
      "Ground truth salary: 0.121. Predicted salary: 0.197\n",
      "Ground truth salary: 0.257. Predicted salary: 0.208\n",
      "Ground truth salary: 0.313. Predicted salary: 0.231\n",
      "Ground truth salary: 0.253. Predicted salary: 0.121\n",
      "Ground truth salary: 0.263. Predicted salary: 0.208\n",
      "Ground truth salary: 0.022. Predicted salary: 0.088\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.129. Predicted salary: 0.153\n",
      "Ground truth salary: 0.121. Predicted salary: 0.153\n",
      "Ground truth salary: 0.129. Predicted salary: 0.197\n",
      "Ground truth salary: 0.102. Predicted salary: 0.220\n",
      "Ground truth salary: 0.192. Predicted salary: 0.251\n",
      "Ground truth salary: 0.380. Predicted salary: 0.208\n",
      "Ground truth salary: 0.168. Predicted salary: 0.153\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.404. Predicted salary: 0.208\n",
      "Ground truth salary: 0.275. Predicted salary: 0.208\n",
      "Ground truth salary: 0.288. Predicted salary: 0.208\n",
      "Ground truth salary: 0.205. Predicted salary: 0.252\n",
      "Ground truth salary: 0.178. Predicted salary: 0.208\n",
      "Ground truth salary: 0.183. Predicted salary: 0.153\n",
      "Ground truth salary: 0.221. Predicted salary: 0.229\n",
      "Ground truth salary: 0.025. Predicted salary: 0.242\n",
      "Ground truth salary: 0.217. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.153\n",
      "Ground truth salary: 0.144. Predicted salary: 0.185\n",
      "Ground truth salary: 0.227. Predicted salary: 0.208\n",
      "Ground truth salary: 0.246. Predicted salary: 0.176\n",
      "Ground truth salary: 0.196. Predicted salary: 0.231\n",
      "Ground truth salary: 0.116. Predicted salary: 0.175\n",
      "Ground truth salary: 0.056. Predicted salary: 0.175\n",
      "Ground truth salary: 0.272. Predicted salary: 0.208\n",
      "Ground truth salary: 0.186. Predicted salary: 0.231\n",
      "Ground truth salary: 0.221. Predicted salary: 0.153\n",
      "Ground truth salary: 0.424. Predicted salary: 0.231\n",
      "Ground truth salary: 0.092. Predicted salary: 0.111\n",
      "Ground truth salary: 0.305. Predicted salary: 0.251\n",
      "Ground truth salary: 0.162. Predicted salary: 0.230\n",
      "Ground truth salary: 0.179. Predicted salary: 0.132\n",
      "Ground truth salary: 0.124. Predicted salary: 0.252\n",
      "Ground truth salary: 0.131. Predicted salary: 0.155\n",
      "Ground truth salary: 0.247. Predicted salary: 0.208\n",
      "Ground truth salary: 0.061. Predicted salary: 0.176\n",
      "Ground truth salary: 0.126. Predicted salary: 0.176\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.268. Predicted salary: 0.208\n",
      "Ground truth salary: 0.211. Predicted salary: 0.208\n",
      "Ground truth salary: 0.149. Predicted salary: 0.219\n",
      "Ground truth salary: 0.129. Predicted salary: 0.142\n",
      "Ground truth salary: 0.079. Predicted salary: 0.111\n",
      "Ground truth salary: 0.251. Predicted salary: 0.197\n",
      "Ground truth salary: 0.230. Predicted salary: 0.185\n",
      "Ground truth salary: 0.221. Predicted salary: 0.143\n",
      "Ground truth salary: 0.225. Predicted salary: 0.208\n",
      "Ground truth salary: 0.148. Predicted salary: 0.274\n",
      "Ground truth salary: 0.163. Predicted salary: 0.207\n",
      "Ground truth salary: 0.052. Predicted salary: 0.219\n",
      "Ground truth salary: 0.357. Predicted salary: 0.143\n",
      "Ground truth salary: 0.089. Predicted salary: 0.176\n",
      "Ground truth salary: 0.127. Predicted salary: 0.176\n",
      "Ground truth salary: 0.330. Predicted salary: 0.185\n",
      "Ground truth salary: 0.201. Predicted salary: 0.208\n",
      "Ground truth salary: 0.178. Predicted salary: 0.208\n",
      "Ground truth salary: 0.156. Predicted salary: 0.207\n",
      "Ground truth salary: 0.129. Predicted salary: 0.208\n",
      "Ground truth salary: 0.263. Predicted salary: 0.229\n",
      "Ground truth salary: 0.217. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.221. Predicted salary: 0.176\n",
      "Ground truth salary: 0.297. Predicted salary: 0.208\n",
      "Ground truth salary: 0.109. Predicted salary: 0.176\n",
      "Ground truth salary: 0.078. Predicted salary: 0.177\n",
      "Ground truth salary: 0.341. Predicted salary: 0.219\n",
      "Ground truth salary: 0.389. Predicted salary: 0.121\n",
      "Ground truth salary: 0.083. Predicted salary: 0.111\n",
      "Ground truth salary: 0.068. Predicted salary: 0.219\n",
      "Ground truth salary: 0.078. Predicted salary: 0.176\n",
      "Ground truth salary: 0.292. Predicted salary: 0.185\n",
      "Ground truth salary: 0.173. Predicted salary: 0.153\n",
      "Ground truth salary: 0.037. Predicted salary: 0.133\n",
      "Ground truth salary: 0.223. Predicted salary: 0.251\n",
      "Ground truth salary: 0.160. Predicted salary: 0.219\n",
      "Ground truth salary: 0.246. Predicted salary: 0.207\n",
      "Ground truth salary: 0.213. Predicted salary: 0.208\n",
      "Ground truth salary: 0.138. Predicted salary: 0.187\n",
      "Ground truth salary: 0.066. Predicted salary: 0.110\n",
      "Ground truth salary: 0.144. Predicted salary: 0.175\n",
      "Ground truth salary: 0.184. Predicted salary: 0.208\n",
      "Ground truth salary: 0.287. Predicted salary: 0.185\n",
      "Ground truth salary: 0.232. Predicted salary: 0.219\n",
      "Ground truth salary: 0.045. Predicted salary: 0.110\n",
      "Ground truth salary: 0.072. Predicted salary: 0.155\n",
      "Ground truth salary: 0.086. Predicted salary: 0.251\n",
      "Ground truth salary: 0.704. Predicted salary: 0.175\n",
      "Ground truth salary: 0.057. Predicted salary: 0.110\n",
      "Ground truth salary: 0.184. Predicted salary: 0.208\n",
      "Ground truth salary: 0.255. Predicted salary: 0.208\n",
      "Ground truth salary: 0.370. Predicted salary: 0.143\n",
      "Ground truth salary: 0.081. Predicted salary: 0.175\n",
      "Ground truth salary: 0.217. Predicted salary: 0.251\n",
      "Ground truth salary: 0.297. Predicted salary: 0.251\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.176\n",
      "Ground truth salary: 0.238. Predicted salary: 0.208\n",
      "Ground truth salary: 0.233. Predicted salary: 0.164\n",
      "Ground truth salary: 0.184. Predicted salary: 0.175\n",
      "Ground truth salary: 0.226. Predicted salary: 0.134\n",
      "Ground truth salary: 0.322. Predicted salary: 0.208\n",
      "Ground truth salary: 0.272. Predicted salary: 0.208\n",
      "Ground truth salary: 0.081. Predicted salary: 0.177\n",
      "Ground truth salary: 0.032. Predicted salary: 0.124\n",
      "Ground truth salary: 0.124. Predicted salary: 0.134\n",
      "Ground truth salary: 0.146. Predicted salary: 0.176\n",
      "Ground truth salary: 0.246. Predicted salary: 0.153\n",
      "Ground truth salary: 0.151. Predicted salary: 0.153\n",
      "Ground truth salary: 0.197. Predicted salary: 0.208\n",
      "Ground truth salary: 0.152. Predicted salary: 0.208\n",
      "Ground truth salary: 0.079. Predicted salary: 0.219\n",
      "Ground truth salary: 0.179. Predicted salary: 0.153\n",
      "Ground truth salary: 0.399. Predicted salary: 0.176\n",
      "Ground truth salary: 0.095. Predicted salary: 0.198\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.104. Predicted salary: 0.134\n",
      "Ground truth salary: 0.129. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.098. Predicted salary: 0.197\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.125. Predicted salary: 0.230\n",
      "Ground truth salary: 0.012. Predicted salary: 0.134\n",
      "Ground truth salary: 0.749. Predicted salary: 0.219\n",
      "Ground truth salary: 0.196. Predicted salary: 0.088\n",
      "Ground truth salary: 0.288. Predicted salary: 0.143\n",
      "Ground truth salary: 0.246. Predicted salary: 0.176\n",
      "Ground truth salary: 0.092. Predicted salary: 0.110\n",
      "Ground truth salary: 0.272. Predicted salary: 0.251\n",
      "Ground truth salary: 0.227. Predicted salary: 0.176\n",
      "Ground truth salary: 0.173. Predicted salary: 0.208\n",
      "Ground truth salary: 0.072. Predicted salary: 0.219\n",
      "Ground truth salary: 0.180. Predicted salary: 0.208\n",
      "Ground truth salary: 0.002. Predicted salary: 0.176\n",
      "Ground truth salary: 0.179. Predicted salary: 0.153\n",
      "Ground truth salary: 0.313. Predicted salary: 0.251\n",
      "Ground truth salary: 0.251. Predicted salary: 0.185\n",
      "Ground truth salary: 0.221. Predicted salary: 0.208\n",
      "Ground truth salary: 0.026. Predicted salary: 0.176\n",
      "Ground truth salary: 0.330. Predicted salary: 0.185\n",
      "Ground truth salary: 0.330. Predicted salary: 0.208\n",
      "Ground truth salary: 0.246. Predicted salary: 0.208\n",
      "Ground truth salary: 0.055. Predicted salary: 0.175\n",
      "Ground truth salary: 0.238. Predicted salary: 0.208\n",
      "Ground truth salary: 0.083. Predicted salary: 0.176\n",
      "Ground truth salary: 0.214. Predicted salary: 0.176\n",
      "Ground truth salary: 0.015. Predicted salary: 0.228\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.162. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.111\n",
      "Ground truth salary: 0.171. Predicted salary: 0.153\n",
      "Ground truth salary: 0.142. Predicted salary: 0.176\n",
      "Ground truth salary: 0.261. Predicted salary: 0.176\n",
      "Ground truth salary: 0.276. Predicted salary: 0.143\n",
      "Ground truth salary: 0.082. Predicted salary: 0.154\n",
      "Ground truth salary: 0.251. Predicted salary: 0.208\n",
      "Ground truth salary: 0.213. Predicted salary: 0.219\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.364. Predicted salary: 0.208\n",
      "Ground truth salary: 0.029. Predicted salary: 0.134\n",
      "Ground truth salary: 0.240. Predicted salary: 0.208\n",
      "Ground truth salary: 0.263. Predicted salary: 0.153\n",
      "Ground truth salary: 0.189. Predicted salary: 0.251\n",
      "Ground truth salary: 0.035. Predicted salary: 0.208\n",
      "Ground truth salary: 0.005. Predicted salary: 0.153\n",
      "Ground truth salary: 0.345. Predicted salary: 0.185\n",
      "Ground truth salary: 0.092. Predicted salary: 0.242\n",
      "Ground truth salary: 0.213. Predicted salary: 0.176\n",
      "Ground truth salary: 0.129. Predicted salary: 0.088\n",
      "Ground truth salary: 0.207. Predicted salary: 0.219\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.216. Predicted salary: 0.251\n",
      "Ground truth salary: 0.255. Predicted salary: 0.208\n",
      "Ground truth salary: 0.247. Predicted salary: 0.208\n",
      "Ground truth salary: 0.026. Predicted salary: 0.111\n",
      "Ground truth salary: 0.272. Predicted salary: 0.208\n",
      "Ground truth salary: 0.002. Predicted salary: 0.154\n",
      "Ground truth salary: 0.124. Predicted salary: 0.176\n",
      "Ground truth salary: 0.050. Predicted salary: 0.176\n",
      "Ground truth salary: 0.137. Predicted salary: 0.208\n",
      "Ground truth salary: 0.027. Predicted salary: 0.251\n",
      "Ground truth salary: 0.104. Predicted salary: 0.231\n",
      "Ground truth salary: 0.144. Predicted salary: 0.176\n",
      "Ground truth salary: 0.105. Predicted salary: 0.251\n",
      "Ground truth salary: 0.188. Predicted salary: 0.175\n",
      "Ground truth salary: 0.330. Predicted salary: 0.153\n",
      "Ground truth salary: 0.087. Predicted salary: 0.088\n",
      "Ground truth salary: 0.243. Predicted salary: 0.208\n",
      "Ground truth salary: 0.243. Predicted salary: 0.208\n",
      "Ground truth salary: 0.166. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.134\n",
      "Ground truth salary: 0.151. Predicted salary: 0.219\n",
      "Ground truth salary: 0.081. Predicted salary: 0.175\n",
      "Ground truth salary: 0.213. Predicted salary: 0.208\n",
      "Ground truth salary: 0.074. Predicted salary: 0.198\n",
      "Ground truth salary: 0.414. Predicted salary: 0.110\n",
      "Ground truth salary: 0.037. Predicted salary: 0.208\n",
      "Ground truth salary: 0.078. Predicted salary: 0.133\n",
      "Ground truth salary: 0.015. Predicted salary: 0.080\n",
      "Ground truth salary: 0.114. Predicted salary: 0.175\n",
      "Ground truth salary: 0.094. Predicted salary: 0.133\n",
      "Ground truth salary: 0.221. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.050. Predicted salary: 0.198\n",
      "Ground truth salary: 0.147. Predicted salary: 0.208\n",
      "Ground truth salary: 0.078. Predicted salary: 0.176\n",
      "Ground truth salary: 0.196. Predicted salary: 0.252\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.177\n",
      "Ground truth salary: 0.171. Predicted salary: 0.176\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.142. Predicted salary: 0.231\n",
      "Ground truth salary: 0.074. Predicted salary: 0.176\n",
      "Ground truth salary: 0.330. Predicted salary: 0.185\n",
      "Ground truth salary: 0.224. Predicted salary: 0.208\n",
      "Ground truth salary: 0.142. Predicted salary: 0.153\n",
      "Ground truth salary: 0.115. Predicted salary: 0.231\n",
      "Ground truth salary: 0.113. Predicted salary: 0.175\n",
      "Ground truth salary: 0.043. Predicted salary: 0.175\n",
      "Ground truth salary: 0.033. Predicted salary: 0.197\n",
      "Ground truth salary: 0.184. Predicted salary: 0.229\n",
      "Ground truth salary: 0.171. Predicted salary: 0.208\n",
      "Ground truth salary: 0.287. Predicted salary: 0.185\n",
      "Ground truth salary: 0.129. Predicted salary: 0.111\n",
      "Ground truth salary: 0.043. Predicted salary: 0.153\n",
      "Ground truth salary: 0.314. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.110\n",
      "Ground truth salary: 0.018. Predicted salary: 0.088\n",
      "Ground truth salary: 0.083. Predicted salary: 0.219\n",
      "Ground truth salary: 0.063. Predicted salary: 0.207\n",
      "Ground truth salary: 0.163. Predicted salary: 0.132\n",
      "Ground truth salary: 0.236. Predicted salary: 0.185\n",
      "Ground truth salary: 1.000. Predicted salary: 0.121\n",
      "Ground truth salary: 0.058. Predicted salary: 0.175\n",
      "Ground truth salary: 0.138. Predicted salary: 0.153\n",
      "Ground truth salary: 0.206. Predicted salary: 0.208\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.290. Predicted salary: 0.208\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.155\n",
      "Ground truth salary: 0.155. Predicted salary: 0.251\n",
      "Ground truth salary: 0.099. Predicted salary: 0.207\n",
      "Ground truth salary: 0.447. Predicted salary: 0.215\n",
      "Ground truth salary: 0.038. Predicted salary: 0.198\n",
      "Ground truth salary: 0.213. Predicted salary: 0.143\n",
      "Ground truth salary: 0.213. Predicted salary: 0.175\n",
      "Ground truth salary: 0.305. Predicted salary: 0.185\n",
      "Ground truth salary: 0.048. Predicted salary: 0.153\n",
      "Ground truth salary: 0.263. Predicted salary: 0.208\n",
      "Ground truth salary: 0.074. Predicted salary: 0.175\n",
      "Ground truth salary: 0.104. Predicted salary: 0.197\n",
      "Ground truth salary: 0.132. Predicted salary: 0.251\n",
      "Ground truth salary: 0.000. Predicted salary: 0.242\n",
      "Ground truth salary: 0.071. Predicted salary: 0.153\n",
      "Ground truth salary: 0.058. Predicted salary: 0.088\n",
      "Ground truth salary: 0.102. Predicted salary: 0.110\n",
      "Ground truth salary: 0.029. Predicted salary: 0.242\n",
      "Ground truth salary: 0.364. Predicted salary: 0.208\n",
      "Ground truth salary: 0.171. Predicted salary: 0.270\n",
      "Ground truth salary: 0.254. Predicted salary: 0.185\n",
      "Ground truth salary: 0.121. Predicted salary: 0.219\n",
      "Ground truth salary: 0.138. Predicted salary: 0.219\n",
      "Ground truth salary: 0.270. Predicted salary: 0.251\n",
      "Ground truth salary: 0.012. Predicted salary: 0.111\n",
      "Ground truth salary: 0.039. Predicted salary: 0.197\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.110. Predicted salary: 0.185\n",
      "Ground truth salary: 0.086. Predicted salary: 0.175\n",
      "Ground truth salary: 0.087. Predicted salary: 0.144\n",
      "Ground truth salary: 0.349. Predicted salary: 0.208\n",
      "Ground truth salary: 0.106. Predicted salary: 0.220\n",
      "Ground truth salary: 0.673. Predicted salary: 0.185\n",
      "Ground truth salary: 0.043. Predicted salary: 0.111\n",
      "Ground truth salary: 0.102. Predicted salary: 0.176\n",
      "Ground truth salary: 0.125. Predicted salary: 0.220\n",
      "Ground truth salary: 0.191. Predicted salary: 0.175\n",
      "Ground truth salary: 0.079. Predicted salary: 0.208\n",
      "Ground truth salary: 0.310. Predicted salary: 0.185\n",
      "Ground truth salary: 0.005. Predicted salary: 0.132\n",
      "Ground truth salary: 0.171. Predicted salary: 0.134\n",
      "Ground truth salary: 0.238. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.111\n",
      "Ground truth salary: 0.225. Predicted salary: 0.208\n",
      "Ground truth salary: 0.100. Predicted salary: 0.185\n",
      "Ground truth salary: 0.345. Predicted salary: 0.208\n",
      "Ground truth salary: 0.230. Predicted salary: 0.251\n",
      "Ground truth salary: 0.154. Predicted salary: 0.207\n",
      "Ground truth salary: 0.099. Predicted salary: 0.197\n",
      "Ground truth salary: 0.102. Predicted salary: 0.219\n",
      "Ground truth salary: 0.339. Predicted salary: 0.251\n",
      "Ground truth salary: 0.141. Predicted salary: 0.153\n",
      "Ground truth salary: 0.072. Predicted salary: 0.219\n",
      "Ground truth salary: 0.196. Predicted salary: 0.208\n",
      "Ground truth salary: 0.133. Predicted salary: 0.153\n",
      "Ground truth salary: 0.072. Predicted salary: 0.231\n",
      "Ground truth salary: 0.250. Predicted salary: 0.185\n",
      "Ground truth salary: 0.236. Predicted salary: 0.207\n",
      "Ground truth salary: 0.100. Predicted salary: 0.155\n",
      "Ground truth salary: 0.318. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.208\n",
      "Ground truth salary: 0.345. Predicted salary: 0.185\n",
      "Ground truth salary: 0.152. Predicted salary: 0.207\n",
      "Ground truth salary: 0.029. Predicted salary: 0.207\n",
      "Ground truth salary: 0.112. Predicted salary: 0.175\n",
      "Ground truth salary: 0.094. Predicted salary: 0.088\n",
      "Ground truth salary: 0.124. Predicted salary: 0.198\n",
      "Ground truth salary: 0.029. Predicted salary: 0.198\n",
      "Ground truth salary: 0.133. Predicted salary: 0.198\n",
      "Ground truth salary: 0.124. Predicted salary: 0.132\n",
      "Ground truth salary: 0.090. Predicted salary: 0.220\n",
      "Ground truth salary: 0.339. Predicted salary: 0.229\n",
      "Ground truth salary: 0.015. Predicted salary: 0.219\n",
      "Ground truth salary: 0.330. Predicted salary: 0.176\n",
      "Ground truth salary: 0.096. Predicted salary: 0.197\n",
      "Ground truth salary: 0.372. Predicted salary: 0.185\n",
      "Ground truth salary: 0.094. Predicted salary: 0.176\n",
      "Ground truth salary: 0.173. Predicted salary: 0.219\n",
      "Ground truth salary: 0.011. Predicted salary: 0.134\n",
      "Ground truth salary: 0.098. Predicted salary: 0.208\n",
      "Ground truth salary: 0.352. Predicted salary: 0.208\n",
      "Ground truth salary: 0.075. Predicted salary: 0.207\n",
      "Ground truth salary: 0.286. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.134\n",
      "Ground truth salary: 0.115. Predicted salary: 0.208\n",
      "Ground truth salary: 0.060. Predicted salary: 0.176\n",
      "Ground truth salary: 0.037. Predicted salary: 0.175\n",
      "Ground truth salary: 0.086. Predicted salary: 0.207\n",
      "Ground truth salary: 0.124. Predicted salary: 0.176\n",
      "Ground truth salary: 0.163. Predicted salary: 0.150\n",
      "Ground truth salary: 0.063. Predicted salary: 0.176\n",
      "Ground truth salary: 0.354. Predicted salary: 0.208\n",
      "Ground truth salary: 0.213. Predicted salary: 0.208\n",
      "Ground truth salary: 0.105. Predicted salary: 0.219\n",
      "Ground truth salary: 0.205. Predicted salary: 0.134\n",
      "Ground truth salary: 0.246. Predicted salary: 0.251\n",
      "Ground truth salary: 0.166. Predicted salary: 0.219\n",
      "Ground truth salary: 0.200. Predicted salary: 0.197\n",
      "Ground truth salary: 0.025. Predicted salary: 0.134\n",
      "Ground truth salary: 0.079. Predicted salary: 0.153\n",
      "Ground truth salary: 0.441. Predicted salary: 0.208\n",
      "Ground truth salary: 0.380. Predicted salary: 0.142\n",
      "Ground truth salary: 0.431. Predicted salary: 0.208\n",
      "Ground truth salary: 0.083. Predicted salary: 0.088\n",
      "Ground truth salary: 0.196. Predicted salary: 0.198\n",
      "Ground truth salary: 0.160. Predicted salary: 0.176\n",
      "Ground truth salary: 0.184. Predicted salary: 0.208\n",
      "Ground truth salary: 0.161. Predicted salary: 0.251\n",
      "Ground truth salary: 0.142. Predicted salary: 0.176\n",
      "Ground truth salary: 0.055. Predicted salary: 0.153\n",
      "Ground truth salary: 0.297. Predicted salary: 0.208\n",
      "Ground truth salary: 0.062. Predicted salary: 0.154\n",
      "Ground truth salary: 0.183. Predicted salary: 0.153\n",
      "Ground truth salary: 0.051. Predicted salary: 0.274\n",
      "Ground truth salary: 0.260. Predicted salary: 0.185\n",
      "Ground truth salary: 0.112. Predicted salary: 0.111\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.161. Predicted salary: 0.251\n",
      "Ground truth salary: 0.241. Predicted salary: 0.175\n",
      "Ground truth salary: 0.147. Predicted salary: 0.175\n",
      "Ground truth salary: 0.221. Predicted salary: 0.208\n",
      "Ground truth salary: 0.147. Predicted salary: 0.175\n",
      "Ground truth salary: 0.092. Predicted salary: 0.242\n",
      "Ground truth salary: 0.230. Predicted salary: 0.229\n",
      "Ground truth salary: 0.087. Predicted salary: 0.155\n",
      "Ground truth salary: 0.201. Predicted salary: 0.208\n",
      "Ground truth salary: 0.046. Predicted salary: 0.088\n",
      "Ground truth salary: 0.142. Predicted salary: 0.176\n",
      "Ground truth salary: 0.163. Predicted salary: 0.176\n",
      "Ground truth salary: 0.178. Predicted salary: 0.230\n",
      "Ground truth salary: 0.280. Predicted salary: 0.176\n",
      "Ground truth salary: 0.111. Predicted salary: 0.208\n",
      "Ground truth salary: 0.129. Predicted salary: 0.231\n",
      "Ground truth salary: 0.005. Predicted salary: 0.133\n",
      "Ground truth salary: 0.277. Predicted salary: 0.274\n",
      "Ground truth salary: 0.087. Predicted salary: 0.219\n",
      "Ground truth salary: 0.246. Predicted salary: 0.134\n",
      "Ground truth salary: 0.272. Predicted salary: 0.251\n",
      "Ground truth salary: 0.317. Predicted salary: 0.208\n",
      "Ground truth salary: 0.414. Predicted salary: 0.164\n",
      "Ground truth salary: 0.028. Predicted salary: 0.153\n",
      "Ground truth salary: 0.128. Predicted salary: 0.185\n",
      "Ground truth salary: 0.100. Predicted salary: 0.220\n",
      "Ground truth salary: 0.031. Predicted salary: 0.133\n",
      "Ground truth salary: 0.061. Predicted salary: 0.219\n",
      "Ground truth salary: 0.263. Predicted salary: 0.153\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.171. Predicted salary: 0.208\n",
      "Ground truth salary: 0.154. Predicted salary: 0.207\n",
      "Ground truth salary: 0.156. Predicted salary: 0.198\n",
      "Ground truth salary: 0.112. Predicted salary: 0.088\n",
      "Ground truth salary: 0.309. Predicted salary: 0.153\n",
      "Ground truth salary: 0.305. Predicted salary: 0.207\n",
      "Ground truth salary: 0.140. Predicted salary: 0.133\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.207\n",
      "Ground truth salary: 0.005. Predicted salary: 0.177\n",
      "Ground truth salary: 0.303. Predicted salary: 0.185\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.263. Predicted salary: 0.231\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.380. Predicted salary: 0.208\n",
      "Ground truth salary: 0.113. Predicted salary: 0.251\n",
      "Ground truth salary: 0.072. Predicted salary: 0.111\n",
      "Ground truth salary: 0.076. Predicted salary: 0.198\n",
      "Ground truth salary: 0.185. Predicted salary: 0.208\n",
      "Ground truth salary: 0.258. Predicted salary: 0.153\n",
      "Ground truth salary: 0.263. Predicted salary: 0.252\n",
      "Ground truth salary: 0.079. Predicted salary: 0.231\n",
      "Ground truth salary: 0.431. Predicted salary: 0.274\n",
      "Ground truth salary: 0.196. Predicted salary: 0.229\n",
      "Ground truth salary: 0.127. Predicted salary: 0.251\n",
      "Ground truth salary: 0.275. Predicted salary: 0.176\n",
      "Ground truth salary: 0.211. Predicted salary: 0.251\n",
      "Ground truth salary: 0.692. Predicted salary: 0.228\n",
      "Ground truth salary: 0.032. Predicted salary: 0.133\n",
      "Ground truth salary: 0.152. Predicted salary: 0.208\n",
      "Ground truth salary: 0.121. Predicted salary: 0.198\n",
      "Ground truth salary: 0.128. Predicted salary: 0.142\n",
      "Ground truth salary: 0.177. Predicted salary: 0.251\n",
      "Ground truth salary: 0.246. Predicted salary: 0.208\n",
      "Ground truth salary: 0.372. Predicted salary: 0.088\n",
      "Ground truth salary: 0.300. Predicted salary: 0.251\n",
      "Ground truth salary: 0.196. Predicted salary: 0.219\n",
      "Ground truth salary: 0.246. Predicted salary: 0.121\n",
      "Ground truth salary: 0.106. Predicted salary: 0.176\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.127. Predicted salary: 0.208\n",
      "Ground truth salary: 0.201. Predicted salary: 0.208\n",
      "Ground truth salary: 0.280. Predicted salary: 0.185\n",
      "Ground truth salary: 0.066. Predicted salary: 0.220\n",
      "Ground truth salary: 0.685. Predicted salary: 0.185\n",
      "Ground truth salary: 0.138. Predicted salary: 0.134\n",
      "Ground truth salary: 0.112. Predicted salary: 0.166\n",
      "Ground truth salary: 0.079. Predicted salary: 0.176\n",
      "Ground truth salary: 0.171. Predicted salary: 0.251\n",
      "Ground truth salary: 0.119. Predicted salary: 0.220\n",
      "Ground truth salary: 0.096. Predicted salary: 0.134\n",
      "Ground truth salary: 0.116. Predicted salary: 0.219\n",
      "Ground truth salary: 0.129. Predicted salary: 0.251\n",
      "Ground truth salary: 0.457. Predicted salary: 0.229\n",
      "Ground truth salary: 0.182. Predicted salary: 0.219\n",
      "Ground truth salary: 0.127. Predicted salary: 0.176\n",
      "Ground truth salary: 0.096. Predicted salary: 0.208\n",
      "Ground truth salary: 0.112. Predicted salary: 0.122\n",
      "Ground truth salary: 0.106. Predicted salary: 0.229\n",
      "Ground truth salary: 0.134. Predicted salary: 0.219\n",
      "Ground truth salary: 0.397. Predicted salary: 0.229\n",
      "Ground truth salary: 0.192. Predicted salary: 0.219\n",
      "Ground truth salary: 0.042. Predicted salary: 0.176\n",
      "Ground truth salary: 0.178. Predicted salary: 0.175\n",
      "Ground truth salary: 0.005. Predicted salary: 0.198\n",
      "Ground truth salary: 0.221. Predicted salary: 0.153\n",
      "Ground truth salary: 0.246. Predicted salary: 0.185\n",
      "Ground truth salary: 0.538. Predicted salary: 0.143\n",
      "Ground truth salary: 0.189. Predicted salary: 0.251\n",
      "Ground truth salary: 0.098. Predicted salary: 0.208\n",
      "Ground truth salary: 0.181. Predicted salary: 0.208\n",
      "Ground truth salary: 0.188. Predicted salary: 0.231\n",
      "Ground truth salary: 0.272. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.153\n",
      "Ground truth salary: 0.255. Predicted salary: 0.208\n",
      "Ground truth salary: 0.265. Predicted salary: 0.208\n",
      "Ground truth salary: 0.111. Predicted salary: 0.176\n",
      "Ground truth salary: 0.013. Predicted salary: 0.102\n",
      "Ground truth salary: 0.356. Predicted salary: 0.229\n",
      "Ground truth salary: 0.004. Predicted salary: 0.166\n",
      "Ground truth salary: 0.254. Predicted salary: 0.185\n",
      "Ground truth salary: 0.097. Predicted salary: 0.207\n",
      "Ground truth salary: 0.193. Predicted salary: 0.166\n",
      "Ground truth salary: 0.297. Predicted salary: 0.229\n",
      "Ground truth salary: 0.203. Predicted salary: 0.219\n",
      "Ground truth salary: 0.364. Predicted salary: 0.208\n",
      "Ground truth salary: 0.069. Predicted salary: 0.176\n",
      "Ground truth salary: 0.179. Predicted salary: 0.143\n",
      "Ground truth salary: 0.146. Predicted salary: 0.134\n",
      "Ground truth salary: 0.164. Predicted salary: 0.185\n",
      "Ground truth salary: 0.129. Predicted salary: 0.111\n",
      "Ground truth salary: 0.188. Predicted salary: 0.208\n",
      "Ground truth salary: 0.172. Predicted salary: 0.208\n",
      "Ground truth salary: 0.127. Predicted salary: 0.219\n",
      "Ground truth salary: 0.142. Predicted salary: 0.251\n",
      "Ground truth salary: 0.196. Predicted salary: 0.229\n",
      "Ground truth salary: 0.191. Predicted salary: 0.207\n",
      "Ground truth salary: 0.246. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.219\n",
      "Ground truth salary: 0.065. Predicted salary: 0.134\n",
      "Ground truth salary: 0.062. Predicted salary: 0.088\n",
      "Ground truth salary: 0.069. Predicted salary: 0.176\n",
      "Ground truth salary: 0.063. Predicted salary: 0.197\n",
      "Ground truth salary: 0.230. Predicted salary: 0.229\n",
      "Ground truth salary: 0.193. Predicted salary: 0.176\n",
      "Ground truth salary: 0.029. Predicted salary: 0.228\n",
      "Ground truth salary: 0.184. Predicted salary: 0.208\n",
      "Ground truth salary: 0.221. Predicted salary: 0.153\n",
      "Ground truth salary: 0.084. Predicted salary: 0.207\n",
      "Ground truth salary: 0.100. Predicted salary: 0.197\n",
      "Ground truth salary: 0.161. Predicted salary: 0.219\n",
      "Ground truth salary: 0.189. Predicted salary: 0.208\n",
      "Ground truth salary: 0.117. Predicted salary: 0.088\n",
      "Ground truth salary: 0.313. Predicted salary: 0.251\n",
      "Ground truth salary: 0.364. Predicted salary: 0.251\n",
      "Ground truth salary: 0.330. Predicted salary: 0.208\n",
      "Ground truth salary: 0.330. Predicted salary: 0.208\n",
      "Ground truth salary: 0.318. Predicted salary: 0.208\n",
      "Ground truth salary: 0.211. Predicted salary: 0.208\n",
      "Ground truth salary: 0.127. Predicted salary: 0.176\n",
      "Ground truth salary: 0.096. Predicted salary: 0.185\n",
      "Ground truth salary: 0.163. Predicted salary: 0.102\n",
      "Ground truth salary: 0.161. Predicted salary: 0.208\n",
      "Ground truth salary: 0.236. Predicted salary: 0.208\n",
      "Ground truth salary: 0.230. Predicted salary: 0.153\n",
      "Ground truth salary: 0.272. Predicted salary: 0.251\n",
      "Ground truth salary: 0.263. Predicted salary: 0.251\n",
      "Ground truth salary: 0.263. Predicted salary: 0.208\n",
      "Ground truth salary: 0.389. Predicted salary: 0.185\n",
      "Ground truth salary: 0.036. Predicted salary: 0.185\n",
      "Ground truth salary: 0.539. Predicted salary: 0.121\n",
      "Ground truth salary: 0.133. Predicted salary: 0.110\n",
      "Ground truth salary: 0.011. Predicted salary: 0.242\n",
      "Ground truth salary: 0.188. Predicted salary: 0.208\n",
      "Ground truth salary: 0.134. Predicted salary: 0.230\n",
      "Ground truth salary: 0.159. Predicted salary: 0.219\n",
      "Ground truth salary: 0.248. Predicted salary: 0.153\n",
      "Ground truth salary: 0.160. Predicted salary: 0.219\n",
      "Ground truth salary: 0.190. Predicted salary: 0.208\n",
      "Ground truth salary: 0.272. Predicted salary: 0.185\n",
      "Ground truth salary: 0.102. Predicted salary: 0.207\n",
      "Ground truth salary: 0.087. Predicted salary: 0.185\n",
      "Ground truth salary: 0.263. Predicted salary: 0.176\n",
      "Ground truth salary: 0.106. Predicted salary: 0.176\n",
      "Ground truth salary: 0.272. Predicted salary: 0.185\n",
      "Ground truth salary: 0.062. Predicted salary: 0.153\n",
      "Ground truth salary: 0.116. Predicted salary: 0.088\n",
      "Ground truth salary: 0.165. Predicted salary: 0.208\n",
      "Ground truth salary: 0.017. Predicted salary: 0.242\n",
      "Ground truth salary: 0.105. Predicted salary: 0.219\n",
      "Ground truth salary: 0.059. Predicted salary: 0.153\n",
      "Ground truth salary: 0.179. Predicted salary: 0.242\n",
      "Ground truth salary: 0.043. Predicted salary: 0.124\n",
      "Ground truth salary: 0.009. Predicted salary: 0.175\n",
      "Ground truth salary: 0.189. Predicted salary: 0.176\n",
      "Ground truth salary: 0.302. Predicted salary: 0.219\n",
      "Ground truth salary: 0.029. Predicted salary: 0.154\n",
      "Ground truth salary: 0.297. Predicted salary: 0.153\n",
      "Ground truth salary: 0.171. Predicted salary: 0.208\n",
      "Ground truth salary: 0.072. Predicted salary: 0.198\n",
      "Ground truth salary: 0.195. Predicted salary: 0.176\n",
      "Ground truth salary: 0.246. Predicted salary: 0.229\n",
      "Ground truth salary: 0.207. Predicted salary: 0.208\n",
      "Ground truth salary: 0.156. Predicted salary: 0.207\n",
      "Ground truth salary: 0.169. Predicted salary: 0.185\n",
      "Ground truth salary: 0.325. Predicted salary: 0.142\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.632. Predicted salary: 0.185\n",
      "Ground truth salary: 0.015. Predicted salary: 0.103\n",
      "Ground truth salary: 0.024. Predicted salary: 0.198\n",
      "Ground truth salary: 0.122. Predicted salary: 0.153\n",
      "Ground truth salary: 0.330. Predicted salary: 0.143\n",
      "Ground truth salary: 0.032. Predicted salary: 0.088\n",
      "Ground truth salary: 0.356. Predicted salary: 0.185\n",
      "Ground truth salary: 0.107. Predicted salary: 0.154\n",
      "Ground truth salary: 0.089. Predicted salary: 0.175\n",
      "Ground truth salary: 0.050. Predicted salary: 0.176\n",
      "Ground truth salary: 0.022. Predicted salary: 0.103\n",
      "Ground truth salary: 0.060. Predicted salary: 0.176\n",
      "Ground truth salary: 0.188. Predicted salary: 0.197\n",
      "Ground truth salary: 0.749. Predicted salary: 0.153\n",
      "Ground truth salary: 0.203. Predicted salary: 0.208\n",
      "Ground truth salary: 0.049. Predicted salary: 0.173\n",
      "Ground truth salary: 0.119. Predicted salary: 0.175\n",
      "Ground truth salary: 0.400. Predicted salary: 0.143\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.073. Predicted salary: 0.153\n",
      "Average Loss: 0.0141\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "torch.manual_seed(1) # for 01\n",
    "\n",
    "salaries = pd.read_csv(\"datasets/ds_salaries.csv\")\n",
    "target_df = salaries[\"salary_in_usd\"]\n",
    "features_df = salaries[[\"experience_level\", \"employment_type\", \"remote_ratio\", \"company_size\"]]\n",
    "features_df.loc[:, \"company_size\"] = features_df[\"company_size\"].astype(\"category\").cat.codes\n",
    "features_df.loc[:, \"experience_level\"] = features_df[\"experience_level\"].astype(\"category\").cat.codes\n",
    "features_df.loc[:, \"employment_type\"] = features_df[\"employment_type\"].astype(\"category\").cat.codes\n",
    "features = features_df.to_numpy(dtype='float32')\n",
    "scaler = MinMaxScaler()\n",
    "target = scaler.fit_transform(target_df.values.reshape(-1, 1))\n",
    "target = target.astype(np.float32)\n",
    "\n",
    "# Create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features), torch.tensor(target))\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = nn.Sequential(nn.Linear(4, 2),                       \n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "# Create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop over the number of epochs and the dataloader\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Get feature and target from the data loader\n",
    "        feature, target = data\n",
    "        # Run a forward pass\n",
    "        pred = model(feature)\n",
    "        # Compute loss and gradients\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c57ad2-d06d-422e-b206-fd8ede58f020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330becc4-4f8c-45a0-bb86-ffd46ed51bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
