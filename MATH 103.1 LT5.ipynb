{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aabeb70-151d-44bf-8c94-f51c004b8cf4",
   "metadata": {},
   "source": [
    "#  Long Exam 5 - On Deep Learning Methods\n",
    "\n",
    "## Name: Yumol, Dianne\n",
    "\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "Save this notebook with your answers/codes and submit here in canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33862e7-95d6-4536-a279-41ccd6640966",
   "metadata": {},
   "source": [
    "## Problem 1. Preprocessing text  (20 points)\n",
    "\n",
    "As a Data Analyst at PyBooks, you're on the trail of mastering text preprocessing, and what better practice text to tackle than text from Sherlock Holmes. Your task is to preprocess a block of text using the various techniques presented in the video in order to prepare it for further analysis.\n",
    "\n",
    "The text variable is an excerpt from The Hound of the Baskervilles by Arther Conan Doyle.\n",
    "\n",
    "The following packages and functions will be used: nltk, `torch`, `get_tokenizer`, `PorterStemmer`, `stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a45cd7-d587-40ce-b3fa-47a1a310828c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1701249329967,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#!pip install torchtext",
    "outputsMetadata": {
     "0": {
      "height": 613,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torchtext) (1.21.5)\n",
      "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: torch==2.1.1 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torchtext) (2.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (4.3.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (3.6.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from requests->torchtext) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1->torchtext) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1->torchtext) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73dc2cc0-025c-4892-ac7d-0dcda1fd7a5e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 2138,
    "lastExecutedAt": 1701249268573,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import nltk\nimport torch\nimport torchtext\n\nnltk.download('stopwords')\n\nfrom torchtext.data.utils import get_tokenizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords",
    "outputsMetadata": {
     "0": {
      "height": 56,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd80adc-8a01-4e20-aa92-df569d920b46",
   "metadata": {},
   "source": [
    "###  Instructions: Fill in the blanks and use the text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0151fb2-4703-4e9f-ab12-cab3fdbf552a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1701249268622,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "text = 'The moor is very sparsely inhabited, and those who live near each other are thrown very much together. For this reason I saw a good deal of Sir Charles Baskerville. With the exception of Mr. Frankland, of Lafter Hall, and Mr. Stapleton, the naturalist, there are no other men of education within many miles. Sir Charles was a retiring man, but the chance of his illness brought us together, and a community of interests in science kept us so. He had brought back much scientific information from South Africa, and many a charming evening we have spent together discussing the comparative anatomy of the Bushman and the Hottentot.'\n"
   },
   "outputs": [],
   "source": [
    "text = 'The moor is very sparsely inhabited, and those who live near each other are thrown very much together. For this reason I saw a good deal of Sir Charles Baskerville. With the exception of Mr. Frankland, of Lafter Hall, and Mr. Stapleton, the naturalist, there are no other men of education within many miles. Sir Charles was a retiring man, but the chance of his illness brought us together, and a community of interests in science kept us so. He had brought back much scientific information from South Africa, and many a charming evening we have spent together discussing the comparative anatomy of the Bushman and the Hottentot.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2047a9e-0a92-4dfe-9a14-a84a2bafe759",
   "metadata": {},
   "source": [
    "### 1.1   Initialize the tokenizer with \"basic_english\" and tokenize the text using the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f35db41-4733-43b0-b93a-157ce8965972",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1701249268671,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Initialize and tokenize the text\n#tokenizer = __________(\"basic_english\")\n#tokens = _______(text)\n#print(tokens)\n\ntokenizer = get_tokenizer(\"basic_english\")\ntokens = tokenizer(text)\nprint(tokens)\n",
    "outputsMetadata": {
     "0": {
      "height": 232,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'moor', 'is', 'very', 'sparsely', 'inhabited', ',', 'and', 'those', 'who', 'live', 'near', 'each', 'other', 'are', 'thrown', 'very', 'much', 'together', '.', 'for', 'this', 'reason', 'i', 'saw', 'a', 'good', 'deal', 'of', 'sir', 'charles', 'baskerville', '.', 'with', 'the', 'exception', 'of', 'mr', '.', 'frankland', ',', 'of', 'lafter', 'hall', ',', 'and', 'mr', '.', 'stapleton', ',', 'the', 'naturalist', ',', 'there', 'are', 'no', 'other', 'men', 'of', 'education', 'within', 'many', 'miles', '.', 'sir', 'charles', 'was', 'a', 'retiring', 'man', ',', 'but', 'the', 'chance', 'of', 'his', 'illness', 'brought', 'us', 'together', ',', 'and', 'a', 'community', 'of', 'interests', 'in', 'science', 'kept', 'us', 'so', '.', 'he', 'had', 'brought', 'back', 'much', 'scientific', 'information', 'from', 'south', 'africa', ',', 'and', 'many', 'a', 'charming', 'evening', 'we', 'have', 'spent', 'together', 'discussing', 'the', 'comparative', 'anatomy', 'of', 'the', 'bushman', 'and', 'the', 'hottentot', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize and tokenize the text\n",
    "### 1.1.1 Answer \n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "### 1.1.2 Answer \n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb43277-c7ca-45e4-9c7f-2d1a7d2f26bc",
   "metadata": {},
   "source": [
    "### 1.2  Create a set of English stopwords and use list comprehension to filter these `stop_words` out of the text, making sure to ignore capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6246d3b-4ea3-4e9a-99dd-fa74e901b7f1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1701249382510,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "## Create a set of English stopwords \n#stop_words = set(stopwords.____(\"english\"))\n\n## Remove any stopwords ignoring capitalization\n#filtered_tokens = [token for token in tokens if token.____ not in ____]\n#print(filtered_tokens)\n\n#stop_words = set(stopwords.words(\"english\"))\n#filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n#print(filtered_tokens)",
    "outputsMetadata": {
     "0": {
      "height": 173,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moor', 'sparsely', 'inhabited', ',', 'live', 'near', 'thrown', 'much', 'together', '.', 'reason', 'saw', 'good', 'deal', 'sir', 'charles', 'baskerville', '.', 'exception', 'mr', '.', 'frankland', ',', 'lafter', 'hall', ',', 'mr', '.', 'stapleton', ',', 'naturalist', ',', 'men', 'education', 'within', 'many', 'miles', '.', 'sir', 'charles', 'retiring', 'man', ',', 'chance', 'illness', 'brought', 'us', 'together', ',', 'community', 'interests', 'science', 'kept', 'us', '.', 'brought', 'back', 'much', 'scientific', 'information', 'south', 'africa', ',', 'many', 'charming', 'evening', 'spent', 'together', 'discussing', 'comparative', 'anatomy', 'bushman', 'hottentot', '.']\n"
     ]
    }
   ],
   "source": [
    "## Create a set of English stopwords \n",
    "### 1.2.1 Answer \n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove any stopwords ignoring capitalization\n",
    "### 1.2.2 Answer \n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe284c9-165c-4ee7-8030-55c1b38ff793",
   "metadata": {},
   "source": [
    "###  1.3  Perform stemming on the filtered_tokens using the appropriate `nltk` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae83b4e6-7fa1-4ae5-b46c-90483acd150b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1701249401026,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "## Perform stemming on the filtered tokens\n#stemmer = ____()\n#stemmed_tokens = [stemmer.____(token) for token in filtered_tokens]\n#print(stemmed_tokens)\n\n## Perform stemming on the filtered tokens\n#stemmer = PorterStemmer()\n#stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n#print(stemmed_tokens)",
    "outputsMetadata": {
     "0": {
      "height": 154,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moor', 'spars', 'inhabit', ',', 'live', 'near', 'thrown', 'much', 'togeth', '.', 'reason', 'saw', 'good', 'deal', 'sir', 'charl', 'baskervil', '.', 'except', 'mr', '.', 'frankland', ',', 'lafter', 'hall', ',', 'mr', '.', 'stapleton', ',', 'naturalist', ',', 'men', 'educ', 'within', 'mani', 'mile', '.', 'sir', 'charl', 'retir', 'man', ',', 'chanc', 'ill', 'brought', 'us', 'togeth', ',', 'commun', 'interest', 'scienc', 'kept', 'us', '.', 'brought', 'back', 'much', 'scientif', 'inform', 'south', 'africa', ',', 'mani', 'charm', 'even', 'spent', 'togeth', 'discuss', 'compar', 'anatomi', 'bushman', 'hottentot', '.']\n"
     ]
    }
   ],
   "source": [
    "# Perform stemming on the filtered tokens\n",
    "### 1.3.1 Answer\n",
    "stemmer = PorterStemmer()\n",
    "### 1.3.2 Answer \n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58485a6a-b59a-4d5f-866d-f04a6146b307",
   "metadata": {},
   "source": [
    "## Problem 2. Creating a Shakespearean language encoder  (25 points)\n",
    "\n",
    "Over at PyBooks, the team wants to transform a vast library of Shakespearean text data for further analysis. The most efficient way to do this is with a text processing pipeline, starting with the preprocessing steps which you have done.\n",
    "\n",
    "With the *preprocessed Shakespearean text* at your fingertips, you now face the challenge of encoding it into a numerical representation. This means, it's time to define the encoding steps before putting the pipeline together. To better handle large amounts of data and efficiently perform the encoding, you will use PyTorch's Dataset and DataLoader for batching and shuffling the data.\n",
    "\n",
    "The processed Shakespearean text data is saved as `processed_shakespeare_df` and the `processed_sentences` have already been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53017ae1-8b7e-4066-bd0d-de94ffd4680c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1701249268818,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data.dataloader import DataLoader"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9dc3f7b-1dbf-42dc-a211-5be410647357",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1701249268866,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\n\n# Define the path to the dataset\ndataset_path = os.path.join(\"Datasets\", \"shakespeare_complete_works.txt\")\n\n# Read the dataset file\nwith open(dataset_path, \"r\") as file:\n    shakespeare = file.readlines()\n\n# Create a list of stopwords\nstop_words = set(stopwords.words(\"english\"))\n\n# Initialize the tokenizer and stemmer\ntokenizer = get_tokenizer(\"basic_english\")\nstemmer = PorterStemmer() ",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# I updated the code so that I didn't encounter any errors\n",
    "dataset_path = os.path.join(\"Datasets\", \"shakespeare_complete_works.txt\")\n",
    "\n",
    "# Read the dataset file with explicit encoding\n",
    "with open(dataset_path, \"r\", errors=\"ignore\") as file:\n",
    "    shakespeare = file.readlines()\n",
    "\n",
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37806552-d874-4ad2-b24d-1dbd1fe3bc10",
   "metadata": {},
   "source": [
    "### 2.1  Define a ShakespeareDataset dataset class and complete the `__init__` and `__getitem__` methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea121437-ce1f-47ce-8022-062f0a26052f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1701249430999,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "## Define your Dataset class\n#class ShakespeareDataset(Dataset):\n#    def __init__(self, data):\n#        self.data = ____\n#    def __len__(self):\n#        return len(self.data)\n#    def __getitem__(self, idx):\n#        return self.data[____]\n    \n    \n## Define your Dataset class\n#class ShakespeareDataset(Dataset):\n#    def __init__(self, data):\n#        self.data = data\n#    def __len__(self):\n#        return len(self.data)\n#    def __getitem__(self, idx):\n#        return self.data[idx]"
   },
   "outputs": [],
   "source": [
    "## Define your Dataset class\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        ### 2.1.1 Answer \n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        ### 2.1.2 Answer \n",
    "        return self.data[idx]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe11ed-a763-4784-8d83-a74e3b324fdb",
   "metadata": {},
   "source": [
    "### 2.2 Complete the `preprocess_sentences()` function to enable tokenization, stop word removal, and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e70122-c92c-420c-8722-a32023853050",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1701249462378,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Complete the function to preprocess sentences\n#def preprocess_sentences(sentences):\n#    processed_sentences = []\n#    for sentence in sentences:\n#        sentence = sentence.lower()\n#        tokens = tokenizer(sentence)\n#        tokens = [token for token in tokens if token not in stop_words]\n#        tokens = [stemmer.stem(token) for token in tokens]\n#        processed_sentences.append(' '.join(tokens))\n#    return processed_sentences\n\n#def preprocess_sentences(sentences):\n#    processed_sentences = []\n#    for sentence in sentences:\n#        #sentence = _________.lower()\n#        #tokens = tokenizer(______)\n#        #tokens = [token for token in tokens if token not in stop_words]\n#        #tokens = [stemmer.stem(token) for token in _____]\n#        processed_sentences.append(' '.join(tokens))\n#    return processed_sentences"
   },
   "outputs": [],
   "source": [
    "# Complete the function to preprocess sentences\n",
    "\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        ### 2.2.1 Answer \n",
    "        sentence = sentence.lower()\n",
    "        ### 2.2.2 Answer \n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        ### 2.2.3 Answer \n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e68848-06b9-4968-9229-7c1952b61fe7",
   "metadata": {},
   "source": [
    "###  2.3 Complete the `encode_sentences()` function to take in a list of sentences and encode them using the `bag-of-words` technique from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4184770b-036b-4b35-92b6-7b22227c7c44",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1701249487763,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "## Complete the encoding function\n#def encode_sentences(sentences):\n#    vectorizer = CountVectorizer()\n#    X = ____.____(sentences)\n#    return X.toarray(), vectorizer\n\n## Complete the encoding function\n#def encode_sentences(sentences):\n#    vectorizer = CountVectorizer()\n#    X = vectorizer.fit_transform(sentences)\n#    return X.toarray(), vectorizer"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Complete the encoding function\n",
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    ### 2.3 Answer \n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e40d3-2ce0-4b62-8cbf-cad588a6b251",
   "metadata": {},
   "source": [
    "###  2.4  Complete and call the `text_processing_pipeline()` function by using `preprocess_sentences()`, `encode_sentences()`, `ShakespeareDataset` class, and `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c33cea6-1fe2-4daa-85b0-b07ab4bba9ea",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 56,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accurs' 'accurst' 'accurs창' 'accus' 'accusativo' 'accuser창' 'accuseth'\n",
      " 'accustom' 'accustom창' 'accus창']\n"
     ]
    }
   ],
   "source": [
    "# Complete the text processing pipeline\n",
    "def text_processing_pipeline(sentences):\n",
    "    ### 2.4.1 Answer \n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    ### 2.4.2 Answer \n",
    "    dataset = ShakespeareDataset(encoded_sentences)\n",
    "    ### 2.4.3 Answer \n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer\n",
    "\n",
    "dataloader, vectorizer = text_processing_pipeline(shakespeare)\n",
    "\n",
    "# Print the vectorizer's feature names from index=1000 to index=1010\n",
    "print(vectorizer.get_feature_names_out()[1000:1010]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a747b1-d162-4f23-983c-eef3d42006eb",
   "metadata": {},
   "source": [
    "##  Problem 3. Convolutional Neural Networks  (25 points)\n",
    "\n",
    "PyBooks has successfully built a book recommendation engine. Their next task is to implement a sentiment analysis model to understand user reviews and gain insight into book preferences.\n",
    "\n",
    "You'll use a Convolutional Neural Network (CNN) model to classify text data (book reviews) based on their sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b714184-ad7d-4f15-8887-71fad82a6126",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1701249814810,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import torch \nimport torch.nn as nn\nimport torch.nn.functional as F"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55caacd8-7381-4810-94a1-fd9f3dd16a5b",
   "metadata": {},
   "source": [
    "####  3.1   Do the following:\n",
    "\n",
    "*   Initialize the embedding layer in the `__init__()` method.\n",
    "*   Apply the `ReLU` activation to this layer within the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a07002b3-476a-4edd-a937-1d274c28c1b6",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1701249843159,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#class TextClassificationCNN(nn.Module):\n#    def __init__(self, vocab_size, embed_dim):\n#        super(TextClassificationCNN, self).__init__()\n#        # Initialize the embedding layer \n#        self.embedding = nn.________(vocab_size, embed_dim)\n#        self.conv = nn.Conv1d(embed_dim, \n#                              embed_dim, \n#                              kernel_size=3, \n#                              stride=1, \n#                              padding=1)\n#        self.fc = nn.Linear(embed_dim, 2)\n#    \n#    def forward(self, text):\n#        embedded = self.embedding(text).permute(0, 2, 1)\n#        # Pass the embedded text through the convolutional layer and apply a ReLU\n#        conved = self._____(self.conv(text))\n#        conved = conved.mean(dim=2) \n#        return self.fc(conved)\n    \n#class TextClassificationCNN(nn.Module):\n#    def __init__(self, vocab_size, embed_dim):\n#        super(TextClassificationCNN, self).__init__()\n#        # Initialize the embedding layer \n#        self.embedding = nn.Embedding(vocab_size, embed_dim)\n#        self.conv = nn.Conv1d(embed_dim, \n#                              embed_dim, \n#                              kernel_size=3, \n#                              stride=1, \n#                              padding=1)\n#        self.fc = nn.Linear(embed_dim, 2)\n#    \n#    def forward(self, text):\n#        embedded = self.embedding(text).permute(0, 2, 1)\n#        # Pass the embedded text through the convolutional layer and apply a ReLU\n#        conved = F.relu(self.conv(embedded))\n#        conved = conved.mean(dim=2) \n#        return self.fc(conved)"
   },
   "outputs": [],
   "source": [
    "class TextClassificationCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TextClassificationCNN, self).__init__()\n",
    "        # Initialize the embedding layer \n",
    "        ###3.1.1 Answer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, \n",
    "                              embed_dim, \n",
    "                              kernel_size=3, \n",
    "                              stride=1, \n",
    "                              padding=1)\n",
    "        self.relu = nn.ReLU() #Added the ReLU activation module so I can utilize self.relu\n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        # Pass the embedded text through the convolutional layer and apply a ReLU\n",
    "        ### 3.1.2 Answer\n",
    "        conved = self.relu(self.conv(embedded))   \n",
    "        conved = conved.mean(dim=2) \n",
    "        return self.fc(conved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfaf1cc-0447-4d9d-a942-937373f8ac8e",
   "metadata": {},
   "source": [
    "PyBooks now needs to train the model to optimize it for accurate sentiment analysis of book reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7477056-812b-40c2-a5a3-06961facf2c1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14,
    "lastExecutedAt": 1701249854409,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n#  text data for this problem\ndata = [(['I', 'love', 'this', 'book'], 1),\n (['This', 'is', 'an', 'amazing', 'novel'], 1),\n (['I', 'really', 'like', 'this', 'story'], 1),\n (['I', 'do', 'not', 'like', 'this', 'book'], 0),\n (['I', 'hate', 'this', 'novel'], 0),\n (['This', 'is', 'a', 'terrible', 'story'], 0)]\n\nfeature, label = zip(*data)\ntokenized_sentences = list(feature)\n\nunique_words = set()\nvocab = []\n\nfor sentence in tokenized_sentences:\n    for word in sentence:\n        if word not in unique_words:\n            vocab.append(word)\n            unique_words.add(word)\n\nword_to_idx = {word: i for i, word in enumerate(vocab)}",
    "outputsMetadata": {
     "0": {
      "height": 95,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#  text data for this problem\n",
    "data = [(['I', 'love', 'this', 'book'], 1),\n",
    " (['This', 'is', 'an', 'amazing', 'novel'], 1),\n",
    " (['I', 'really', 'like', 'this', 'story'], 1),\n",
    " (['I', 'do', 'not', 'like', 'this', 'book'], 0),\n",
    " (['I', 'hate', 'this', 'novel'], 0),\n",
    " (['This', 'is', 'a', 'terrible', 'story'], 0)]\n",
    "\n",
    "feature, label = zip(*data)\n",
    "tokenized_sentences = list(feature)\n",
    "\n",
    "unique_words = set()\n",
    "vocab = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in unique_words:\n",
    "            vocab.append(word)\n",
    "            unique_words.add(word)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309f78f-c741-49c8-acb7-1a8778f55eff",
   "metadata": {},
   "source": [
    "### 3.2 Define a loss function used for binary classification and save as criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8213f54b-8f47-4da8-8e72-69110f671539",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1701249995199,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "vocab_size = len(vocab)\nembed_dim = 10\nmodel = TextClassificationCNN(vocab_size, embed_dim)\n\n## Define the loss function\n#criterion = nn.____()\n#optimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n\n",
    "outputsMetadata": {
     "0": {
      "height": 56,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "model = TextClassificationCNN(vocab_size, embed_dim)\n",
    "\n",
    "## Define the loss function\n",
    "## 3.2 Answer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d06561-83ac-4a65-883c-f969d7fcf60c",
   "metadata": {},
   "source": [
    "### 3.3  Zero the gradients at the start of the training loop and update the parameters at the end of the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae285378-4e86-4fbd-83fe-75f7b1376d71",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1701250069788,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#for epoch in range(10):\n#    for sentence, label in data:     \n#        # Clear the gradients\n#        model.____()\n#        sentence = torch.LongTensor([word_to_ix.get(w, 0) for w in sentence]).unsqueeze(0) \n#        label = torch.LongTensor([int(label)])\n#        outputs = model(sentence)\n#        loss = criterion(outputs, label)\n#        loss.backward()\n#        # Update the parameters\n#       ____.____()#\n# print('Training complete!')\n\n\n#for epoch in range(10):\n#    for sentence, label in data:     \n#        # Clear the gradients\n#        model.zero_grad()\n#        sentence = torch.LongTensor([word_to_idx.get(w, 0) for w in sentence]).unsqueeze(0) \n#       label = torch.LongTensor([int(label)])\n#        outputs = model(sentence)\n#        loss = criterion(outputs, label)\n#        loss.backward()\n#        # Update the parameters\n#        optimizer.step()\n#print('Training complete!')",
    "outputsMetadata": {
     "0": {
      "height": 37,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.6123\n",
      "Epoch 2: Loss: 0.6016\n",
      "Epoch 3: Loss: 0.5883\n",
      "Epoch 4: Loss: 0.5769\n",
      "Epoch 5: Loss: 0.5634\n",
      "Epoch 6: Loss: 0.5421\n",
      "Epoch 7: Loss: 0.5222\n",
      "Epoch 8: Loss: 0.4992\n",
      "Epoch 9: Loss: 0.4813\n",
      "Epoch 10: Loss: 0.4580\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for sentence, label in data:     \n",
    "        # Clear the gradients\n",
    "        ### 3.3.1 Answer\n",
    "        model.zero_grad()\n",
    "        sentence = torch.LongTensor([word_to_idx.get(w, 0) for w in sentence]).unsqueeze(0)\n",
    "        label = torch.LongTensor([int(label)])\n",
    "        label = torch.nn.functional.one_hot(label, num_classes=2).float() #Added this line to bypass the error\n",
    "        outputs = model(sentence)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        ### 3.3.2 Answer\n",
    "        optimizer.step()\n",
    "    print('Epoch {}: Loss: {:.4f}'.format(epoch+1, loss.item()))\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79b920-2f34-496c-88b7-51f06c27fc5e",
   "metadata": {},
   "source": [
    "### 3.4 Testing the Sentiment Analysis CNN Model\n",
    "\n",
    "Now that model is trained, PyBooks wants to check its performance on some new book reviews.\n",
    "\n",
    "You need to check if the sentiment in a review is `positive` or `negative`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80c3e519-c82d-45b6-8f6b-4c3dc6abb9ba",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 51,
    "lastExecutedAt": 1701250753496,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "book_reviews = [\n    \"I love this book\".split(),\n    \"I do not like this book\".split()\n]\n\n#for review in book_reviews:\n#    # Convert the review words into tensor form\n#    input_tensor = torch.____([word_to_ix[w] for w in review], dtype=torch.long).unsqueeze(0) \n#    # Get the model's output\n#    outputs = model(____)\n#    # Find the index of the most likely sentiment category\n#    _, predicted_label = torch.____(outputs.data, 1)\n#    # Convert the predicted label into a sentiment string\n#    sentiment = \"Positive\" if ____ else \"Negative\"\n#    print(f\"Book Review: {' '.join(review)}\")\n#    print(f\"Sentiment: {sentiment}\\n\")\n    \nfor review in book_reviews:\n    # Convert the review words into tensor form\n    input_tensor = torch.tensor([word_to_idx[w] for w in review], dtype=torch.long).unsqueeze(0) \n    # Get the model's output for the input tensor\n    outputs = model(input_tensor)\n    # Extract the predicted label from the outputs tensor\n    _, predicted_label = torch.max(outputs.data, 1)\n    # Convert the predicted label into a sentiment string\n    sentiment = \"Positive\" if predicted_label.item() == 1 else \"Negative\"\n    print(f\"Book Review: {' '.join(review)}\")\n    print(f\"Sentiment: {sentiment}\\n\")",
    "outputsMetadata": {
     "0": {
      "height": 134,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Review: I love this book\n",
      "Sentiment: Positive\n",
      "\n",
      "Book Review: I do not like this book\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_reviews = [\n",
    "    \"I love this book\".split(),\n",
    "    \"I do not like this book\".split()\n",
    "]\n",
    "\n",
    "for review in book_reviews:\n",
    "    # Convert the review words into tensor form\n",
    "    input_tensor = torch.LongTensor([word_to_idx[w] for w in review]).unsqueeze(0)\n",
    "    \n",
    "    # Get the model's output\n",
    "    outputs = model(input_tensor)\n",
    "    \n",
    "    # Find the index of the most likely sentiment category\n",
    "    _, predicted_label = torch.max(outputs.data, 1)\n",
    "    \n",
    "    # Convert the predicted label into a sentiment string\n",
    "    sentiment = \"Positive\" if predicted_label.item() == 1 else \"Negative\"\n",
    "    \n",
    "    print(f\"Book Review: {' '.join(review)}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d570a5-bb45-4918-af01-f53d55adc7a3",
   "metadata": {},
   "source": [
    "##  Problem 4  Long Short Term Memory  (30 points)\n",
    "\n",
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f29e8c0-748e-4f28-822c-5bb0fcfdfbcf",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1379,
    "lastExecutedAt": 1701250764473,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#!pip install scikit-learn\n\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Specify the categories you want to download. You can also use 'all' to get all categories.\ncategories = ['rec.autos', 'sci.med', 'comp.graphics']\n\n# Fetch the dataset\nnewsgroups = fetch_20newsgroups(categories=categories, shuffle=True, random_state=42)\ny = newsgroups.target\n\n# Specify the categories you want to download. You can also use 'all' to get all categories.\ncategories = ['rec.autos', 'sci.med', 'comp.graphics']\n\n# Load and preprocess the dataset\nnewsgroups = fetch_20newsgroups(categories=categories, remove=('headers', 'footers', 'quotes'))\n\n# news_data = newsgroups.data\nnewsgroups_train = fetch_20newsgroups(subset='train')\n#newsgroups_train"
   },
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Specify the categories you want to download. You can also use 'all' to get all categories.\n",
    "categories = ['rec.autos', 'sci.med', 'comp.graphics']\n",
    "\n",
    "# Fetch the dataset\n",
    "newsgroups = fetch_20newsgroups(categories=categories, shuffle=True, random_state=42)\n",
    "y = newsgroups.target\n",
    "\n",
    "# Specify the categories you want to download. You can also use 'all' to get all categories.\n",
    "categories = ['rec.autos', 'sci.med', 'comp.graphics']\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "newsgroups = fetch_20newsgroups(categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# news_data = newsgroups.data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "#newsgroups_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "532fc9f3-ea91-4432-8c00-9358de6609b5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 16,
    "lastExecutedAt": 1701250768272,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import re\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\ndef set_clean(raw_text):\n    set_stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer()\n    \n    no_punc = re.sub(r'[^\\w\\s]', '', raw_text)\n    lowercase_no_punc = no_punc.lower()\n    tokenized_text= word_tokenize(lowercase_no_punc)\n    no_stop = [w for w in tokenized_text if w not in set_stop_words]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"v\") for word in no_stop]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"n\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"a\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"r\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"s\") for word in lc_text]\n    return(lc_text)",
    "outputsMetadata": {
     "0": {
      "height": 173,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def set_clean(raw_text):\n",
    "    set_stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    no_punc = re.sub(r'[^\\w\\s]', '', raw_text)\n",
    "    lowercase_no_punc = no_punc.lower()\n",
    "    tokenized_text= word_tokenize(lowercase_no_punc)\n",
    "    no_stop = [w for w in tokenized_text if w not in set_stop_words]\n",
    "    lc_text = [lemmatizer.lemmatize(word, pos=\"v\") for word in no_stop]\n",
    "    lc_text = [lemmatizer.lemmatize(word, pos=\"n\") for word in lc_text]\n",
    "    lc_text = [lemmatizer.lemmatize(word, pos=\"a\") for word in lc_text]\n",
    "    lc_text = [lemmatizer.lemmatize(word, pos=\"r\") for word in lc_text]\n",
    "    lc_text = [lemmatizer.lemmatize(word, pos=\"s\") for word in lc_text]\n",
    "    return(lc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd4a1de9-0942-4677-83f7-44e45e9adb64",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1701250773070,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# parameters \ninput_size = 21500\nhidden_size = 32\nnum_layers = 2\nnum_classes = 3"
   },
   "outputs": [],
   "source": [
    "# parameters \n",
    "input_size = 21500\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d4248-58ea-4bb2-a3b7-66380ea07f57",
   "metadata": {},
   "source": [
    "###  4.1 Tokenize and vectorize the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9a74c5a-f3e5-4cc1-9480-f66abb055fc8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3451,
    "lastExecutedAt": 1701250824889,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import normalize\n\n#  Tokenize and vectorize the text data:\nvectorizer = CountVectorizer(analyzer=set_clean, max_features=input_size)  # Adjust the \n\n#fit and transform the vectorizer\nbow_matrix = vectorizer.fit_transform(newsgroups.data)\n\n\n##  Tokenize and vectorize the text data:\n#vectorizer = __________(analyzer=set_clean, max_features=input_size)  # Adjust the \n#\n##fit and transform the vectorizer\n#bow_matrix = vectorizer._________(newsgroups.data)\n\n\n# normalize all predictors\nX = normalize(bow_matrix, norm='l2')\n"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#  Tokenize and vectorize the text data:\n",
    "### 4.1.1 Answer\n",
    "vectorizer = CountVectorizer(analyzer=set_clean, max_features=input_size)\n",
    "\n",
    "#fit and transform the vectorizer\n",
    "### 4.1.2 Answer\n",
    "bow_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "\n",
    "# normalize all predictors\n",
    "X = normalize(bow_matrix, norm='l2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52d0a8-ded4-43a4-a285-eb77a4184d67",
   "metadata": {},
   "source": [
    "###  4.2 Split the text data into train and test sets and convert them to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0e065c7-78dd-4f85-9738-2c8acc54e1fc",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 141,
    "lastExecutedAt": 1701250832571,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Split into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321)\nX_train_seq = torch.tensor(X_train.toarray()).unsqueeze(1).to(torch.float32)\nX_test_seq = torch.tensor(X_test.toarray()).unsqueeze(1).to(torch.float32)\ny_train_seq = torch.tensor(y_train).to(torch.long)\ny_test_seq = torch.tensor(y_test).to(torch.long)\n\n## Split into training and testing\n#X_train, X_test, y_train, y_test = ________________(X, y, test_size=0.2, random_state=321)\n#X_train_seq = torch.tensor(X_train.toarray()).unsqueeze(1).to(torch.float32)\n#X_test_seq = torch.tensor(X_test._________.unsqueeze(1).to(torch.float32)\n#y_train_seq = torch.tensor(y_train).to(torch.long)\n#y_test_seq = torch.tensor(y_test).____(torch.long)"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing\n",
    "### 4.2.1 Answer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321)\n",
    "X_train_seq = torch.tensor(X_train.toarray()).unsqueeze(1).to(torch.float32)\n",
    "### 4.2.2 Answer\n",
    "X_test_seq = torch.tensor(X_test.toarray()).unsqueeze(1).to(torch.float32)\n",
    "y_train_seq = torch.tensor(y_train).to(torch.long)\n",
    "### 4.2.3 Answer\n",
    "y_test_seq = torch.tensor(y_test).to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2928c2-990f-45aa-98e5-9f2bf0eb28e4",
   "metadata": {},
   "source": [
    "### 4.3  Set up the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef0f64a0-73d7-4685-a5d4-8e6b72d2d454",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1701250840717,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Set up an LSTM model by completing the LSTM and linear layers with the necessary parameters.\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)        \n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n        out, _ = self.lstm(x, (h0, c0))\n        out = out[:, -1, :] \n        out = self.fc(out)\n        return out\n    \n#class LSTMModel(nn.Module):\n#    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n#        super(___________, self).__init__()\n#        self.hidden_size = hidden_size\n#        self.num_layers = __________\n#        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n#        self.fc = nn.________(hidden_size, num_classes)        \n#\n#    def forward(self, x):\n#        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n#        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n#        out, _ = self.lstm(x, (h0, c0))\n#        out = out[:, -1, :] \n#        out = self.fc(out)\n#        return out\n "
   },
   "outputs": [],
   "source": [
    "# Set up an LSTM model by completing the LSTM and linear layers with the necessary parameters.\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        ### 4.3.1 Answer\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        ### 4.3.2 Answer\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        ### 4.3.3 Answer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)       \n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356fab8-e903-4091-8c3c-de6afa78cbbd",
   "metadata": {},
   "source": [
    "###  4.4  Initialize the model with the necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee108150-8506-4a9b-8126-8377620412cc",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 30,
    "lastExecutedAt": 1701250848726,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Initialize model with required parameters\nlstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n\n## Initialize model with required parameters\n#lstm_model = LSTMModel(_________, ________, ________, ________)\n#criterion = nn.CrossEntropyLoss()\n#optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)"
   },
   "outputs": [],
   "source": [
    "# Initialize model with required parameters\n",
    "### 4.4 Answer\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7edf41-2ad7-4a59-9f2e-a1de4396b234",
   "metadata": {},
   "source": [
    "### 4.5 Train the LSTM model resetting the gradients to zero and passing the input data X_train_seq through the model and calculate the loss based on the predicted outputs and the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5dcd33ee-c8a5-47e1-8fcc-31e68d0bf8d5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 407,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.17680732905864716\n",
      "Epoch: 2, Loss: 0.14261922240257263\n",
      "Epoch: 3, Loss: 0.11559505015611649\n",
      "Epoch: 4, Loss: 0.09492459148168564\n",
      "Epoch: 5, Loss: 0.07962517440319061\n",
      "Epoch: 6, Loss: 0.06864938139915466\n",
      "Epoch: 7, Loss: 0.06098884716629982\n",
      "Epoch: 8, Loss: 0.055756185203790665\n",
      "Epoch: 9, Loss: 0.05222829431295395\n",
      "Epoch: 10, Loss: 0.04984162747859955\n",
      "Epoch: 11, Loss: 0.04817504063248634\n",
      "Epoch: 12, Loss: 0.04693993926048279\n",
      "Epoch: 13, Loss: 0.04597131162881851\n",
      "Epoch: 14, Loss: 0.04520771652460098\n",
      "Epoch: 15, Loss: 0.044646967202425\n",
      "Epoch: 16, Loss: 0.04429207742214203\n",
      "Epoch: 17, Loss: 0.04410171136260033\n",
      "Epoch: 18, Loss: 0.043969057500362396\n",
      "Epoch: 19, Loss: 0.04378441721200943\n",
      "Epoch: 20, Loss: 0.04352302476763725\n"
     ]
    }
   ],
   "source": [
    "# Train the model by passing the correct parameters and zeroing the gradient\n",
    "    \n",
    "# Train the model by passing the correct parameters and zeroing the gradifor epoch in range(10): \n",
    "for epoch in range(20): \n",
    "    optimizer.zero_grad()\n",
    "     ### 4.5.1 Answer\n",
    "    outputs = lstm_model(X_train_seq)\n",
    "     ### 4.5.2 Answer\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f5eb3-841d-42d9-aff0-602e0fbbcfda",
   "metadata": {},
   "source": [
    "### 4.6   Evaluating the model's performance using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "790294db-f78e-48bf-998c-deaa1a614cf3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1701250873873,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# !pip install torchmetrics\nimport torchmetrics\nfrom torchmetrics import Accuracy\nfrom torchmetrics import Precision\nfrom torchmetrics import Recall\nfrom torchmetrics import F1Score",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
      "     ------------------------------------- 805.2/805.2 kB 12.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torchmetrics) (1.21.5)\n",
      "Collecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: torch>=1.8.1 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torchmetrics) (2.1.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (63.4.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.3.0)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (3.6.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2022.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (1.10.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from packaging>=17.1->lightning-utilities>=0.8.0->torchmetrics) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\don bosco\\anaconda3\\lib\\site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.2.1)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "import torchmetrics\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import Precision\n",
    "from torchmetrics import Recall\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac63d92c-4cb7-4097-afc3-d3dfdf2ed91e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 52,
    "lastExecutedAt": 1701250944083,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Applying the lstm_model on the test set\ny_pred_lstm = lstm_model(X_test_seq)\n\n# Create an instance of the metrics\naccuracy = Accuracy(task=\"multiclass\", num_classes=3)\nprecision = Precision(task=\"multiclass\", num_classes=3)\nrecall = Recall(task=\"multiclass\", num_classes=3)\nf1 = F1Score(task=\"multiclass\", num_classes=3)\n\n## Calculate metrics for the LSTM model\n#accuracy = accuracy(___________, __________)\n#precision = precision(_________, __________)\n#recall = recall(_________, ________)\n#f1 = f1(__________, ___________)\n\n# Calculate metrics for the LSTM model\naccuracy = accuracy(y_pred_lstm, y_test_seq)\nprecision = precision(y_pred_lstm, y_test_seq)\nrecall = recall(y_pred_lstm, y_test_seq)\nf1 = f1(y_pred_lstm, y_test_seq)\nprint(\"LSTM Model: \\n Accuracy: {},\\n Precision: {},\\n Recall: {},\\n F1 Score: {}\".format(accuracy, precision, recall, f1))\n",
    "outputsMetadata": {
     "0": {
      "height": 115,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model: \n",
      " Accuracy: 0.9239436388015747,\n",
      " Precision: 0.9239436388015747,\n",
      " Recall: 0.9239436388015747,\n",
      " F1 Score: 0.9239436388015747\n"
     ]
    }
   ],
   "source": [
    "# Applying the lstm_model on the test set\n",
    "y_pred_lstm = lstm_model(X_test_seq)\n",
    "\n",
    "# Create an instance of the metrics\n",
    "accuracy_metric = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "precision_metric = Precision(task=\"multiclass\", num_classes=3)\n",
    "recall_metric = Recall(task=\"multiclass\", num_classes=3)\n",
    "f1_metric = F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "## Calculate metrics for the LSTM model\n",
    "### Answer 4.6.1\n",
    "accuracy = accuracy_metric(y_pred_lstm, y_test_seq)\n",
    "### Answer 4.6.2\n",
    "precision = precision_metric(y_pred_lstm, y_test_seq)\n",
    "### Answer 4.6.3\n",
    "recall = recall_metric(y_pred_lstm, y_test_seq)\n",
    "### Answer 4.6.4\n",
    "f1 = f1_metric(y_pred_lstm, y_test_seq)\n",
    "\n",
    "print(\"LSTM Model: \\n Accuracy: {},\\n Precision: {},\\n Recall: {},\\n F1 Score: {}\".format(accuracy, precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
