{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568a6cbe-925a-40e0-aef4-448f267cb7f3",
   "metadata": {},
   "source": [
    "# MATH 103.1 Exam 3 - Transforming texts to numbers\n",
    "Name: Yumol, Dianne\n",
    "\n",
    "\n",
    "**TA**: Load the data set `news_data.csv` and save it to `news_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa29a3a1-3689-4460-9083-55e56556b0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11314 entries, 0 to 11313\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Text      11314 non-null  object\n",
      " 1   Category  11314 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 176.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "news_data = pd.read_csv(\"news_data.csv\")\n",
    "news_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7779b4b-c01b-4eb7-a12d-274993d3b531",
   "metadata": {},
   "source": [
    "## Problem 1. (10 pts)\n",
    "\n",
    "A sample is a row in the data frame. \n",
    "\n",
    "### 1.1 How many samples are there in `news_data`?  \n",
    "\n",
    "**Answer**: 11314\n",
    "\n",
    "### 1.2  How many columns?  \n",
    "\n",
    "**Answer**: 2\n",
    "\n",
    "### 1.3 What are the column names?  \n",
    "\n",
    "**Answer**: Text and Category\n",
    "\n",
    "### 1.4 Are there missing values?  \n",
    "\n",
    "**Answer**: No, there are 0 null values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5148297f-36e6-4388-a0eb-dbcc02462dd2",
   "metadata": {},
   "source": [
    "## Problem 2. (10 pts)\n",
    "\n",
    "Each sample in `news_data` is labelled according to Category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61fbac-a508-4274-bca1-668610d06ec8",
   "metadata": {},
   "source": [
    "### 2.1 Create a pandas Series which is a frequency table of the `Category` column in `news_data`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823f2f7d-26a7-4d56-b56a-daae7b96e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\n",
      "rec.sport.hockey            600\n",
      "soc.religion.christian      599\n",
      "rec.motorcycles             598\n",
      "rec.sport.baseball          597\n",
      "sci.crypt                   595\n",
      "rec.autos                   594\n",
      "sci.med                     594\n",
      "comp.windows.x              593\n",
      "sci.space                   593\n",
      "sci.electronics             591\n",
      "comp.os.ms-windows.misc     591\n",
      "comp.sys.ibm.pc.hardware    590\n",
      "misc.forsale                585\n",
      "comp.graphics               584\n",
      "comp.sys.mac.hardware       578\n",
      "talk.politics.mideast       564\n",
      "talk.politics.guns          546\n",
      "alt.atheism                 480\n",
      "talk.politics.misc          465\n",
      "talk.religion.misc          377\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Answer to 2.1 \n",
    "categories_count = news_data.value_counts(\"Category\")\n",
    "print(categories_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2371764-6685-4dc6-9d8d-7452300b0e74",
   "metadata": {},
   "source": [
    "### 2.2 View some basic statistical details like percentile, mean, std, etc. of `categories_count`?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f898f387-e53a-458e-a2ba-9f5586ac5477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     20.000000\n",
       "mean     565.700000\n",
       "std       58.251813\n",
       "min      377.000000\n",
       "25%      574.500000\n",
       "50%      591.000000\n",
       "75%      594.250000\n",
       "max      600.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer to 2.2\n",
    "import numpy as np\n",
    "categories_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ae8eb2-14ec-4091-be3d-f9f2fd59192d",
   "metadata": {},
   "source": [
    "### 2.3 Which category has the highest count? Which has the lowest count?\n",
    "\n",
    "**Answer**: rec.sport.hockey has the highest count (600) while talk.religion.misc has the smallest count (377)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8c8d6-ea1f-476a-a348-97ca7c725ad4",
   "metadata": {},
   "source": [
    "## Problem 3.  Use the `categories_count` in *Problem 2*. (20 points)\n",
    "\n",
    "### 3.1 Create a pandas DataFrame from `categories_count` with two columns: `Category` and `Count`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a54e2f-d3fe-496e-90bc-5c59cc79d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Category  Count\n",
      "0           rec.sport.hockey    600\n",
      "1     soc.religion.christian    599\n",
      "2            rec.motorcycles    598\n",
      "3         rec.sport.baseball    597\n",
      "4                  sci.crypt    595\n",
      "5                  rec.autos    594\n",
      "6                    sci.med    594\n",
      "7             comp.windows.x    593\n",
      "8                  sci.space    593\n",
      "9            sci.electronics    591\n",
      "10   comp.os.ms-windows.misc    591\n",
      "11  comp.sys.ibm.pc.hardware    590\n",
      "12              misc.forsale    585\n",
      "13             comp.graphics    584\n",
      "14     comp.sys.mac.hardware    578\n",
      "15     talk.politics.mideast    564\n",
      "16        talk.politics.guns    546\n",
      "17               alt.atheism    480\n",
      "18        talk.politics.misc    465\n",
      "19        talk.religion.misc    377\n"
     ]
    }
   ],
   "source": [
    "# Answer to 3.1\n",
    "categories_df = pd.DataFrame({\"Category\": categories_count.index, \"Count\": categories_count.values})\n",
    "categories_df = categories_df.reset_index(drop=True)\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1961c-b888-49e8-9e50-69925cbcdc02",
   "metadata": {},
   "source": [
    "### 3.2 Create a new column called `Branch` in `categories_df`.  \n",
    "\n",
    "Its elements are the respective substring of the elements consisting of characters before the `.` symbol in `Category` column. For example, since the category in the first row is `rec.sport.hockey`, then the first element of `Branch` is `rec`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab662aa3-d2c9-43b2-92a6-37dc778b7bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Category  Count Branch\n",
      "0           rec.sport.hockey    600    rec\n",
      "1     soc.religion.christian    599    soc\n",
      "2            rec.motorcycles    598    rec\n",
      "3         rec.sport.baseball    597    rec\n",
      "4                  sci.crypt    595    sci\n",
      "5                  rec.autos    594    rec\n",
      "6                    sci.med    594    sci\n",
      "7             comp.windows.x    593   comp\n",
      "8                  sci.space    593    sci\n",
      "9            sci.electronics    591    sci\n",
      "10   comp.os.ms-windows.misc    591   comp\n",
      "11  comp.sys.ibm.pc.hardware    590   comp\n",
      "12              misc.forsale    585   misc\n",
      "13             comp.graphics    584   comp\n",
      "14     comp.sys.mac.hardware    578   comp\n",
      "15     talk.politics.mideast    564   talk\n",
      "16        talk.politics.guns    546   talk\n",
      "17               alt.atheism    480    alt\n",
      "18        talk.politics.misc    465   talk\n",
      "19        talk.religion.misc    377   talk\n"
     ]
    }
   ],
   "source": [
    "# Answer to 3.2\n",
    "categories_df[\"Branch\"] = categories_df[\"Category\"].str.split(\".\", expand=True)[0]\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fd386-44c3-4a42-8b6b-aa4ff70d97b9",
   "metadata": {},
   "source": [
    "### 3.3 What are the unique branches and how many in `categories_df`?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e5cfbfb-8f98-4951-b8e3-59782ea7e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique branches: ['rec' 'soc' 'sci' 'comp' 'misc' 'talk' 'alt']\n",
      "number of branches: 7\n"
     ]
    }
   ],
   "source": [
    "# Answer to 3.3\n",
    "print(\"unique branches:\", categories_df[\"Branch\"].unique())\n",
    "print(\"number of branches:\", len(categories_df[\"Branch\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a242a6c-70d6-4d09-bd91-5a3698e3fa69",
   "metadata": {},
   "source": [
    "### 3.4 Sort `categories_df` with respect to `Branch` in ascending order followed by sorting `Count` in descending order.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735bc7ce-25e0-4883-b577-73aef9cd7a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Category  Count Branch\n",
      "17               alt.atheism    480    alt\n",
      "7             comp.windows.x    593   comp\n",
      "10   comp.os.ms-windows.misc    591   comp\n",
      "11  comp.sys.ibm.pc.hardware    590   comp\n",
      "13             comp.graphics    584   comp\n",
      "14     comp.sys.mac.hardware    578   comp\n",
      "12              misc.forsale    585   misc\n",
      "0           rec.sport.hockey    600    rec\n",
      "2            rec.motorcycles    598    rec\n",
      "3         rec.sport.baseball    597    rec\n",
      "5                  rec.autos    594    rec\n",
      "4                  sci.crypt    595    sci\n",
      "6                    sci.med    594    sci\n",
      "8                  sci.space    593    sci\n",
      "9            sci.electronics    591    sci\n",
      "1     soc.religion.christian    599    soc\n",
      "15     talk.politics.mideast    564   talk\n",
      "16        talk.politics.guns    546   talk\n",
      "18        talk.politics.misc    465   talk\n",
      "19        talk.religion.misc    377   talk\n"
     ]
    }
   ],
   "source": [
    "categories_df = categories_df.sort_values([\"Branch\", \"Count\"], ascending=[True, False])\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cc0c5-46f1-4b18-8a78-42f19e3cd684",
   "metadata": {},
   "source": [
    "## Problem 4 Create a subset of `news_data` that are `comp`s in the `Branch` column. (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1cefb-8bb9-4663-94f6-c09b73c1428f",
   "metadata": {},
   "source": [
    "### 4.1 Find the categories that belong to the `comp` branch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "849bf1f5-2291-4ae2-ac19-3167168ef969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7               comp.windows.x\n",
      "10     comp.os.ms-windows.misc\n",
      "11    comp.sys.ibm.pc.hardware\n",
      "13               comp.graphics\n",
      "14       comp.sys.mac.hardware\n",
      "Name: Category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Answer to 4.1\n",
    "comp_branch = categories_df[categories_df.Branch == \"comp\"][\"Category\"]\n",
    "print(comp_branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34c29d-5f85-4cde-92a8-22cc8ce0b623",
   "metadata": {},
   "source": [
    "### Problem 4.2  Create a subset of `news_data` whose samples are in the `comp_branch`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110a201b-b57f-462c-add9-7793593feaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2936 entries, 1 to 11312\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Text      2936 non-null   object\n",
      " 1   Category  2936 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 68.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Answer to 4.2\n",
    "comp_news = news_data[news_data['Category'].isin(comp_branch)]\n",
    "comp_news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5bf63c-457e-4ba8-aeaf-4a4b0c155a0e",
   "metadata": {},
   "source": [
    "### 4.3 Create a frequency table from the `Category` column of `comp_news`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a76ef904-36ae-41f4-b21b-b2c572cf825a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency Table:  Category\n",
      "comp.windows.x              593\n",
      "comp.os.ms-windows.misc     591\n",
      "comp.sys.ibm.pc.hardware    590\n",
      "comp.graphics               584\n",
      "comp.sys.mac.hardware       578\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Answer to 4.3\n",
    "print(\"\\nFrequency Table: \", comp_news.value_counts(\"Category\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981c670-0067-4b96-b2ee-3e51846acc6b",
   "metadata": {},
   "source": [
    "### 4.4 What is the shape of `comp_news` and print its  first five samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d2c6749-c24c-4a2f-821c-32f5824eef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rows = 2936, num_cols = 2\n",
      "\n",
      "First five samples:\n",
      "                                                 Text                  Category\n",
      "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...     comp.sys.mac.hardware\n",
      "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...     comp.sys.mac.hardware\n",
      "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...             comp.graphics\n",
      "7  From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...  comp.sys.ibm.pc.hardware\n",
      "8  From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...   comp.os.ms-windows.misc\n"
     ]
    }
   ],
   "source": [
    "#print(\"shape of comp_news: \", \n",
    "nrows, ncols = comp_news.shape\n",
    "print(f\"num_rows = {nrows}, num_cols = {ncols}\")\n",
    "print(f\"\\nFirst five samples:\\n {comp_news.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500206f9-b24e-44b6-9fb4-fe4c81b9523a",
   "metadata": {},
   "source": [
    "## Problem 5. Clean, lowering case,  tokenize, remove stopwords, and lemmatize the dataset (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5257297-f69d-4c68-8944-5ce821d4edab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Don\n",
      "[nltk_data]     Bosco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182113d0-7d62-49ec-97f8-9dd70298a534",
   "metadata": {},
   "source": [
    "### 5.1 Remove punctuations and lower the case in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b05df0-2a87-4648-a9a1-d4c08b289422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_punct_lowercase_news length: 2936\n",
      "\n",
      " first no_punct_lowercase_news: \n",
      " from guykuocarsonuwashingtonedu guy kuo\n",
      "subject si clock poll  final call\n",
      "summary final call for si clock reports\n",
      "keywords siaccelerationclockupgrade\n",
      "articleid shelley1qvfo9innc3s\n",
      "organization university of washington\n",
      "lines 11\n",
      "nntppostinghost carsonuwashingtonedu\n",
      "\n",
      "a fair number of brave souls who upgraded their si clock oscillator have\n",
      "shared their experiences for this poll please send a brief message detailing\n",
      "your experiences with the procedure top speed attained cpu rated speed\n",
      "add on cards and adapters heat sinks hour of usage per day floppy disk\n",
      "functionality with 800 and 14 m floppies are especially requested\n",
      "\n",
      "i will be summarizing in the next two days so please add to the network\n",
      "knowledge base if you have done the clock upgrade and havent answered this\n",
      "poll thanks\n",
      "\n",
      "guy kuo guykuouwashingtonedu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_punct_lowercase_news = [re.sub(r'[^\\w\\s]', '', text.lower()) for text in  comp_news.Text]\n",
    "\n",
    "print(f\"no_punct_lowercase_news length: {len(no_punct_lowercase_news)}\")\n",
    "print(f\"\\n first no_punct_lowercase_news: \\n {no_punct_lowercase_news[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7736b8-f839-4450-8229-1c8feafff7d3",
   "metadata": {},
   "source": [
    "### 5.2 Tokenize the text in `no_punct_lowercase_news`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feacc099-754f-4637-bbbe-00b5f7366311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_news length: 2936\n",
      "\n",
      " first tokenized news: \n",
      " ['from', 'guykuocarsonuwashingtonedu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', 'siaccelerationclockupgrade', 'articleid', 'shelley1qvfo9innc3s', 'organization', 'university', 'of', 'washington', 'lines', '11', 'nntppostinghost', 'carsonuwashingtonedu', 'a', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '14', 'm', 'floppies', 'are', 'especially', 'requested', 'i', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'havent', 'answered', 'this', 'poll', 'thanks', 'guy', 'kuo', 'guykuouwashingtonedu']\n"
     ]
    }
   ],
   "source": [
    "# Answer to 5.2\n",
    "tokenized_news = [word_tokenize(text) for text in no_punct_lowercase_news]\n",
    "print(f\"tokenized_news length: {len(tokenized_news)}\")\n",
    "print(f\"\\n first tokenized news: \\n {tokenized_news[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f71bd-1137-4807-986c-eca29736b4f8",
   "metadata": {},
   "source": [
    "### 5.3 Remove the stop words in the `tokenized_news`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82a05893-dfdf-4511-bb57-adf0959a3a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_tokenized_news length:  2936\n",
      "\n",
      " first cleaned and tokenized news: \n",
      " ['guykuocarsonuwashingtonedu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'si', 'clock', 'reports', 'keywords', 'siaccelerationclockupgrade', 'articleid', 'shelley1qvfo9innc3s', 'organization', 'university', 'washington', 'lines', '11', 'nntppostinghost', 'carsonuwashingtonedu', 'fair', 'number', 'brave', 'souls', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'experiences', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'cards', 'adapters', 'heat', 'sinks', 'hour', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', '800', '14', 'floppies', 'especially', 'requested', 'summarizing', 'next', 'two', 'days', 'please', 'add', 'network', 'knowledge', 'base', 'done', 'clock', 'upgrade', 'havent', 'answered', 'poll', 'thanks', 'guy', 'kuo', 'guykuouwashingtonedu']\n"
     ]
    }
   ],
   "source": [
    "# Answer to 5.3\n",
    "set_stop_words = set(stopwords.words('english'))\n",
    "cleaned_tokenized_news = []\n",
    "for tokens in tokenized_news:\n",
    "    cleaned_tokenized_news.append([token for token in tokens if token not in set_stop_words])\n",
    "\n",
    "print(\"cleaned_tokenized_news length: \", len(cleaned_tokenized_news))\n",
    "print(\"\\n first cleaned and tokenized news: \\n\", cleaned_tokenized_news[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eb0eed-25f9-4f58-b5cd-8b245dfb6d62",
   "metadata": {},
   "source": [
    "### 5.4 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04e9d503-d2e5-4dad-9634-3ed71dd622f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_news length:  2936\n",
      "\n",
      " first lemma_news: \n",
      " ['guykuocarsonuwashingtonedu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'si', 'clock', 'report', 'keywords', 'siaccelerationclockupgrade', 'articleid', 'shelley1qvfo9innc3s', 'organization', 'university', 'washington', 'line', '11', 'nntppostinghost', 'carsonuwashingtonedu', 'fair', 'number', 'brave', 'soul', 'upgrade', 'si', 'clock', 'oscillator', 'share', 'experience', 'poll', 'please', 'send', 'brief', 'message', 'detail', 'experience', 'procedure', 'top', 'speed', 'attain', 'cpu', 'rat', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', '800', '14', 'floppy', 'especially', 'request', 'summarize', 'next', 'two', 'day', 'please', 'add', 'network', 'knowledge', 'base', 'do', 'clock', 'upgrade', 'havent', 'answer', 'poll', 'thank', 'guy', 'kuo', 'guykuouwashingtonedu']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_news = []\n",
    "for tokens in cleaned_tokenized_news:\n",
    "    lc_news = [lemmatizer.lemmatize(token, pos=\"v\") for token in tokens]\n",
    "    lc_news = [lemmatizer.lemmatize(token, pos=\"n\") for token in lc_news]\n",
    "    lc_news = [lemmatizer.lemmatize(token, pos=\"a\") for token in lc_news]\n",
    "    lc_news = [lemmatizer.lemmatize(token, pos=\"r\") for token in lc_news]\n",
    "    lc_news = [lemmatizer.lemmatize(token, pos=\"s\") for token in lc_news]\n",
    "    lemma_news.append(lc_news)\n",
    "\n",
    "print(\"lemma_news length: \", len(lemma_news))\n",
    "print(\"\\n first lemma_news: \\n\", lemma_news[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ff1a9-c94b-45b2-bf53-175bafb8e013",
   "metadata": {},
   "source": [
    "## Problem 6.  `CountVectorizer` (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb69e46-0aec-4598-8980-8edc0b0bf80f",
   "metadata": {},
   "source": [
    "### 6.1 Create a function which will serve as a value to the `analyzer` parameter of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3021f37-0a7e-4d81-8c97-93b4ebc21c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analyzer(text):\n",
    "    set_stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    no_punct_lowercase_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    tokenized_text = word_tokenize(no_punct_lowercase_text)\n",
    "    no_stopwords = [token for token in tokenized_text if token not in set_stop_words]\n",
    "    lc_text = [lemmatizer.lemmatize(token, pos=\"v\") for token in no_stopwords]\n",
    "    lc_text = [lemmatizer.lemmatize(token, pos=\"n\") for token in lc_text]\n",
    "    lc_text = [lemmatizer.lemmatize(token, pos=\"a\") for token in lc_text]\n",
    "    lc_text = [lemmatizer.lemmatize(token, pos=\"r\") for token in lc_text]\n",
    "    lc_text = [lemmatizer.lemmatize(token, pos=\"s\") for token in lc_text]\n",
    "    return lc_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08ad92-9ceb-4d52-a1a9-09e169f9d78b",
   "metadata": {},
   "source": [
    "### 6.2 Create an instance of `CountVectorizer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ffbe102-9314-4ab9-b98d-260b3d901589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to 6.2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "news_vectorizer = CountVectorizer(analyzer=text_analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fdcc05-07e5-49b9-9641-ed53b3a9c158",
   "metadata": {},
   "source": [
    "### 6.3 Fit and transform `news_vectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f21230a-60c1-4435-a8a3-0fe34ef15d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2936x55472 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 271491 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer to 6.3\n",
    "# Fit and transform the vectorizer to the `comp_news.Text` data\n",
    "news_tokens_bag = news_vectorizer.fit_transform(comp_news.Text)\n",
    "news_tokens_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a10fcd32-6bcf-4c5f-a291-87dc6af820af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density = 0.17%\n"
     ]
    }
   ],
   "source": [
    "x = news_tokens_bag.nnz / (news_tokens_bag.shape[0] * news_tokens_bag.shape[1])\n",
    "print(f\"density = {format(x,'.2%')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e9de1-80ae-435c-875e-79a37335b1a1",
   "metadata": {},
   "source": [
    "### 6.4 Extract column names of the sparse matrix (`news_tokens_bag`) and save them to `features`. There are how many elements in `features`? Print 20 `features`' elements starting from index 29200.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a00fe6c-302e-4b2d-a035-1f6ec328a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55472\n",
      "['matqqs4u7v_orq7uqs4uslqrs2pl7qs4u' 'matrix' 'matrixvector' 'matrox'\n",
      " 'matsbredelludacuuse' 'matsumura' 'matt' 'matt1ry69a1e967f4c8gsejx9kka4'\n",
      " 'matt_harropmagicbbscorpapplecom' 'mattcenterlinecom' 'matteo' 'matter'\n",
      " 'matthew' 'matthew_j_wilsonmcontentapanaorgau'\n",
      " 'matthewalchemytncornelledu' 'matthewdabeefdesedu' 'matthias' 'mattias'\n",
      " 'mattix' 'mattksuksuedu']\n"
     ]
    }
   ],
   "source": [
    "features = news_vectorizer.get_feature_names_out()\n",
    "\n",
    "# The number of features\n",
    "print(len(features))\n",
    "\n",
    "# Twenty elements starting from index 29200\n",
    "print(features[29200:29220])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
