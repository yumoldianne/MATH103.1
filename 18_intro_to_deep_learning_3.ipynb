{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da8180a2-1cdd-41e7-b4fc-8f07329dc97a",
   "metadata": {},
   "source": [
    "# 5.3 Deep Learning Methods, Part 3\n",
    "\n",
    "Prerequisite: \n",
    "\n",
    "*  A basic understanding of the training process for neural networks, including forward pass, loss computation, and backpropagation. \n",
    "*  You should also be able to train and evaluate basic models in PyTorch using Datasets and DataLoaders.\n",
    "\n",
    "Activity:\n",
    "\n",
    "*  We will explore deep learning using PyTorch for text classification.\n",
    "*  We will cover encoding and deep learning models for text using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3823cb3-0bbc-45e3-954c-e48d8a1ccdcd",
   "metadata": {},
   "source": [
    "## 5.3.1 Text processing pipeline in PyTorch\n",
    "\n",
    "![](images/text_processing_pipeline.png)\n",
    "\n",
    "The text analysis approach in PyTorch involves preprocessing, encoding, and Dataset and DataLoader. \n",
    "\n",
    "Given a **raw data**. Then we do the following components in order:\n",
    "\n",
    "1. The first pipeline component is **preprocessing**. We clean and prepare the text data for encoding. We'll use PyTorch and NLTK to transform raw text into processed text.\n",
    "\n",
    "* The first step in text preprocessing is **tokenization**. We'll use the PyTorch `get_tokenizer` function imported from `torchtext..data.utils`. By applying tokenization, our output becomes a list of `tokens`.\n",
    "\n",
    "* The second step is **stop word removal**. We'll eliminate stopwords using NLTK. We will download the stopwords collection of words from nltk using `nltk.download` and import the `stopwords` package. Note that we will be using the **lower** method. We will create a set of stopwords with no duplicates using `stopwords.words`. With list comprehension, we iterate through the `tokens` we previously created and filter out any stopwords. Finally, we print the `filtered tokens`.\n",
    "\n",
    "* The third step is to use **stemming** which reduces words or tokens to their base or root form for simplified analysis. We shall use the NLTK library's `PorterStemmer `package to perform stemming on a set of words or tokens. \n",
    "\n",
    "2. The second pipeline component is **encoding**. The preprocessed text data is given to  ` CountVectorizer` from `sklearn.feature_extraction.text`.\n",
    "\n",
    "3. The pipeline is completed when we use the PyTorch's **Dataset** and **DataLoader**. The `Dataset` serves as a container for our processed and encoded text data. `DataLoader` then allows us to iterate over this dataset in **batches**, **shuffle** the data, and apply **multiprocessing** for efficient loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53dc38a9-4e5a-4a90-92f5-bf8e0aa67b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6335 entries, 8476 to 4330\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   6335 non-null   object\n",
      " 1   text    6335 non-null   object\n",
      " 2   label   6335 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 198.0+ KB\n"
     ]
    }
   ],
   "source": [
    "## 5.3.2  Our text data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "news_df = pd.read_csv(\"datasets/fake_or_real_news.csv\", index_col = \"Unnamed: 0\")\n",
    "\n",
    "news_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288c74b1-1cfb-45a6-b8a3-f36b60b4cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "REAL    3171\n",
       "FAKE    3164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a02028f5-bf89-4fdc-887e-03b2280c6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install dill\n",
    "# !pip install huggingface-hub\n",
    "# !pip install multiprocess\n",
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49f2f82-9890-4541-89d2-e6769016d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Create a list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "## Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7eca0dc-234b-44c1-bc87-3bf7540c6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function to preprocess texts\n",
    "def preprocess_news(data):\n",
    "    processed_news = []\n",
    "    for news in data:\n",
    "        news = news.lower()\n",
    "        tokens = tokenizer(news)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_news.append(' '.join(tokens))\n",
    "    return processed_news\n",
    "\n",
    "# processed_news = preprocess_news(news_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbcfa55d-21ec-4b03-9e77-c7eab9bd81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(processed_news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbe2238-913b-457d-9bfb-b49063595a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_news[0][:274]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6298ea59-85f0-4ca8-bd24-125956f22a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(processed_news))\n",
    "# print(news_df.text.shape)\n",
    "# print(news_df.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58fa70a-edb1-4d54-8e8a-e6c867736224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6335,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = news_df.label\n",
    "encoded_labels = np.array([0 if label == \"FAKE\" else 1 for label in labels])\n",
    "encoded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3255325f-a3dc-4d8c-b419-260014bb1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data.dataset import Dataset\n",
    "# from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "batch_size = 2  # Adjust as needed\n",
    "test_size = 0.2  # Adjust as needed\n",
    "random_state = 42  # Adjust as needed\n",
    "num_classes = 2\n",
    "\n",
    "# Complete the encoding function\n",
    "def encode_news(news):\n",
    "    vectorizer = CountVectorizer()\n",
    "    features = vectorizer.fit_transform(news)\n",
    "    return features.toarray(), vectorizer\n",
    "\n",
    "# Step 2: Create a dataset and dataloader\n",
    "# Complete the text processing pipeline\n",
    "def text_processing_pipeline(news):\n",
    "    processed_news = preprocess_news(news)\n",
    "    encoded_news, vectorizer = encode_news(processed_news)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(encoded_news, encoded_labels, \n",
    "                                                        test_size = test_size, \n",
    "                                                        random_state = random_state)\n",
    "    input_dim = X_train.shape[1]\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test =  X_test.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    y_test =  y_test.astype(np.float32)\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train), \n",
    "                                  torch.tensor(y_train))\n",
    "    test_dataset =  TensorDataset(torch.tensor(X_test), \n",
    "                                  torch.tensor(y_test))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)  \n",
    "\n",
    "    dataset = [train_dataset, test_dataset]\n",
    "    dataloader = [train_dataloader, test_dataloader]\n",
    "    trainset = [X_train, y_train]\n",
    "    testset = [X_test, y_test]\n",
    "    return  dataset, dataloader, input_dim, trainset, testset, processed_news\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4e57fc-59ab-4ee6-9e1b-884a63d20fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 56.43265104293823 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "#train_dataset, test_dataset, train_dataloader, test_dataloader, input_dim, X_train, X_test, y_train, y_test = text_processing_pipeline(news_df.text)\n",
    "\n",
    "dataset, dataloader, input_dim, trainset, testset, processed_news = text_processing_pipeline(news_df.text)\n",
    "\n",
    "train_dataset, test_dataset = dataset\n",
    "train_dataloader, test_dataloader = dataloader\n",
    "X_train, y_train = trainset\n",
    "X_test, y_test = testset\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "print('Execution time:', et - st, 'seconds')\n",
    "\n",
    "\n",
    "# execution time: about 54.36970829963684 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833bd905-be55-4913-a7b1-bb5af0b47050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Execution time: 55.265727519989014 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# get the start time\n",
    "st = time.time()\n",
    "input_dim = input_dim\n",
    "output_dim = num_classes\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Step 3: Define the neural network model\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Step 4: Define the loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "output_dim = num_classes\n",
    "\n",
    "model = TextClassificationModel(input_dim, output_dim )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Step 5: Train the model\n",
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for data in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        features, targets = data   \n",
    "        predictions = model(features)\n",
    "\n",
    "        loss = criterion(predictions, targets.type(torch.LongTensor))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Training loop\n",
    "\n",
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model, train_dataloader, optimizer, criterion)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "# from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, iterator):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data in iterator:\n",
    "            features, targets = data\n",
    "            predictions = model(features)\n",
    "            all_predictions.extend(predictions.argmax(1).tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "    return all_predictions, all_targets\n",
    "\n",
    "# Evaluate the model\n",
    "predictions, true_targets = evaluate_model(model, test_dataloader)\n",
    "accuracy = accuracy_score(true_targets, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "print('Execution time:', et - st, 'seconds')\n",
    "\n",
    "# Accuracy: 0.93\n",
    "# Execution time: 56.27982807159424 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6f126d-9dbf-47f9-a751-31f1f35417a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[589,  39],\n",
       "       [ 54, 585]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix =  confusion_matrix(y_true = true_targets, \n",
    "                                y_pred = predictions)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378825c1-f0fe-4fba-9c66-895da54f1260",
   "metadata": {},
   "source": [
    "**Confusion matrix**: `i`-th row and `j`-th column entry indicates the number of samples with `true label` being `i`-th class and `predicted label` being `j`-th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59aa08fa-f61d-4a1a-a12a-659fb6f0fb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negative = 589\n",
      "False Positive = 39\n",
      "False Negative = 54\n",
      "True Positive = 585\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(true_targets, predictions).ravel()\n",
    "print(f'True Negative = {tn}')\n",
    "print(f'False Positive = {fp}')\n",
    "print(f'False Negative = {fn}')\n",
    "print(f'True Positive = {tp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "063dd727-c50b-44ae-bdb2-d8589987e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity = 0.915\n"
     ]
    }
   ],
   "source": [
    "# sensitivity = tp / (tp + fn) or Recall  \n",
    "# is the number of correctly identified points in the class of true positives\n",
    "print(f'sensitivity = {round(tp/(tp + fn), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46b0f8eb-7b4c-4ce1-bf3b-2d5617715102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specificity = 0.938\n"
     ]
    }
   ],
   "source": [
    "# specificity = tn / (tn + fp) \n",
    "print(f'specificity = {round(tn/(tn + fp),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7962f8ae-6189-49d6-a1f0-4baf6083ca6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.938\n"
     ]
    }
   ],
   "source": [
    "# Precision = tp / (tp +fp)\n",
    "print(f'Precision = {round(tp / (tp + fp), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c083d23-1d0a-47aa-812b-5c7282acdf2f",
   "metadata": {},
   "source": [
    "# Increasing the number of hidden layers and using relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7360366-5321-4dce-b47b-b6a56ebe4578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "Execution time: 81.53462433815002 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "hidden_dim = 4\n",
    "#output_dim = 2\n",
    "num_classes = 2\n",
    "\n",
    "# Step 3: Define the neural network model\n",
    "class TextClassificationModel2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TextClassificationModel2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)  # Apply first ReLU activation\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)  # Apply second ReLU activation\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Step 4: Define the loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "output_dim = num_classes\n",
    "\n",
    "model2 = TextClassificationModel2(input_dim, hidden_dim, output_dim)\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# Step 5: Train the model\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model2, train_dataloader, optimizer2, criterion2)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "\n",
    "# Evaluate the model\n",
    "predictions2, true_targets = evaluate_model(model2, test_dataloader)\n",
    "accuracy2 = accuracy_score(true_targets, predictions2)\n",
    "print(f'Accuracy: {accuracy2:.2f}')\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "# Accuracy: 0.93\n",
    "# Execution time: 74.64704155921936 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d3538-976b-436c-8cb9-056d278b01c5",
   "metadata": {},
   "source": [
    "## Using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "066b9c44-06e6-4ebf-8841-833e2295b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Validation Accuracy for Fold 1: 0.91\n",
      "Fold 2\n",
      "Validation Accuracy for Fold 2: 0.99\n",
      "Fold 3\n",
      "Validation Accuracy for Fold 3: 1.00\n",
      "Fold 4\n",
      "Validation Accuracy for Fold 4: 1.00\n",
      "Fold 5\n",
      "Validation Accuracy for Fold 5: 1.00\n",
      "Mean Accuracy: 0.98\n",
      "Standard Deviation: 0.03\n",
      "Execution time: 389.9281520843506 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "# Define the number of folds\n",
    "num_folds = 5  # You can adjust this as needed\n",
    "\n",
    "# Initialize a list to store accuracy scores\n",
    "accuracy_scores = []\n",
    "\n",
    "# Define the input and output dimensions\n",
    "input_dim = input_dim\n",
    "output_dim = num_classes\n",
    "\n",
    "\n",
    "# Define the model_cv\n",
    "model_cv  = TextClassificationModel2(input_dim, hidden_dim, output_dim)\n",
    "criterion_cv = nn.CrossEntropyLoss()\n",
    "optimizer_cv = optim.Adam(model_cv.parameters(), lr=0.001)\n",
    "\n",
    "# Create KFold cross-validator\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    \n",
    "    # Split the data into training and validation sets for this fold\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    train_data = TensorDataset(torch.tensor(X_train_fold), torch.tensor(y_train_fold))\n",
    "    val_data = TensorDataset(torch.tensor(X_val_fold), torch.tensor(y_val_fold))\n",
    "\n",
    "    train_dataloader_cv = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # Create and train a model for this fold\n",
    "    for epoch in range(num_epochs):\n",
    "        train_model(model_cv, train_dataloader_cv, optimizer_cv, criterion_cv)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    predictions_cv, true_labels_cv = evaluate_model(model_cv, val_dataloader)\n",
    "    accuracy_cv = accuracy_score(true_labels_cv, predictions_cv)\n",
    "    accuracy_scores.append(accuracy_cv)\n",
    "    print(f'Validation Accuracy for Fold {fold + 1}: {accuracy_cv:.2f}')\n",
    "\n",
    "# Calculate and print the mean and standard deviation of accuracy scores\n",
    "mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "std_accuracy = (sum((x - mean_accuracy) ** 2 for x in accuracy_scores) / len(accuracy_scores)) ** 0.5\n",
    "print(f'Mean Accuracy: {mean_accuracy:.2f}')\n",
    "print(f'Standard Deviation: {std_accuracy:.2f}')\n",
    "\n",
    "# Get the end time\n",
    "et = time.time()\n",
    "\n",
    "# Get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n",
    "# Fold 1\n",
    "# Validation Accuracy for Fold 1: 0.91\n",
    "# Fold 2\n",
    "# Validation Accuracy for Fold 2: 0.97\n",
    "# Fold 3\n",
    "# Validation Accuracy for Fold 3: 0.99\n",
    "# Fold 4\n",
    "# Validation Accuracy for Fold 4: 0.99\n",
    "# Fold 5\n",
    "# Validation Accuracy for Fold 5: 1.00\n",
    "# Mean Accuracy: 0.97\n",
    "# Standard Deviation: 0.03\n",
    "# Execution time: 1232.3690402507782 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58e9610e-d00d-448b-806f-5b89d8d64f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model_cv\n",
    "predictions3, true_targets = evaluate_model(model_cv, test_dataloader)\n",
    "accuracy3 = accuracy_score(true_targets, predictions3)\n",
    "print(f'Accuracy: {accuracy3:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a939a-7dc5-4722-8a45-9b2ab6b360a5",
   "metadata": {},
   "source": [
    "## Using embedding with the vocabulary of the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1147e98-f925-4fc4-b3c2-e777ed90853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b941c39-e1dc-412f-bc25-f77db9cec3ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_embed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     38\u001b[0m features, targets \u001b[38;5;241m=\u001b[39m data   \n\u001b[1;32m---> 39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor))\n\u001b[0;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 50\u001b[0m, in \u001b[0;36mTextClassificationModelWithEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 50\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m embedded\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Average the embeddings\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "# st = time.time()\n",
    "\n",
    "# Step 2: Tokenization and Vocabulary \n",
    "# processed_news = preprocess_news(news_df.text)\n",
    "nopunc_news = [re.sub(r'[^\\w\\s]', '', item) for item in processed_news]\n",
    "tokenized_news = [word_tokenize(item) for item in nopunc_news]\n",
    "\n",
    "words  = list(chain(*map(lambda line: [word for word in line], tokenized_news)))\n",
    "\n",
    "unique_words = set()\n",
    "vocab = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in unique_words:\n",
    "        vocab.append(word)\n",
    "        unique_words.add(word)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "# idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Convert word_to_idx to a tensor\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "\n",
    "# Step 3: Create an Embedding Layer\n",
    "embedding_dim = 500  # Adjust the dimension as needed\n",
    "num_embeddings = len(vocab)\n",
    "\n",
    "# Initialize embedding layer \n",
    "embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "# Pass the tensor to the embedding layer\n",
    "output = embedding(inputs)\n",
    "\n",
    "\n",
    "# Step 4: Modify the Model Architecture\n",
    "class TextClassificationModelWithEmbedding(nn.Module):\n",
    "    def __init__(self, embedding, hidden_dim, output_dim):\n",
    "        super(TextClassificationModelWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        x = embedded.mean(dim=0)  # Average the embeddings\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)  # Apply ReLU activation\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 5: Define the loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "output_dim = num_classes\n",
    "\n",
    "model_embed = TextClassificationModelWithEmbedding(embedding, hidden_dim, output_dim)\n",
    "criterion_embed = nn.CrossEntropyLoss()\n",
    "optimizer_embed = optim.Adam(model_embed.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Step 6: Train the model\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model_embed, train_dataloader, optimizer_embed, criterion_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf44c06-fb7f-4d8b-90c0-40f98a37f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ad03635-febe-4104-8d03-201698a700e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_embed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     38\u001b[0m features, targets \u001b[38;5;241m=\u001b[39m data   \n\u001b[1;32m---> 39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor))\n\u001b[0;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 51\u001b[0m, in \u001b[0;36mTextClassificationModelWithEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     50\u001b[0m    \u001b[38;5;66;03m# embedded = self.embedding(x)\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m embedded\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Average the embeddings\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model_embed, train_dataloader, optimizer_embed, criterion_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "401d0374-133b-4714-90e8-f409ab8e19f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Step 7: Evaluate the model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m predictions_embed, true_targets \u001b[38;5;241m=\u001b[39m evaluate_model(model_embed, test_dataloader)\n",
      "Cell \u001b[1;32mIn[12], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     38\u001b[0m features, targets \u001b[38;5;241m=\u001b[39m data   \n\u001b[1;32m---> 39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor))\n\u001b[0;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 41\u001b[0m, in \u001b[0;36mTextClassificationModelWithEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 41\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m embedded\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Average the embeddings\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\math103b\\lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Evaluate the model\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_embed, true_targets = evaluate_model(model_embed, test_dataloader)\n",
    "accuracy_embed = accuracy_score(true_targets, predictions_embed)\n",
    "print(f'Accuracy: {accuracy_embed:.2f}')\n",
    "\n",
    "# get the end time\n",
    "# et = time.time()\n",
    "\n",
    "# Get the execution time\n",
    "# elapsed_time = et - st\n",
    "# print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be25559-51d3-40c1-a8d6-82abe3e07c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
