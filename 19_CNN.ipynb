{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755d97c1-0bdf-4086-a519-9c206b87a085",
   "metadata": {},
   "source": [
    "#  Convolutional Neural Networks (CNN) in Text Classification with PyTorch\n",
    "\n",
    "The text classification problem can be approached in a number of ways with respect to encoding the text data to numerical values.\n",
    "\n",
    "1. Text is modeled as the *frequency of occurrence of words* in a given text with respect of these words in the complete corpus. Example: `CountVectorizer()` and `TfidfVectorizer()` in `scikit-learn`.\n",
    "2. Text is modeled as the *sequence of words or characters*. This type of approach is used mainly by the **Recurrent Neural Networks** (**RNN**).\n",
    "3. Text is modeled as a *distribution of words in a given space*. This is achieved through the use of the **Convolutional Neural Network** architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42031b4d-6848-465e-b60f-587c69fc01fc",
   "metadata": {},
   "source": [
    "## The architecture of CNN in text classification\n",
    "\n",
    "### What is a semantic space? \n",
    "\n",
    "Semantic spaces are representations of natural language that are capable of capturing meaning.\n",
    "\n",
    "Reference: https://en.wikipedia.org/wiki/Semantic_space.\n",
    "\n",
    "A *semantic space* is a way of representing the meaning of words using vectors, matrices, or other mathematical structures. \n",
    "\n",
    "The idea: \n",
    "\n",
    "**\"Words that are similar in meaning will have similar or close vectors in the semantic space, while words that are different or unrelated will have distant or orthogonal vectors\".**\n",
    "\n",
    "\n",
    "Slogan: **\"You shall know a word by the company it keeps\"** (J.R. Firth).\n",
    "\n",
    "\n",
    "For example, \n",
    "\n",
    "*   `fire and dog` are two words unrelated in their meaning, and in fact they are not often used in the same sentence. \n",
    "*   On the other hand, the words `dog and cat` are sometimes seen together, so they may share some aspect of meaning.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "![](images/cos.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f3487-da35-4a66-a0e2-82bb156cd1fb",
   "metadata": {},
   "source": [
    "## A common architecture for CNN in text classification\n",
    "\n",
    "\n",
    "*   each word in a document is represented as an *embedding vector*, \n",
    "*   a single convolutional layer with m filters is applied, producing an m-dimensional vector for each document ngram.\n",
    "*  The vectors are combined using max-pooling followed by a ReLU activation.\n",
    "*  The result is then passed to a linear layer for the final classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c55c91-ffc8-4da7-99a2-00c4a4331a3c",
   "metadata": {},
   "source": [
    "### Word embedding\n",
    "\n",
    "*Word embedding* is a technique of representing words in a numeric vector form that captures their meaning and relationships with other words.\n",
    "\n",
    "In pytorch, \n",
    "\n",
    "*  the *embedding* is computed by using a simple lookup table that maps an index value to a weight matrix of some dimension. \n",
    "*  The weight matrix is initialized randomly and then optimized during training to produce more useful vectors. \n",
    "*  The input to the `nn.Embedding layer` is a **tensor of indices**, and the output is the corresponding embedding vectors. \n",
    "\n",
    "We need to form a python dictionary called `word_to_index` that maps each word in the vocabular, $V$, the collection of unique words in the corpus,  to its corresponding index of appearance in the corpus. \n",
    "\n",
    "In pytorch, we use `nn.Embedding` where the number of input nodes, `num_embeddings`, is equal to $seq_len$, the length of each row in $X$ which is \n",
    "equalize by adding extra $0$s at the end of each text and the number of output nodes  is the dimensionality of the embeddings, `embedding_dim` or $D$. Embeddings are stored as a $∣V∣ times D$ matrix, such that the word assigned index $i$ has its embedding stored in the $i$th row of the matrix. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa17ebc-fa6f-4ac8-8735-fffd8e97a4b6",
   "metadata": {},
   "source": [
    "### Convolution and max pooling\n",
    "\n",
    "In the context of Natural Language Processing (NLP), *convolution* refers to a mathematical operation that combines two input functions to produce a third. It differs from the *composition of two functions*, since convolutions are commutative while compositions are not.\n",
    "\n",
    "*  Consider $A = \\begin{bmatrix}1 & 0 & 1 & 1\\\\0 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix}$, $B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$.  Then the `dot product` of $A$ and $B$ is \n",
    "\n",
    "$$A\\cdot B = \\begin{bmatrix}1 & 0 & 1 & 1\\\\0 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1\\\\ 1 & 1 \\\\ 0 & 2 \\end{bmatrix}$$\n",
    "\n",
    "In convolution, the *kernel* is a small matrix that slides over the input image to perform the convolution operation. \n",
    "\n",
    "*  The `num_embeddings` is `seq_len`, the length of a row (standardized).\n",
    "*  The  `embedding_dim = D`, the dimensionality of the embeddings.\n",
    "*  The `input_size` of the embeddings is `shape(seq_len, D)`\n",
    "*  The `kernel size = n` means that the kernel consists of `n` tokens.\n",
    "*  The `window size` of the kernel is the size of the kernel matrix with `(n,D)` \n",
    "*  The `stride` is the number of rows by which the kernel moves down the embeddings. \n",
    "*  The `output size` of the convolution operation depends on the size of the input image, the size of the kernel, the stride, and the padding. \n",
    "*  `Padding` is used to add extra `0`s at the end of the input text.\n",
    "*  `Stride` is used to downsample the size of the output. The general rule is to use `stride=1` in usual convolutions and preserve the spatial size.\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "* Consider the input. $X = \\begin{bmatrix}1 & 0 & 1 & 1\\\\0 & 1 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix}$, and the kernels \n",
    "\n",
    "    * $K_1 = \\begin{bmatrix} 1 & 0 & 1 & 0 \\end{bmatrix}$,\n",
    "    \n",
    "    * $K_2 = \\begin{bmatrix} 0 & 1 & 0 & 1 \\end{bmatrix}$, and \n",
    "\n",
    "    * $K_3 = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix}$,\n",
    " \n",
    "We have 3 convolutions on $X$ with 3 kernels and `stride=1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42636361-b6be-4e46-bcb2-814dd0254fe6",
   "metadata": {},
   "source": [
    " \n",
    "**Convolution 1:** $ \\;\\;\\;X \\star K_1 = \\begin{bmatrix} \n",
    "2 \\\\ 1 \\\\ 0 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "**1st stride:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\color{red} 1 & \\color{red} 0 & \\color{red} 1 & \\color{red} 1 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 1 \\\\ \n",
    "\\end{bmatrix} \\star \n",
    "\\begin{bmatrix} \n",
    "\\color{red}1 & \\color{red}0 & \\color{red}1 & \\color{red}0 \\\\\n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\color{red}2 \\\\ \\phantom{1} \\\\ \\phantom{0}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**2nd stride:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 1 &  1 \\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}1 &\\color{red} 0 \\\\\n",
    "0 & 1 & 0 & 1 \\\\ \n",
    "\\end{bmatrix} \\star \n",
    "\\begin{bmatrix} \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\color{red}1 & \\color{red}0 & \\color{red}1 & \\color{red}0 \\\\\n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\phantom{2} \\\\ \\color{red}1 \\\\ \\phantom{0} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**3rd stride:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 1 & 1 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1 \\\\ \n",
    "\\end{bmatrix} \\star \n",
    "\\begin{bmatrix} \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\color{red}1 & \\color{red}0 & \\color{red}1 & \\color{red}0 \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\phantom{2} \\\\\\phantom{1} \\\\ \\color{red}0 \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e20076-85d1-4690-b41f-d6bab51e79f4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 19,
    "lastExecutedAt": 1699764548104,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "$X \\star K_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}$\n\n**1st stride:**\n\n$$\\begin{bmatrix}\n\\color{red} 1 & \\color{red} 0 & \\color{red} 1 & \\color{red} 1 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\ \n\\end{bmatrix} \\star \n\\begin{bmatrix} \n\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1 \n\\end{bmatrix} = \n\\begin{bmatrix} \n\\color{red}2 \\\\ 1 \\\\ 0 \n\\end{bmatrix}$$\n\n**2nd stride:**\n\n$$\\begin{bmatrix}\n1 & 0 & 1 &  1 \\\\\n\\color{red}0 & \\color{red}1 & \\color{red}1 &\\color{red} 0 \\\\\n0 & 1 & 0 & 1 \\\\ \n\\end{bmatrix} \\star \n\\begin{bmatrix} \n\\color{red}1 & \\color{red}0 & \\color{red}1 & \\color{red}0 \n\\end{bmatrix} = \n\\begin{bmatrix} \n2 \\\\ \\color{red}1 \\\\ 0 \n\\end{bmatrix}$$\n\n**3rd stride:**\n\n$$\\begin{bmatrix}\n1 & 0 & 1 & 1 \\\\\n0 & 1 & 1 & 0 \\\\\n\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1 \\\\ \n\\end{bmatrix} \\star \n\\begin{bmatrix} \n\\color{red}1 & \\color{red}0 & \\color{red}1 & \\color{red}0 \n\\end{bmatrix} = \n\\begin{bmatrix} \n2 \\\\ 1 \\\\ \\color{red}0 \n\\end{bmatrix}$$"
   },
   "source": [
    "**Convolution 2:** $\\;\\;\\;X \\star K_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "**1st stride:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\color{red} 1 & \\color{red} 0 & \\color{red} 1 & \\color{red} 1 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 1 \\\\ \n",
    "\\end{bmatrix} \\star \n",
    "\\begin{bmatrix} \n",
    "\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1 \\\\\n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\color{red}1 \\\\ \\phantom{1} \\\\ \\phantom{2} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**2nd stride:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 1 &  1 \\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}1 &\\color{red} 0 \\\\\n",
    "0 & 1 & 0 & 1 \\\\ \n",
    "\\end{bmatrix} \\star \n",
    "\\begin{bmatrix} \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1 \\\\\n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\phantom{1} \\\\ \\color{red}1 \\\\\\phantom{2} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**3rd stride:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 1 & 1 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1 \\\\ \n",
    "\\end{bmatrix} \\star \n",
    "\\begin{bmatrix} \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1 \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\phantom{1} \\\\ \\phantom{1} \\\\ \\color{red}2 \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e31bd5-7d91-4c57-b1f8-96e5698ad487",
   "metadata": {},
   "source": [
    "**Convolution 3:** $ \\;\\;\\;X \\star K_3 = \\begin{bmatrix} \n",
    "3 \\\\ 1 \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "**1st stride:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\color{red}1 & \\color{red}0 & \\color{red}1 & \\color{red}1 \\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}1 & \\color{red}0 \\\\\n",
    "0 & 1 & 0 & 1 \\\\ \n",
    "\\end{bmatrix} \\star \n",
    "\\begin{bmatrix} \n",
    "\\color{red}1 & \\color{red}0 & \\color{red}1 & \\color{red}0\\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1\\\\\n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\color{red}3 \\\\ \\phantom{1}  \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**2nd stride:**\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 1 & 1 \\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}1 &\\color{red} 0 \\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1 \\\\ \n",
    "\\end{bmatrix} \\star \n",
    "\\begin{bmatrix} \n",
    "\\phantom{1} & \\phantom{0} & \\phantom{1} & \\phantom{0} \\\\ \n",
    "\\color{red}1 & \\color{red}0 & \\color{red}1 & \\color{red}0\\\\\n",
    "\\color{red}0 & \\color{red}1 & \\color{red}0 & \\color{red}1\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "\\phantom{3} \\\\ \\color{red}1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40adfa01-6f03-459b-b785-4f275cae1dc6",
   "metadata": {},
   "source": [
    "**Max pooling** is a technique that reduces the dimensionality of the output by taking the maximum value of a patch of the output.\n",
    "\n",
    "**Max Pooling on Convolution 1**:\n",
    "\n",
    "$X \\star K_1 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix} \\Longrightarrow \n",
    "\\max\\left({\\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix}}\\right) = \n",
    "\\begin{bmatrix} 2 \\end{bmatrix}$\n",
    "\n",
    "___\n",
    "\n",
    "**Max Pooling on Convolution 2**:\n",
    "\n",
    "$X \\star K_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} \\Longrightarrow \n",
    "\\max\\left({\\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}}\\right) = \n",
    "\\begin{bmatrix} 2 \\end{bmatrix}$\n",
    "\n",
    "___\n",
    "\n",
    "**Max Pooling on Convolution 3**:\n",
    "\n",
    "$X \\star K_3 = \\begin{bmatrix} 3 \\\\ 1 \\\\ \\end{bmatrix} \\Longrightarrow \n",
    "\\max\\left({\\begin{bmatrix} 3 \\\\ 1 \\\\ \\end{bmatrix}}\\right) = \n",
    "\\begin{bmatrix} 3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101dfddb-de04-48cf-9090-d9034b3deaca",
   "metadata": {},
   "source": [
    "Finally, each of these outputs will be concatenated in a single tensor to be introduced to a linear layer which will be filtered by an activation function to obtain the final result.\n",
    "\n",
    "$\\begin{align*}\n",
    "X \\star K_1 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix} &\\Longrightarrow \n",
    "\\max\\left({\\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix}}\\right) = \n",
    "\\begin{bmatrix} 2 \\end{bmatrix} \\\\\n",
    "X \\star K_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} &\\Longrightarrow \n",
    "\\max\\left({\\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}}\\right) = \n",
    "\\begin{bmatrix} 2 \\end{bmatrix} \\;\\;\\; \\Longrightarrow \\;\\;\\;\n",
    "\\begin{bmatrix} 2\\\\ 2\\\\ 3 \\end{bmatrix}\\\\\n",
    " X\\star K_3 = \\begin{bmatrix} 3 \\\\ 1 \\\\ \\end{bmatrix} &\\Longrightarrow \n",
    "\\max\\left({\\begin{bmatrix} 3 \\\\ 1 \\\\ \\end{bmatrix}}\\right) = \n",
    "\\begin{bmatrix} 3 \\end{bmatrix}\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc7cbc7-c7a7-49db-8c95-dfe223f560b1",
   "metadata": {},
   "source": [
    "### A CNN Architecture\n",
    "\n",
    "![](images/waakss1l.png)\n",
    "\n",
    "\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e871ba8-6654-45de-b2f5-9d15f792bdc8",
   "metadata": {},
   "source": [
    "## Text processing pipeline\n",
    "\n",
    "![](images/text_processing_pipeline.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db653ead-642a-48ad-8a71-dce21591be60",
   "metadata": {},
   "source": [
    "### Step 1. Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81890e0c-c454-424e-a25b-7c6a83fbba12",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 789,
    "lastExecutedAt": 1699799436080,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Code 1. Loading raw data\n# Reads the raw csv file and split into \n# news (x) and label (y)\n\nimport pandas as pd\n\ndf = pd.read_csv('datasets/fake_or_real_news.csv')\n\nX_raw = df[\"text\"].values\ny = df[\"label\"].values"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Code 1. Loading raw data\n",
    "# Reads the raw csv file and split into \n",
    "# text (x) and target (y)\n",
    "# tweets on real disaster (1) or not (0).\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/tweets.csv')\n",
    "\n",
    "X_raw = df[\"text\"].values\n",
    "y = df[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30729ec6-b410-4830-883e-e4b38435d70f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1699799436135,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "df.info()",
    "outputsMetadata": {
     "0": {
      "height": 232,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a2f688-b84c-4030-8e6a-3cf58a68d5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "1 : Forest fire near La Ronge Sask. Canada\n",
      "2 : All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "3 : 13,000 people receive #wildfires evacuation orders in California \n",
      "4 : Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i,\":\",df.text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a062378-9f41-4faf-8cbe-9eb1e8dcc0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 1\n",
      "1 : 1\n",
      "2 : 1\n",
      "3 : 1\n",
      "4 : 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i,\":\",df.target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81abb42-1417-4e9a-a1a9-7c2486543c37",
   "metadata": {},
   "source": [
    "### Step 2. Lowering cases and removing special special symbols \n",
    "\n",
    "In this step, we will need to remove special symbols and numbers from the text. We are only going to work with lowercase words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86d636e-39a6-46b0-bc36-fe34b554ad52",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 535,
    "lastExecutedAt": 1699799436670,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Code 2 Lowering cases and removing special symbols\n\nimport re \n\nX_lower = [x.lower() for x in X_raw]\nX_no_punctuation = [re.sub(r'[^\\w\\s]', '', x) for x in X_lower]"
   },
   "outputs": [],
   "source": [
    "# Code 2 Lowering cases and removing special symbols\n",
    "\n",
    "import re \n",
    "\n",
    "X_lower = [x.lower() for x in X_raw]\n",
    "X_no_punctuation = [re.sub(r'[^\\w\\s]', '', x) for x in X_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3860bb-5cd4-437a-9d2b-d6b3dec6e11f",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1699799436718,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(X_no_punctuation[0])",
    "outputsMetadata": {
     "0": {
      "height": 565,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 our deeds are the reason of this earthquake may allah forgive us all\n",
      "1 forest fire near la ronge sask canada\n",
      "2 all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected\n",
      "3 13000 people receive wildfires evacuation orders in california \n",
      "4 just got sent this photo from ruby alaska as smoke from wildfires pours into a school \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i, X_no_punctuation[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9639f24-95ea-4a4e-9b07-f19d8becfcd8",
   "metadata": {},
   "source": [
    "### Step 3. Tokenization, Removing Stop Words, and Lemmatization\n",
    "\n",
    "For tokenization, removing stop words and lemmatizaiton, we are going to make use of the functions from the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936d69d8-372c-4480-bb3b-8d764ddb78af",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 134,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lexmuga\\anaconda3\\envs\\math103b\\lib\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b565caac-49c7-447c-b933-ba9d8bc06223",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 83218,
    "lastExecutedAt": 1699799520722,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Code 3. Tokenization, removing stop words and lemmatization\n\nlemmatizer = WordNetLemmatizer()\n\n# set_stop_words = set(stopwords.words('english'))\nX_tokens = [word_tokenize(x) for x in X_no_punctuation]\n#X_no_stops = [[word for word in tokens if word not in set_stop_words] for tokens in X_tokens]\n\nX_lemmas = []\nfor sentence in X_tokens:\n    lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in sentence]\n    lemmas = [lemmatizer.lemmatize(word, pos=\"n\") for word in lemmas]\n    lemmas = [lemmatizer.lemmatize(word, pos=\"a\") for word in lemmas]\n    lemmas = [lemmatizer.lemmatize(word, pos=\"r\") for word in lemmas]\n    lemmas = [lemmatizer.lemmatize(word, pos=\"s\") for word in lemmas]\n    X_lemmas.append(lemmas)"
   },
   "outputs": [],
   "source": [
    "# Code 3. Tokenization, removing stop words and lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# set_stop_words = set(stopwords.words('english'))\n",
    "X_tokens = [word_tokenize(sentence) for sentence in X_no_punctuation]\n",
    "#X_no_stops = [[word for word in tokens if word not in set_stop_words] for tokens in X_tokens]\n",
    "\n",
    "X_lemmas = []\n",
    "for sentence in X_tokens:\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in sentence]\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos=\"n\") for word in lemmas]\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos=\"a\") for word in lemmas]\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos=\"r\") for word in lemmas]\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos=\"s\") for word in lemmas]\n",
    "    X_lemmas.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5b923f-7480-49fc-a965-f7704fca1004",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1699799520771,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(0, X_tokens[0])\n",
    "outputsMetadata": {
     "0": {
      "height": 565,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : ['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n",
      "1 : ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
      "2 : ['all', 'residents', 'asked', 'to', 'shelter', 'in', 'place', 'are', 'being', 'notified', 'by', 'officers', 'no', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']\n",
      "3 : ['13000', 'people', 'receive', 'wildfires', 'evacuation', 'orders', 'in', 'california']\n",
      "4 : ['just', 'got', 'sent', 'this', 'photo', 'from', 'ruby', 'alaska', 'as', 'smoke', 'from', 'wildfires', 'pours', 'into', 'a', 'school']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'{i} : {X_tokens[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0113d0-8647-4b93-80c5-db20cf376534",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1699799520819,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(0, X_lemmas[0])",
    "outputsMetadata": {
     "0": {
      "height": 565,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : ['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n",
      "1 : ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
      "2 : ['all', 'residents', 'asked', 'to', 'shelter', 'in', 'place', 'are', 'being', 'notified', 'by', 'officers', 'no', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']\n",
      "3 : ['13000', 'people', 'receive', 'wildfires', 'evacuation', 'orders', 'in', 'california']\n",
      "4 : ['just', 'got', 'sent', 'this', 'photo', 'from', 'ruby', 'alaska', 'as', 'smoke', 'from', 'wildfires', 'pours', 'into', 'a', 'school']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'{i} : {X_tokens[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8999d-104a-410d-921b-f355204e75f0",
   "metadata": {},
   "source": [
    "### Step 4. Building the word_to_index dictionary and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf89b773-32bf-4ef4-9630-dba8bf9267f3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 839,
    "lastExecutedAt": 1699799521658,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Code 4. word_to_idx\n  # By using the dictionary (vocabulary), it is transformed\n  # each token into its index based representation\t\n\nfrom itertools import chain\n\nwords  = list(chain(*map(lambda line: [word for word in line], X_lemmas)))\n\nunique_words = set()\nvocab = []\n\nfor word in words:\n    if word not in unique_words:\n        vocab.append(word)\n        unique_words.add(word)\n\nword_to_idx = {word: i+1 for i, word in enumerate(vocab)}\nvocab_size = len(vocab)"
   },
   "outputs": [],
   "source": [
    "# Code 4. word_to_idx\n",
    "  # By using the dictionary (vocabulary), it is transformed\n",
    "  # each token into its index based representation\t\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "words  = list(chain(*map(lambda sentence: [word for word in sentence], X_lemmas)))\n",
    "\n",
    "unique_words = set()\n",
    "vocab = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in unique_words:\n",
    "        vocab.append(word)\n",
    "        unique_words.add(word)\n",
    "\n",
    "word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9393ba23-3a62-45b8-b697-a64512f0ef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20098\n",
      "20098\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)\n",
    "print(len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe4a7974-e7da-4347-bad2-b9fdc1818835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('our', 1),\n",
       " ('deed', 2),\n",
       " ('be', 3),\n",
       " ('the', 4),\n",
       " ('reason', 5),\n",
       " ('of', 6),\n",
       " ('this', 7),\n",
       " ('earthquake', 8),\n",
       " ('may', 9),\n",
       " ('allah', 10)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_idx.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a430f5-2b07-44cd-9699-e7a6ee9d5142",
   "metadata": {},
   "source": [
    "### Step 5. Encoding the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab7b84f-24df-4f30-a1bf-daa66d249e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = []\n",
    "for sentence in X_lemmas:\n",
    "    word_encoded = []\n",
    "    for word in sentence:\n",
    "        word_encoded.append(word_to_idx[word])\n",
    "    X_encoded.append(word_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70f883d2-bded-409b-af6a-bcf1fb43013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "1 [14, 15, 16, 17, 18, 19, 20]\n",
      "2 [13, 21, 22, 23, 24, 25, 26, 3, 3, 27, 28, 29, 30, 31, 32, 33, 24, 25, 26, 34, 3, 35]\n",
      "3 [36, 37, 38, 39, 32, 34, 25, 40]\n",
      "4 [41, 42, 43, 7, 44, 45, 46, 47, 48, 49, 45, 39, 50, 51, 48, 52]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i, X_encoded[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b72fe5c-50a8-49e4-8093-d6968bf1cafc",
   "metadata": {},
   "source": [
    "### Step 5. Padding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96c2c556-8caa-4685-a9fb-f149c37e85b5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 14333,
    "lastExecutedAt": 1699800131874,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Code 5. Padding\n# Each sentence which does not fulfill the required length\n# is padded with the index 0\n\nimport numpy as np\n\npad_idx = 0\nX_padded = list()\nseq_len = np.max([len(x) for x in X_lemmas])\n\nfor sentence in X_lemmas:\n    while len(sentence) < seq_len:\n        sentence.append(pad_idx)\n    X_padded.append(sentence)\n\nX_padded = np.array(X_padded)"
   },
   "outputs": [],
   "source": [
    "# Code 5. Padding\n",
    "# Each sentence which does not fulfill the required length\n",
    "# is padded with the index 0\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "pad_idx = 0\n",
    "X_padded = list()\n",
    "seq_len = np.max([len(x) for x in X_encoded])\n",
    "\n",
    "for sentence in X_encoded:\n",
    "    while len(sentence) < seq_len:\n",
    "        sentence.append(pad_idx)\n",
    "    X_padded.append(sentence)\n",
    "\n",
    "X_padded = np.array(X_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ba0d049-2883-405e-91ad-df10b5269bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "929b0a55-38cc-4804-9f4d-170386c39056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1 [14, 15, 16, 17, 18, 19, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2 [13, 21, 22, 23, 24, 25, 26, 3, 3, 27, 28, 29, 30, 31, 32, 33, 24, 25, 26, 34, 3, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "3 [36, 37, 38, 39, 32, 34, 25, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "4 [41, 42, 43, 7, 44, 45, 46, 47, 48, 49, 45, 39, 50, 51, 48, 52, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i, X_encoded[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b3872-b49b-4384-a403-e325824247a8",
   "metadata": {},
   "source": [
    "### Step 6. Split data \n",
    "\n",
    "**Split intro train and test**. The last step in this preprocessing pipeline is to divide the data into training and testing. For this we will use the function provided by scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee7a48c0-e135-43a6-a838-ee8738c4a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 6. Split the train and test data sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5583c47c-40a5-4bcf-bcc9-f65b474f4365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8760dfab-5160-40cc-8d1a-c20ea7d85a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15262 15263     7   421  1615  3666  1599  3585  3666    94    45    83\n",
      "     99    83   800     3   525    23   619    79 15264  3585     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [17640     4    68     6  8424  5660     3 17641 16232    79   201   202\n",
      "    256     3  6033  1916 17642     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [ 1188    48    80     3 11135    85  4157 11136   189   635  4918    79\n",
      "   3295   418    13   520     4    26   185   158   139   790     3   418\n",
      "     23   811    48   197   707     0     0]\n",
      " [    4  1876    76  3737   237   563     4    12    51    48  1876  1423\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]\n",
      " [ 2197  9347    65   180  9348  9349     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd63a4f0-09b9-4d20-8598-10c568bc0b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "31\n",
      "31\n",
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(len(X_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "604372f6-37a4-44c1-8188-74b65b9a9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da507c39-e5e3-4722-b87d-1c3869280108",
   "metadata": {},
   "source": [
    "### Step 7. Building TensorDataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b84f936c-1c5d-4f31-a7b8-403f8e7dfd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 2  # Adjust as needed\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), \n",
    "                                  torch.tensor(y_train))\n",
    "test_dataset =  TensorDataset(torch.tensor(X_test), \n",
    "                                  torch.tensor(y_test))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d1a55-9baf-4e38-8477-56372518677d",
   "metadata": {},
   "source": [
    "![](images/text_processing_pipeline.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d813cf-2227-479c-ba60-0eb235e0414c",
   "metadata": {},
   "source": [
    "# Next steps: creating the cnn model, training and evaluating the model\n",
    "\n",
    "![](images/text_classification_model.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94528dd-1bd4-4ce9-af54-5ef657769a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
