{"cells":[{"source":" # 5.5 Long Short Term Memory (LSTM) Networks\n \n References: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n \nLong Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) that is specifically designed to handle sequential data, such as time series, speech, and text1. LSTM networks are capable of learning long-term dependencies in sequential data, which makes them well suited for tasks such as language translation, speech recognition, and time series forecasting.\n\n![Traditional RNN](Images/RNN_unrolled.png)\n\n![](Images/RNN_layer.png)\n\nA traditional RNN has a single hidden state that is passed through time, which can make it difficult for the network to learn long-term dependencies. LSTMs address this problem by introducing a memory cell, which is a container that can hold information for an extended period of time. \n\n\n\nThe memory cell is controlled by three gates: \n*   the input gate, \n*   the forget gate, and \n*   the output gate. \n  \n___\n\n![LSTM](Images/LSTM_with_4_layers.png)\n\n___\n\n![Operations](Images/LSTM_operations.png)\n\n___\n\nThese gates decide what information to add to, remove from, and output from the memory cell.","metadata":{},"cell_type":"markdown","id":"b86c6394-a202-4a04-97a8-f6885859bd26"},{"source":"## The cell state of LSTM\n\n*   The key to LSTMs is the *cell state*, the horizontal line running through the top of the diagram.\n*   The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\n\n![The cell state](Images/cell_state.png)\n\n## The gates of LSTM\n\n* Gates are a way to optionally let information through. \n* They are composed out of a *sigmoid neural net layer* and a *pointwise multiplication operation*.\n* The sigmoid layer outputs numbers between *zero* and *one*. \n    * a `value=0` means `“let nothing through,”` while \n    * a `value=1` means `“let everything through!”`.\n*  An LSTM has three of these gates, to protect and control the cell state.\n\n![A gate of LSTM](Images/gate.png)\n\n## Step-by-Step LSTM Walk \n\n### Step 1. Decide what information to throw away from the cell state.\n\n*Using a sigmoid forget gate layer*:\n\n![Forget Gate](Images/forget_gate_layer.png)\n\n\n### Step 2. Decide what new information to store in the cell state.\n\n*Using a sigmoid input gate layer*  and  *tanh layer* creating a vector of new candidates,$\\tilde{C}_t$ to be added to the cell state\n\n![](Images/sigmoid_input_gate_with_tanh_gate.png)\n\n### Step 3. Update to the new cell state\n\n![](Images/creating_new_cell_state.png)\n\n### Step 4. Decide what to output.\n\n![](Images/output.png)\n","metadata":{},"cell_type":"markdown","id":"f65fa587-f971-4873-b9f7-5611e44fb5c9"},{"source":"import nltk\nimport numpy as np\nimport pandas as pd\nimport re \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\n\nclass Preprocessing:\n    \n    def __init__(self):\n    \n        self.data = 'datasets/tweets.csv'\n\n        self.X_raw = None\n        self.y = None\n        self.X_cleaned = None\n        self.X_tokenized = None\n        self.X_stopwords_removed = None\n        self.X_lemmatized = None\n        #self.vocabular = None\n        #self.word2idx = None\n        #self.vector_size = None\n        #self.X_encoded = None\n        #self.X_padded = None\n\n    def load_data(self):\n        # Reads the raw csv file and split into\n        # features (X) and target (y)\n        \n        df = pd.read_csv(self.data)\n        #df.drop(['id','keyword','location'], axis=1, inplace=True)\n        \n        self.X_raw = df['text'].values\n        self.y = df['target'].values\n\n    def clean_text(self):\n        # Removes special symbols and just keep\n        # words in lower or upper form\n        \n        self.X_cleaned = [x.lower() for x in self.X_raw]\n        self.X_cleaned = [re.sub(r'[^\\w\\s]', '', x) for x in self.X_cleaned]\n        \n    def text_tokenized(self):\n        # Tokenizes each sentence by implementing the nltk tool\n        \n        self.X_tokenized = [word_tokenize(x) for x in self.X_cleaned]\n\n    def text_stopwords_removed(self):\n        ## Create a list of stopwords\n        \n        stop_words = set(stopwords.words(\"english\"))\n        no_stopwords = []\n        \n        for tokens in self.X_tokenized:\n            tokens = [token for token in tokens if token not in stop_words]\n            no_stopwords.append(tokens)\n            \n        self.X_stopwords_removed = no_stopwords\n\n    def text_lemmatized(self):\n    \n        lemmatizer = WordNetLemmatizer()\n\n        text_lemmas = []\n        for tokens in self.X_stopwords_removed:\n            lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"n\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"a\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"r\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"s\") for word in lemmas]\n            text_lemmas.append(lemmas)\n        \n        self.X_lemmatized = text_lemmas\n        \n    #preprocessing.load_data()\n    #preprocessing.clean_text()\n    #preprocessing.text_tokenized()\n    #preprocessing.text_stopwords_removed()\n    #preprocessing.text_lemmatized()","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":134,"type":"stream"}}},"cell_type":"code","id":"9beee9ee-e92e-44ba-b6e5-5a1ff7af75b5","execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /home/repl/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /home/repl/nltk_data...\n[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"}]},{"source":"import copy\n\nclass Encoding:\n    \n    def __init__(self, lemmatized_texts, num_words):\n        \n        self.X_lemmatized = lemmatized_texts\n        self.num_words = num_words\n        self.vector_size = None\n        self.fdist = None\n        self.X_encoded_texts = None\n        self.text4encoding = None\n        self.X_padded_codes = None\n    \n    def text_encoding(self):\n\n        vocabulary = dict()\n        fdist = nltk.FreqDist()  \n        \n        for tokens in self.X_lemmatized:  \n            for word in tokens:\n                fdist[word] += 1\n        \n        self.fdist = fdist\n        common_words = fdist.most_common(self.num_words)\n\n        for idx, word in enumerate(common_words):\n            vocabulary[word[0]] = (idx+1)\n        \n        self.vocabulary = vocabulary\n      \n        encoded_texts = list()\n        texts4encoding = list()\n        \n        for tokens in self.X_lemmatized:\n            temp_codes = list()\n            temp_words = list()\n            \n            for word in tokens:\n                if word in self.vocabulary.keys():\n                    temp_codes.append(self.vocabulary[word])\n                    temp_words.append(word)\n                             \n            encoded_texts.append(temp_codes)\n            texts4encoding.append(temp_words)\n\n        self.vector_size = np.max([len(x) for x in encoded_texts])\n        self.X_encoded_texts = encoded_texts\n        self.texts4encoding = texts4encoding\n  \n    def codes_padding(self):\n        pad_idx = 0\n        padded_codes = list()\n        \n        codes_from_texts = copy.deepcopy(self.X_encoded_texts)\n        for encoded_text in codes_from_texts:\n            while len(encoded_text) < self.vector_size:\n                encoded_text.append(pad_idx)\n            padded_codes.append(encoded_text)\n\n        self.X_padded_codes = np.array(padded_codes)","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1700695586796,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import copy\n\nclass Encoding:\n    \n    def __init__(self, lemmatized_texts, num_words):\n        \n        self.X_lemmatized = lemmatized_texts\n        self.num_words = num_words\n        self.vector_size = None\n        self.fdist = None\n        self.X_encoded_texts = None\n        self.text4encoding = None\n        self.X_padded_codes = None\n    \n    def text_encoding(self):\n\n        vocabulary = dict()\n        fdist = nltk.FreqDist()  \n        \n        for tokens in self.X_lemmatized:  \n            for word in tokens:\n                fdist[word] += 1\n        \n        self.fdist = fdist\n        common_words = fdist.most_common(self.num_words)\n\n        for idx, word in enumerate(common_words):\n            vocabulary[word[0]] = (idx+1)\n        \n        self.vocabulary = vocabulary\n      \n        encoded_texts = list()\n        texts4encoding = list()\n        \n        for tokens in self.X_lemmatized:\n            temp_codes = list()\n            temp_words = list()\n            \n            for word in tokens:\n                if word in self.vocabulary.keys():\n                    temp_codes.append(self.vocabulary[word])\n                    temp_words.append(word)\n                             \n            encoded_texts.append(temp_codes)\n            texts4encoding.append(temp_words)\n\n        self.vector_size = np.max([len(x) for x in encoded_texts])\n        self.X_encoded_texts = encoded_texts\n        self.texts4encoding = texts4encoding\n  \n    def codes_padding(self):\n        pad_idx = 0\n        padded_codes = list()\n        \n        codes_from_texts = copy.deepcopy(self.X_encoded_texts)\n        for encoded_text in codes_from_texts:\n            while len(encoded_text) < self.vector_size:\n                encoded_text.append(pad_idx)\n            padded_codes.append(encoded_text)\n\n        self.X_padded_codes = np.array(padded_codes)"},"cell_type":"code","id":"2fa936da-75f7-4077-8231-a5eb67f7820c","execution_count":2,"outputs":[]},{"source":"from torch.utils.data import Dataset, DataLoader\n\nclass DatasetMapping(Dataset):\n\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n      \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n    \nfrom sklearn.model_selection import train_test_split\n\nclass   DatasetLoading:\n    \n    def __init__(self, padded_codes, targets):\n        \n        self.X = padded_codes\n        self.y = targets\n        self.X_train = None\n        self.y_train = None\n        self.X_test = None\n        self.y_test = None\n        \n    def data_split(self):\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.20, random_state=20231116)    \n\n    def data_mapping(self):\n        \n        self.train = DatasetMapping(self.X_train, self.y_train)\n        self.test = DatasetMapping(self.X_test, self.y_test)\n\n    def data_loading(self):\n        self.loader_train = DataLoader(self.train, batch_size=params.batch_size)\n        self.loader_test = DataLoader(self.test, batch_size=params.batch_size)  ","metadata":{"executionCancelledAt":null,"executionTime":798,"lastExecutedAt":1700695597982,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from torch.utils.data import Dataset, DataLoader\n\nclass DatasetMapping(Dataset):\n\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n      \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n    \nfrom sklearn.model_selection import train_test_split\n\nclass   DatasetLoading:\n    \n    def __init__(self, padded_codes, targets):\n        \n        self.X = padded_codes\n        self.y = targets\n        self.X_train = None\n        self.y_train = None\n        self.X_test = None\n        self.y_test = None\n        \n    def data_split(self):\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.20, random_state=20231116)    \n\n    def data_mapping(self):\n        \n        self.train = DatasetMapping(self.X_train, self.y_train)\n        self.test = DatasetMapping(self.X_test, self.y_test)\n\n    def data_loading(self):\n        self.loader_train = DataLoader(self.train, batch_size=params.batch_size)\n        self.loader_test = DataLoader(self.test, batch_size=params.batch_size)  "},"cell_type":"code","id":"45fa8d7c-8d4e-4ed1-b0bf-0f0449e36e4e","execution_count":3,"outputs":[]},{"source":"from dataclasses import dataclass\n\n@dataclass\nclass Parameters:\n    # Preprocessing parameters\n    vector_size: int = 25   # standard length of each row vector in the input\n    num_words: int = 9300  # number of words in the vocabulary\n    test_size = 0.20         \n    random_state = 42\n   \n    # Model parameters\n    embedding_dim: int = 256\n    num_layers: int = 2 # number of lstm layers\n    #out_size: int = 32\n    #tride: int = 2\n    #dilation: int = 2\n   \n    \n    # Training parameters\n    epochs: int = 20\n    batch_size: int = 128\n    learning_rate: float = 0.001\n    dropout: float = 0.5\n    \nparams=Parameters()","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1700698663145,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from dataclasses import dataclass\n\n@dataclass\nclass Parameters:\n    # Preprocessing parameters\n    vector_size: int = 25   # standard length of each row vector in the input\n    num_words: int = 9300  # number of words in the vocabulary\n    test_size = 0.20         \n    random_state = 42\n   \n    # Model parameters\n    embedding_dim: int = 256\n    num_layers: int = 2 # number of lstm layers\n    #out_size: int = 32\n    #tride: int = 2\n    #dilation: int = 2\n   \n    \n    # Training parameters\n    epochs: int = 20\n    batch_size: int = 128\n    learning_rate: float = 0.001\n    dropout: float = 0.5\n    \nparams=Parameters()"},"cell_type":"code","id":"f5dc478f-474c-43ae-8202-3d95afa42957","execution_count":83,"outputs":[]},{"source":"### Step 1. Preprocessing\ndata = Preprocessing()\ndata.load_data()\ndata.clean_text()\ndata.text_tokenized()\ndata.text_stopwords_removed()\ndata.text_lemmatized()\n\n### Step 2. Encoding\ncode = Encoding(data.X_lemmatized, params.num_words)\ncode.text_encoding()\ncode.codes_padding()\n\n### Step 3. Dataset and DataLoader\ndsl = DatasetLoading(code.X_padded_codes, data.y)\ndsl.data_split()\ndsl.data_mapping()\ndsl.data_loading()","metadata":{"executionCancelledAt":null,"executionTime":3510,"lastExecutedAt":1700695617133,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"### Step 1. Preprocessing\ndata = Preprocessing()\ndata.load_data()\ndata.clean_text()\ndata.text_tokenized()\ndata.text_stopwords_removed()\ndata.text_lemmatized()\n\n### Step 2. Encoding\ncode = Encoding(data.X_lemmatized, params.num_words)\ncode.text_encoding()\ncode.codes_padding()\n\n### Step 3. Dataset and DataLoader\ndsl = DatasetLoading(code.X_padded_codes, data.y)\ndsl.data_split()\ndsl.data_mapping()\ndsl.data_loading()"},"cell_type":"code","id":"d6e65bed-0a2e-40f1-8431-67396e372e08","execution_count":5,"outputs":[]},{"source":"print(f\"input length: {len(data.X_lemmatized)}\")\nprint(f\"               {len(code.X_padded_codes)}\")\nprint(f\"total number of tokens:  {np.sum([len(x) for x in data.X_lemmatized])}\")\nprint(f\"                         {sum(code.fdist.values())}\")\nprint(f\"total number of unique tokens: {len(code.fdist)}\")\nprint(f\"length of vocabulary: {len(code.vocabulary)}\")\nprint(f\"max number of tokens per row: {np.max([len(x) for x in data.X_lemmatized])}\")\nprint(f\"                              {set([len(x) for x in code.X_padded_codes])}\")","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1700697612586,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(f\"input length: {len(data.X_lemmatized)}\")\nprint(f\"               {len(code.X_padded_codes)}\")\nprint(f\"total number of tokens:  {np.sum([len(x) for x in data.X_lemmatized])}\")\nprint(f\"                         {sum(code.fdist.values())}\")\nprint(f\"total number of unique tokens: {len(code.fdist)}\")\nprint(f\"length of vocabulary: {len(code.vocabulary)}\")\nprint(f\"max number of tokens per row: {np.max([len(x) for x in data.X_lemmatized])}\")\nprint(f\"                              {set([len(x) for x in code.X_padded_codes])}\")\nprint(f\"X_train shape: {dsl.X_train.shape}\")\nprint(f\"X_test shape: {dsl.X_test.shape}\")\nprint(f\"y_train shape: {dsl.y_train.shape}\")\nprint(f\"y_test shape: {dsl.y_test.shape}\")","outputsMetadata":{"0":{"height":251,"type":"stream"}}},"cell_type":"code","id":"a771272c-b7e9-4ad7-be2f-6baa36efb330","execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":"input length: 7613\n               7613\ntotal number of tokens:  76462\n                         76462\ntotal number of unique tokens: 19999\nlength of vocabulary: 9300\nmax number of tokens per row: 25\n                              {25}\nX_train shape: (6090, 25)\nX_test shape: (1523, 25)\ny_train shape: (6090,)\ny_test shape: (1523,)\n"}]},{"source":"for x_batch, y_batch in dsl.loader_train:\n    print(x_batch.size(), y_batch.size())","metadata":{"executionCancelledAt":null,"executionTime":29,"lastExecutedAt":1700697639232,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"for x_batch, y_batch in dsl.loader_train:\n    print(x_batch.size(), y_batch.size())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"469556ee-1736-4963-992e-27f04bcd6f2c","execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":"torch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([74, 25]) torch.Size([74])\n"}]},{"source":"print(f\"\\nbatch_size:  {params.batch_size}\")\nprint(f\"X_train shape: {dsl.X_train.shape}\")\nprint(f\"y_train: (quotient, remainder) = {divmod(len(dsl.X_train), 128)}\")\nprint(f\"         128 x 47 + 74 = {128*47 + 74}\")\nprint(f\"\\nX_test shape: {dsl.X_test.shape}\")\nprint(f\"\\y_test: (quotient, remainder) = {divmod(len(dsl.X_test), 128)}\")\nprint(f\"         128 x 11 + 115 = {128*11 + 115}\")","metadata":{"executionCancelledAt":null,"executionTime":12,"lastExecutedAt":1700698534089,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(f\"\\nbatch_size:  {params.batch_size}\")\nprint(f\"X_train shape: {dsl.X_train.shape}\")\nprint(f\"y_train: (quotient, remainder) = {divmod(len(dsl.X_train), 128)}\")\nprint(f\"         128 x 47 + 74 = {128*47 + 74}\")\nprint(f\"\\nX_test shape: {dsl.X_test.shape}\")\nprint(f\"\\y_test: (quotient, remainder) = {divmod(len(dsl.X_test), 128)}\")\nprint(f\"         128 x 11 + 115 = {128*11 + 115}\")","outputsMetadata":{"0":{"height":193,"type":"stream"}}},"cell_type":"code","id":"058d707d-73ab-436f-9b49-5897b6802012","execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":"\nbatch_size:  128\nX_train shape: (6090, 25)\ny_train: (quotient, remainder) = (47, 74)\n         128 x 47 + 74 = 6090\n\nX_test shape: (1523, 25)\n\\y_test: (quotient, remainder) = (11, 115)\n         128 x 11 + 115 = 1523\n"}]},{"source":"for x_batch, y_batch in dsl.loader_test:\n    print(x_batch.size(), y_batch.size())","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":251,"type":"stream"}}},"cell_type":"code","id":"4af36c2f-2016-4bde-8ec5-5c7caa85dc52","execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":"torch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([128, 25]) torch.Size([128])\ntorch.Size([115, 25]) torch.Size([115])\n"}]},{"source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LSTMNet(nn.ModuleList):\n\n    def __init__(self, params):\n        super(LSTMNet, self).__init__()\n        \n        self.batch_size = params.batch_size\n        self.hidden_dim = params.embedding_dim\n        self.num_layers = params.num_layers\n        self.input_size = params.num_words+1\n        \n        self.dropout = nn.Dropout(0.5)\n        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True)\n        \n        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=256)\n        self.fc2 = nn.Linear(256, 1)\n        \n    def forward(self, x):\n        \n        h = torch.zeros((self.num_layers, x.size(0), self.hidden_dim))\n        c = torch.zeros((self.num_layers, x.size(0), self.hidden_dim))\n        \n        torch.nn.init.xavier_normal_(h)\n        torch.nn.init.xavier_normal_(c)\n        \n        out = self.embedding(x)\n        out, (hidden, cell) = self.lstm(out, (h,c))\n        out = self.dropout(out)\n        out = torch.relu_(self.fc1(out[:,-1,:]))\n        out = self.dropout(out)\n        out = torch.sigmoid(self.fc2(out))\n        return out","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1700699135497,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LSTMNet(nn.ModuleList):\n\n    def __init__(self, params):\n        super(LSTMNet, self).__init__()\n        \n        self.batch_size = params.batch_size\n        self.hidden_dim = params.embedding_dim\n        self.num_layers = params.num_layers\n        self.input_size = params.num_words+1\n        \n        self.dropout = nn.Dropout(0.5)\n        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, batch_first=True)\n        \n        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=256)\n        self.fc2 = nn.Linear(256, 1)\n        \n    def forward(self, x):\n        \n        h = torch.zeros((self.num_layers, x.size(0), self.hidden_dim))\n        c = torch.zeros((self.num_layers, x.size(0), self.hidden_dim))\n        \n        torch.nn.init.xavier_normal_(h)\n        torch.nn.init.xavier_normal_(c)\n        \n        out = self.embedding(x)\n        out, (hidden, cell) = self.lstm(out, (h,c))\n        out = self.dropout(out)\n        out = torch.relu_(self.fc1(out[:,-1,:]))\n        out = self.dropout(out)\n        out = torch.sigmoid(self.fc2(out))\n        return out"},"cell_type":"code","id":"2f2af0f8-17a2-417a-9c88-385027db12ab","execution_count":108,"outputs":[]},{"source":"import math\nimport torch.optim as optim\n\nlstm_model = LSTMNet(params)\noptimizer = optim.RMSprop(lstm_model.parameters(), lr=params.learning_rate)\n\nloader_train = dsl.loader_train\nloader_test = dsl.loader_test\ny_train = dsl.y_train\ny_test = dsl.y_test\n\n\n# Starts training phase\nfor epoch in range(params.epochs):\n    \n    # Set model in training model\n    lstm_model.train()\n    train_predictions = []\n    \n    for x_batch, y_batch in loader_train:\n\n        y_batch = y_batch.type(torch.FloatTensor)\n            \n        # Feed the model\n        y_pred = lstm_model(x_batch)\n            \n        # Reshape y_pred to a vector\n        y_pred = y_pred.view(-1)\n         \n        # Loss calculation\n        loss = F.binary_cross_entropy(y_pred, y_batch)\n         \n        # Clean gradientes\n        optimizer.zero_grad()\n         \n        # Gradients calculation\n        loss.backward()\n         \n        # Gradients update\n        optimizer.step()\n         \n        # Save predictions\n        train_predictions += list(y_pred.detach().numpy())\n        \n    #Metrics calculation for train accuracy\n     \n    true_positives = 0\n    true_negatives = 0\n    \n    for true, pred in zip(y_train, train_predictions):\n        if (pred >= 0.5) and (true == 1):\n            true_positives += 1\n        elif (pred < 0.5) and (true == 0):\n            true_negatives += 1\n        else:\n        \tpass\n    train_accuracy = (true_positives + true_negatives) / len(y_train)\n    \n    # Metrics calculation for test accuracy\n    \n    # Set the model in evaluation mode\n    lstm_model.eval()\n    test_predictions = []\n    \n    # Start evaluation phase\n    with torch.no_grad():\n        for x_batch, y_batch in loader_test:\n            y_pred = lstm_model(x_batch)\n            test_predictions += list(y_pred.detach().numpy())\n    \n    true_positives = 0\n    true_negatives = 0\n    \n    for true, pred in zip(y_test, test_predictions):\n        if (pred >= 0.5) and (true == 1):\n            true_positives += 1\n        elif (pred < 0.5) and (true == 0):\n            true_negatives += 1\n        else:\n        \tpass\n    test_accuracy = (true_positives + true_negatives) / len(y_test)\n     \n    \n    \n    print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuracy, test_accuracy))\n        ","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":407,"type":"stream"}}},"cell_type":"code","id":"a60286ed-d296-44a8-be9f-92238c6f0e94","execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 1, loss: 0.67631, Train accuracy: 0.56470, Test accuracy: 0.58569\nEpoch: 2, loss: 0.65785, Train accuracy: 0.58046, Test accuracy: 0.62180\nEpoch: 3, loss: 0.52507, Train accuracy: 0.73120, Test accuracy: 0.73999\nEpoch: 4, loss: 0.42199, Train accuracy: 0.81149, Test accuracy: 0.75378\nEpoch: 5, loss: 0.30641, Train accuracy: 0.86289, Test accuracy: 0.74064\nEpoch: 6, loss: 0.23199, Train accuracy: 0.88982, Test accuracy: 0.75246\nEpoch: 7, loss: 0.32090, Train accuracy: 0.90591, Test accuracy: 0.75115\nEpoch: 8, loss: 0.29578, Train accuracy: 0.92217, Test accuracy: 0.74524\nEpoch: 9, loss: 0.22986, Train accuracy: 0.93596, Test accuracy: 0.73933\nEpoch: 10, loss: 0.17212, Train accuracy: 0.94138, Test accuracy: 0.74261\nEpoch: 11, loss: 0.18812, Train accuracy: 0.94516, Test accuracy: 0.74984\nEpoch: 12, loss: 0.09530, Train accuracy: 0.95107, Test accuracy: 0.76494\nEpoch: 13, loss: 0.11883, Train accuracy: 0.94910, Test accuracy: 0.75312\nEpoch: 14, loss: 0.14110, Train accuracy: 0.95813, Test accuracy: 0.75837\nEpoch: 15, loss: 0.07334, Train accuracy: 0.96207, Test accuracy: 0.74852\nEpoch: 16, loss: 0.13031, Train accuracy: 0.96502, Test accuracy: 0.73605\nEpoch: 17, loss: 0.08886, Train accuracy: 0.96092, Test accuracy: 0.75246\nEpoch: 18, loss: 0.09270, Train accuracy: 0.96453, Test accuracy: 0.75312\nEpoch: 19, loss: 0.06271, Train accuracy: 0.96979, Test accuracy: 0.74852\nEpoch: 20, loss: 0.04890, Train accuracy: 0.97488, Test accuracy: 0.74984\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}