{"cells":[{"source":"# 3.3 Text data vectorization\n\nAfter *tokenizing, removing stopwords and punctuations, converting text to lowercase, and lemmatizing* the text data, the next common preprocessing steps for text data often include:\n\n*   **Count-based representation/Feature extraction**: This step involves converting the text data into a numerical format that can be used for machine learning algorithms. Two common approaches are:\n\n       a.   **Bag of Words** (**BoW**): This method creates a vocabulary of unique words in the corpus and represents each document as a vector indicating the frequency of each word's occurrence.  It ignores the order of words in the text.\n\n       b.  **Term Frequency-Inverse Document Frequency (TF-IDF)**: This technique takes into account not only the frequency of words in a document but also the importance of words in the entire corpus. It can help in identifying words that are distinctive to a document.\n\n<br>\n\n*   **Word embeddings**: If you want to capture semantic meaning and context, you can use pre-trained word embeddings like `Word2Vec`, `GloVe`, or `FastText`. These embeddings provide vector representations for words, where words with similar meanings have similar vectors. This step is especially useful for tasks like text classification, sentiment analysis, and machine translation.\n","metadata":{"executionCancelledAt":null,"executionTime":144,"lastExecutedAt":1697366186854,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 3.3 Text Data Vectorization\n\nAfter *tokenizing, removing stopwords and punctuations, converting text to lowercase, and lemmatizing* the text data, the next common preprocessing steps for text data often include:\n\n*   **Vectorization/Feature Extraction**: This step involves converting the text data into a numerical format that can be used for machine learning algorithms. Two common approaches are:\n\n    a.   **Bag of Words** (**BoW**): This method creates a vocabulary of unique words in the corpus and represents each document as a vector indicating the frequency of each word's occurrence.  It ignores the order of words in the text.\n\n    b.  **Term Frequency-Inverse Document Frequency (TF-IDF)**: This technique takes into account not only the frequency of words in a document but also the importance of words in the entire corpus. It can help in identifying words that are distinctive to a document.\n\n<br>\n\n***One Hot Encoding**  One-hot encoding is a common technique used in Natural Language Processing (NLP) to represent categorical data, such as words or characters, as binary vectors. In this encoding method, each unique category is represented by a binary vector where all elements are zero except for the element corresponding to the category, which is set to 1.\n\n<br>\n\n*   **Word Embeddings**: If you want to capture semantic meaning and context, you can use pre-trained word embeddings like `Word2Vec`, `GloVe`, or `FastText`. These embeddings provide vector representations for words, where words with similar meanings have similar vectors. This step is especially useful for tasks like text classification, sentiment analysis, and machine translation.\n"},"cell_type":"markdown","id":"748c1e03-e904-4709-8ed0-8cbbc1802646"},{"source":"## Bag of Words as count-based representation \n\nHere's how the BoW model works:\n\n**Vocabulary Creation**: The first step is to create a vocabulary, which is a list of all unique words in your corpus (collection of documents). Each word in the vocabulary is assigned a unique index.\n\n**Counting Word Occurrences**: For each document in your corpus, you count how many times each word in the vocabulary appears in that document.\n\n**Vector Representation**: You represent each document as a vector where each element corresponds to a word in the vocabulary, and the value of each element is the count of how many times that word appears in the document. These vectors are typically quite high-dimensional and sparse.","metadata":{},"cell_type":"markdown","id":"dc0c2dec-e704-40e8-88fa-796bd0920d86"},{"source":"**CountVectorizer** is a feature extraction method provided by the scikit-learn (sklearn) library, which is a popular machine learning library in Python. It is used for converting a collection of text documents into a matrix of token (word) counts. ","metadata":{},"cell_type":"markdown","id":"9912c097-8714-4d7c-8e65-5ac444fcaea3"},{"source":"# Import CountVectorizer in sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nannak = ['Happy families are all alike;', 'Every unhappy family is unhappy in its own way.']\n\n# Create an instance of CountVectorizer\nannak_vectorizer = CountVectorizer()\n\n# Fit your text data\nannak_vec = annak_vectorizer.fit(annak)\n\n# Transform your text data\nannak_bow = annak_vectorizer.transform(annak)\n\n# Extract the vocabulary\nvocabulary = annak_vectorizer.get_feature_names_out()","metadata":{"executionCancelledAt":null,"executionTime":394,"lastExecutedAt":1697418729762,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import CountVectorizer in sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nannak = ['Happy families are all alike;', 'Every unhappy family is unhappy in its own way.']\n\n# Create an instance of CountVectorizer\nannak_vectorizer = CountVectorizer()\n\n# Fit your text data\nannak_vec = annak_vectorizer.fit(annak)\n\n# Transform your text data\nannak_bow = annak_vectorizer.transform(annak)\n\n# Extract the vocabulary\nvocabulary = annak_vectorizer.get_feature_names_out()"},"cell_type":"code","id":"3e46d106-5944-4415-b620-872ae26d602b","execution_count":1,"outputs":[]},{"source":"print(\"annak_bow: \\n\", annak_bow, \"\\n\")\nprint(\"annak_bow.toarray(): \\n\", annak_bow.toarray(), \"\\n\")\nprint(\"vocabulary: \\n\",vocabulary)","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1697418729814,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"annak_bow: \\n\", annak_bow, \"\\n\")\nprint(\"annak_bow.toarray(): \\n\", annak_bow.toarray(), \"\\n\")\nprint(\"vocabulary: \\n\",vocabulary)","outputsMetadata":{"0":{"height":446,"type":"stream"}}},"cell_type":"code","id":"5e2093d8-df05-4da0-8e4f-822d2512c498","execution_count":2,"outputs":[]},{"source":"import pandas as pd\n\npd.set_option('display.max_columns', None)\nprint(pd.DataFrame(annak_bow.toarray(), columns = annak_vectorizer.get_feature_names()))","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":154,"type":"stream"}}},"cell_type":"code","id":"7c9fe8e1-62a6-45c8-998a-08d5d166204a","execution_count":3,"outputs":[]},{"source":"`from sklearn.feature_extraction.text import CountVectorizer`\n\n`CountVectorizer` implements both tokenization and occurrence counting in a single class.  It is used to convert a collection of text documents to a matrix of token (word) counts. \n\nHere are some of the most commonly used parameters:\n\n*   `input`: This parameter can be a list of strings, where each string represents a document, or it can be a filename that contains text data. It specifies the data that should be vectorized.\n*   `stop_words`: You can pass a list of words that should be considered as stop words and not included in the feature matrix. Stop words are common words like \"the,\" \"and,\" \"in,\" which are often removed to reduce noise in text data.\n*   `lowercase`: A boolean `(True`/`False`) parameter that determines whether the text should be converted to lowercase. This helps in treating words in a case-insensitive manner.\n*   `analyzer`: It defines what constitutes a \"token\" in the text. The default is \"word,\" but you can set it to \"char\" to consider character n-grams, or you can define a custom analyzer function.\n*   `ngram_range`: This parameter controls the range of n-grams to consider. An n-gram is a contiguous sequence of n items from the given text. For example, if you set `ngram_range` to `(1, 2)`, it will consider both `unigrams` (individual words) and `bigrams` (pairs of consecutive words).\n*   `max_features`: Limits the number of features (words) to be used. It can be set to an integer value to specify the maximum number of features, or you can use None to include all features.\n*   `max_df` and `min_df`: Parameters that control the maximum and minimum document frequency of tokens to be included in the vectorization process.\n*  ` binary`: If set to `True`, it will output binary values (`0` or `1`) for word presence or absence, instead of word counts. \n*  `token_pattern`: A regular expression pattern that defines what a token is. By default, it considers words as tokens.  \n*  `vocabulary`: You can pass a custom vocabulary as a dictionary where keys are terms (words) and values are the indices to be used in the matrix.\n\nThe output of `CountVectorizer` is typically a sparse matrix with *rows* representing documents and *columns* representing unique tokens (words or n-grams). Each cell in the matrix contains the *count* of how many times a particular token appears in a particular document.\n\n The attributes in the CountVectorizer output are as follows:\n \n*   `fit_transform()` or `transform()`: These are methods used to convert a collection of text documents into the matrix of token counts. You apply these methods to your text data to get the transformed representation.\n\n*   `vocabulary_`: This attribute is a dictionary that maps the unique tokens (words or n-grams) to their respective column indices in the matrix. It provides a way to understand which column corresponds to which token.\n\n*   `stop_words_`: If stop words were removed during the vectorization process, this attribute contains the list of stop words that were removed.\n\n*   `get_feature_names()`: This method returns an array of feature names, which corresponds to the columns in the transformed matrix. The feature names are typically the unique tokens.\n\n*   `inverse_transform()`: This method allows you to revert the transformed data back to its original text form. It takes the transformed matrix as input and returns a list of text documents.\n\n\nThese attributes and methods allow you to inspect and work with the output of the `CountVectorizer` to better understand and utilize the vectorized representation of your text data.\n","metadata":{},"cell_type":"markdown","id":"85cc10e8-d67e-435a-85e6-ca8a82452348"},{"source":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nannak_vectorizer2 = CountVectorizer(stop_words = ENGLISH_STOP_WORDS)\nannak_vect2 = annak_vectorizer2.fit(annak)\nannak_bow2 = annak_vectorizer2.transform(annak)\n\nprint(pd.DataFrame(annak_bow2.toarray(), columns = annak_vectorizer2.get_feature_names()))","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1697418729917,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nannak_vectorizer2 = CountVectorizer(stop_words = ENGLISH_STOP_WORDS)\nannak_vect2 = annak_vectorizer2.fit(annak)\nannak_bow2 = annak_vectorizer2.transform(annak)\n\nprint(pd.DataFrame(annak_bow2.toarray(), columns = annak_vectorizer2.get_feature_names()))","outputsMetadata":{"0":{"height":76,"type":"stream"}}},"cell_type":"code","id":"af7ba8be-89d9-4430-8abf-f4f0e46aa62c","execution_count":4,"outputs":[]},{"source":"annak_vectorizer3 = CountVectorizer(ngram_range =(3,3))\nannak_bow3 = annak_vectorizer3.fit_transform(annak)\n\nprint(\"annak: \\n\", annak, \"\\n\")\nprint(pd.DataFrame(annak_bow3.toarray(), columns = annak_vectorizer3.get_feature_names()))","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1697418729965,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"annak_vectorizer3 = CountVectorizer(ngram_range =(3,3))\nannak_bow3 = annak_vectorizer3.fit_transform(annak)\n\nprint(\"annak: \\n\", annak, \"\\n\")\nprint(pd.DataFrame(annak_bow3.toarray(), columns = annak_vectorizer3.get_feature_names()))","outputsMetadata":{"0":{"height":290,"type":"stream"}}},"cell_type":"code","id":"3125c430-50e2-4aad-9c3b-e14afbe28d59","execution_count":5,"outputs":[]},{"source":"### Using the parameter `analyzer`","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null},"cell_type":"code","id":"3bc90904-26d4-454a-b0c3-c588cce16e1d","execution_count":6,"outputs":[]},{"source":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","metadata":{"executionCancelledAt":null,"executionTime":663,"lastExecutedAt":1697418730675,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","outputsMetadata":{"0":{"height":134,"type":"stream"}}},"cell_type":"code","id":"ee3409f1-d349-4216-981b-e210b6cc7721","execution_count":7,"outputs":[]},{"source":"import re\n\ndef set_clean(raw_text):\n    set_stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer()\n    \n    no_punc = re.sub(r'[^\\w\\s]', '', raw_text)\n    lowercase_no_punc = no_punc.lower()\n    tokenized_text= word_tokenize(lowercase_no_punc)\n    no_stop = [w for w in tokenized_text if w not in set_stop_words]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"v\") for word in no_stop]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"n\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"a\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"r\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"s\") for word in lc_text]\n    return(lc_text)","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1697418730724,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import re\n\ndef set_clean(raw_text):\n    set_stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer()\n    \n    no_punc = re.sub(r'[^\\w\\s]', '', raw_text)\n    lowercase_no_punc = no_punc.lower()\n    tokenized_text= word_tokenize(lowercase_no_punc)\n    no_stop = [w for w in tokenized_text if w not in set_stop_words]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"v\") for word in no_stop]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"n\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"a\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"r\") for word in lc_text]\n    lc_text = [lemmatizer.lemmatize(word, pos=\"s\") for word in lc_text]\n    return(lc_text)"},"cell_type":"code","id":"e4aa2b98-3cec-4c22-9013-2d60dcc8cc94","execution_count":8,"outputs":[]},{"source":"# Create an instance of CountVectorizer\nannak_vect3 = CountVectorizer(analyzer=set_clean)\n\n# Fit and transform your text data\nannak_bow3 = annak_vect3.fit_transform(annak)\n\nprint(pd.DataFrame(annak_bow3.toarray(), columns = annak_vect3.get_feature_names()))","metadata":{"executionCancelledAt":null,"executionTime":1245,"lastExecutedAt":1697418731970,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create an instance of CountVectorizer\nannak_vect3 = CountVectorizer(analyzer=set_clean)\n\n# Fit and transform your text data\nannak_bow3 = annak_vect3.fit_transform(annak)\n\nprint(pd.DataFrame(annak_bow3.toarray(), columns = annak_vect3.get_feature_names()))","outputsMetadata":{"0":{"height":76,"type":"stream"}}},"cell_type":"code","id":"57623ad3-e74b-4e4b-a3f9-1b2ea606d9a9","execution_count":9,"outputs":[]},{"source":"tweets = pd.read_csv(\"datasets/tweets_sample.csv\")\nprint(\"tweets.shape = \", tweets.shape)\nprint(\"tweets.columns = \", tweets.columns)","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1697418732021,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"tweets = pd.read_csv(\"datasets/tweets_sample.csv\")\nprint(\"tweets.shape = \", tweets.shape)\nprint(\"tweets.columns = \", tweets.columns)","outputsMetadata":{"0":{"height":56,"type":"stream"}}},"cell_type":"code","id":"f8d4c4d0-b856-435f-aec2-505650d1c125","execution_count":10,"outputs":[]},{"source":"# Create an instance of CountVectorizer\ntweets_vectorizer = CountVectorizer(analyzer=set_clean)\n\n# Fit and transform your text data\ntweets_bow = tweets_vectorizer.fit_transform(tweets.text)\n\n# Extract the vocabulary\ntweets_vocabulary = tweets_vectorizer.get_feature_names()\n\n# bag-of-words as a data frame\ntweets_bow_df = pd.DataFrame(tweets_bow.toarray(), columns = tweets_vocabulary)\n\nprint(\"tweets_bow_df.shape: \",tweets_bow_df.shape,\"\\n\")\nprint(\"last five columns: \\n\", tweets_bow_df.iloc[:, -5:])","metadata":{"executionCancelledAt":null,"executionTime":66,"lastExecutedAt":1697418732087,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create an instance of CountVectorizer\ntweets_vectorizer = CountVectorizer(analyzer=set_clean)\n\n# Fit and transform your text data\ntweets_bow = tweets_vectorizer.fit_transform(tweets.text)\n\n# Extract the vocabulary\ntweets_vocabulary = tweets_vectorizer.get_feature_names()\n\n# bag-of-words as a data frame\ntweets_bow_df = pd.DataFrame(tweets_bow.toarray(), columns = tweets_vocabulary)\n\nprint(\"tweets_bow_df.shape: \",tweets_bow_df.shape,\"\\n\")\nprint(\"last five columns: \\n\", tweets_bow_df.iloc[:, -5:])","outputsMetadata":{"0":{"height":349,"type":"stream"}}},"cell_type":"code","id":"53f21b3c-6df3-412d-bbdb-483930f4c5ed","execution_count":11,"outputs":[]},{"source":"### TFIDF as count-based representation\n\nIn scikit-learn, the `TfidfVectorizer` is a feature extraction method that is used to transform a collection of text documents into a matrix of` TF-IDF` (**Term Frequency-Inverse Document Frequency**) features. \n\nTF-IDF is a statistical measure used in natural language processing and information retrieval to evaluate the importance of a word within a document relative to a collection of documents (corpus).\n\n$$\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)$$\n\nwhere \n\n*   **Term Frequency** (**TF**): This measures how frequently a term occurs in a specific document. It is calculated as the number of times a term appears in a document divided by the total number of terms in that document. This can be expressed as:\n\n$$\\text{TF}(t, d) = (\\text{Number of times term } t \\text{ appears in document } d) / (\\text{Total number of terms in document } d)$$\n\n*   **Inverse Document Frequency** (**IDF**): This measures how important a term is within the entire corpus. It is calculated as the logarithm of the total number of documents in the corpus divided by the number of documents containing the term, with a small constant added to prevent division by zero. The formula is:\n\n$$\\text{IDF}(t) = \\log((\\text{Total number of documents in the corpus}) / (\\text{Number of documents containing term }t)) + 1$$","metadata":{},"cell_type":"markdown","id":"d273be41-a5d4-4676-b12a-ebeb3d1d5e4c"},{"source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create an instance of TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(analyzer = set_clean)\n\n# Fit and transform your text data\nannak_tfidf = tfidf_vectorizer.fit_transform(annak)\n\n# Extract the vocabulary\nvocabulary = tfidf_vectorizer.get_feature_names_out()\n\nprint(\"annak: \\n\", annak, \"\\n\")\nprint(\"vocabulary: \", vocabulary, \"\\n\")\n\nprint(pd.DataFrame(annak_tfidf.toarray(), columns = tfidf_vectorizer.get_feature_names()))","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1697418732137,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create an instance of TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(analyzer = set_clean)\n\n# Fit and transform your text data\nannak_tfidf = tfidf_vectorizer.fit_transform(annak)\n\n# Extract the vocabulary\nvocabulary = tfidf_vectorizer.get_feature_names_out()\n\nprint(\"annak: \\n\", annak, \"\\n\")\nprint(\"vocabulary: \", vocabulary, \"\\n\")\n\nprint(pd.DataFrame(annak_tfidf.toarray(), columns = tfidf_vectorizer.get_feature_names()))","outputsMetadata":{"0":{"height":173,"type":"stream"}}},"cell_type":"code","id":"e0f8212b-546e-4764-a166-dcabfa6deec6","execution_count":12,"outputs":[]},{"source":"# Tfidf on the tweets dataset\ntweets_tfidf_vect = TfidfVectorizer(analyzer=set_clean)\n\n# Fit and transform your text data\ntweets_tfidf = tweets_tfidf_vect.fit_transform(tweets.text)\n\n# Extract the vocabulary\ntweets_vocabulary = tweets_tfidf_vect.get_feature_names_out()\n\nprint(\"tweets_vocabulary shape:\", tweets_vocabulary.shape,\"\\n\")\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nprint(\"Word vectors of 5 tokens: \\n\",pd.DataFrame(tweets_tfidf.toarray(), columns=tweets_tfidf_vect.get_feature_names()).iloc[:, -15:-10])","metadata":{"executionCancelledAt":null,"executionTime":102,"lastExecutedAt":1697418732240,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Tfidf on the tweets dataset\ntweets_tfidf_vect = TfidfVectorizer(analyzer=set_clean)\n\n# Fit and transform your text data\ntweets_tfidf = tweets_tfidf_vect.fit_transform(tweets.text)\n\n# Extract the vocabulary\ntweets_vocabulary = tweets_tfidf_vect.get_feature_names_out()\n\nprint(\"tweets_vocabulary shape:\", tweets_vocabulary.shape,\"\\n\")\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nprint(\"Word vectors of 5 tokens: \\n\",pd.DataFrame(tweets_tfidf.toarray(), columns=tweets_tfidf_vect.get_feature_names()).iloc[:, -15:-10])","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"f31c60ec-1d7b-4a45-91dd-f3d52742b20b","execution_count":13,"outputs":[]},{"source":"## One-hot encoding for text data vectorization","metadata":{},"cell_type":"markdown","id":"2fc04b0e-7af3-497c-b29f-97094f211fd7"},{"source":"annak_tokens = [set_clean(line) for line in annak]\n\nprint(\"annak_tokens: \\n\", annak_tokens,\"\\n\")\n\n# One-hot encoding with pandas get_dummies function\nprint(\"word vectors with one-hot encording:\")\n\n[pd.get_dummies(tokens) for tokens in annak_tokens]","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1697418732289,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"annak_tokens = [set_clean(line) for line in annak]\n\nprint(\"annak_tokens: \\n\", annak_tokens,\"\\n\")\n\n# One-hot encoding with pandas get_dummies function\nprint(\"word vectors with one-hot encording:\")\n\n[pd.get_dummies(tokens) for tokens in annak_tokens]","outputsMetadata":{"0":{"height":95,"type":"stream"}}},"cell_type":"code","id":"b0a39650-b1f7-4bc5-a795-29a95bc76597","execution_count":14,"outputs":[]},{"source":"annak_str = ' '.join(annak)\nannak_str_tokens = word_tokenize(annak_str)\nprint(\"annak_str_tokens: \\n\", annak_str_tokens,\"\\n\")\nprint(pd.get_dummies(annak_str_tokens))","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1697418732341,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"annak_str = ' '.join(annak)\nannak_str_tokens = word_tokenize(annak_str)\nprint(\"annak_str_tokens: \\n\", annak_str_tokens,\"\\n\")\nprint(pd.get_dummies(annak_str_tokens))","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"7cd58f53-df1e-4ac6-8c15-27dc0c1a37e2","execution_count":15,"outputs":[]},{"source":"### Word embeddings for text data vectorization\n\nWord embeddings are used to convert words or phrases into dense vectors of real numbers, typically with hundreds of dimensions, where each dimension represents a feature of the word or its context. \n\n* `GloVe`, short for \"Global Vectors for Word Representation,\" is a popular and widely used word embedding model in natural language processing (NLP). It is developed by researchers at Stanford University.\n\n Here are some of the key features and concepts associated with GloVe:\n\n*   **Vector Space Model**: GloVe is based on the Vector Space Model (VSM) of word semantics. It operates on the idea that words that frequently co-occur in similar contexts have related meanings.\n\n*   **Word Co-occurrence**: GloVe takes a co-occurrence matrix as input, where each element of the matrix represents the number of times a word occurs in the context of another word within a specified window of text. This matrix is derived from a large corpus of text.\n\n*   **Objective Function**: GloVe's objective function aims to minimize the difference between the dot product of word vectors and the logarithm of the word co-occurrence probabilities. It formulates word vectors in a way that words with similar meanings have similar vectors.\n\n*   **Training**: GloVe is trained using a global approach, considering the entire corpus of text, as opposed to the local context window approach used in Word2Vec. It leverages statistical information from the entire dataset for word embedding creation.\n\n*   **Dimensionality**: Users can specify the dimensionality of the word embeddings when training GloVe, typically ranging from 50 to 300 dimensions, depending on the application and available resources.\n\nReference and downloads: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)","metadata":{},"cell_type":"markdown","id":"4e8cc433-0a4a-4024-bc5d-a5732d7705c6"},{"source":"import numpy as np\n\n# Define the path to your downloaded GloVe file\nglove_file = 'datasets/glove.6B.50d.txt'  # Example file for 50-dimensional vectorsw","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1697418732388,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import numpy as np\n\n# Define the path to your downloaded GloVe file\nglove_file = 'datasets/glove.6B.50d.txt'  # Example file for 50-dimensional vectorsw"},"cell_type":"code","id":"655a9f27-98a1-49bb-9042-ec777bcdbf47","execution_count":16,"outputs":[]},{"source":"#Load the GloVe word vectors into a dictionary\ndef load_glove_vectors(filename):\n    word_vectors = {}\n    with open(filename, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split()\n            word = parts[0]\n            vector = np.array(parts[1:], dtype=np.float32)\n            word_vectors[word] = vector\n    return word_vectors","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1697418732436,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Load the GloVe word vectors into a dictionary\ndef load_glove_vectors(filename):\n    word_vectors = {}\n    with open(filename, 'r', encoding='utf-8') as f:\n        for line in f:\n            parts = line.strip().split()\n            word = parts[0]\n            vector = np.array(parts[1:], dtype=np.float32)\n            word_vectors[word] = vector\n    return word_vectors"},"cell_type":"code","id":"2d540ecb-2cf6-4091-936c-228791d09ac1","execution_count":17,"outputs":[]},{"source":"glove_vectors = load_glove_vectors(glove_file)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null},"cell_type":"code","id":"cbcdc79a-9556-4f4f-b893-36de86e1b85d","execution_count":18,"outputs":[]},{"source":"# Access the vector for a specific word\nword = \"king\"\nif word in glove_vectors:\n    vector = glove_vectors[word]\n    print(f\"Vector for '{word}':\\n{vector}\")","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1697418735788,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Access the vector for a specific word\nword = \"king\"\nif word in glove_vectors:\n    vector = glove_vectors[word]\n    print(f\"Vector for '{word}':\\n{vector}\")","outputsMetadata":{"0":{"height":193,"type":"stream"}}},"cell_type":"code","id":"d740f72b-fa95-4009-ba2d-6d99d33cf6d9","execution_count":19,"outputs":[]},{"source":"# !pip install pip install scikit-learn\n# Compute the cosine similarity between two words\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1697418735836,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# !pip install pip install scikit-learn\n# Compute the cosine similarity between two words\nfrom sklearn.metrics.pairwise import cosine_similarity"},"cell_type":"code","id":"22eb165e-4776-440c-90fe-2165224b5bd1","execution_count":20,"outputs":[]},{"source":"word1 = \"king\"\nword2 = \"queen\"\nif word1 in glove_vectors and word2 in glove_vectors:\n    similarity = cosine_similarity([glove_vectors[word1]], [glove_vectors[word2]])[0][0]\n    print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1697418735885,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"word1 = \"king\"\nword2 = \"queen\"\nif word1 in glove_vectors and word2 in glove_vectors:\n    similarity = cosine_similarity([glove_vectors[word1]], [glove_vectors[word2]])[0][0]\n    print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"9076829f-865b-4a87-94b3-38f7e1fdb302","execution_count":21,"outputs":[]},{"source":"Here's a step-by-step guide on how to apply GloVe to text data:\n\n*   **Download Pre-trained GloVe Vectors**: You can download pre-trained GloVe vectors from the official website or other sources. GloVe comes in different dimensions (e.g., 50D, 100D, 300D), and you should choose the one that best suits your needs. A larger dimension generally captures more semantic information but requires more memory.\n\n*   **Load GloVe Vectors**: You can load the pre-trained GloVe vectors into your Python script using a library like gensim or simply by reading the text file. Here's an example using `gensim`:","metadata":{},"cell_type":"markdown","id":"e30188cd-21dc-472f-a222-e4a4540f757d"},{"source":"from gensim.models import KeyedVectors\n\n# Load pre-trained GloVe word vectors\nglove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)","metadata":{"executionCancelledAt":null,"executionTime":10848,"lastExecutedAt":1697418746734,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from gensim.models import KeyedVectors\n\n# Load pre-trained GloVe word vectors\nglove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"},"cell_type":"code","id":"2396e822-9429-4b2c-83c0-8e6c08de1e7a","execution_count":22,"outputs":[]},{"source":"def text_to_vector(text, model):\n    words = text.split()\n    word_vectors = [model[word] for word in words if word in model]\n    #if not word_vectors:\n    #    return None\n    #text_vector = sum(word_vectors) / len(word_vectors)\n    words_list = [word for word in words if word in model]\n    return words_list, word_vectors","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1697418746784,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def text_to_vector(text, model):\n    words = text.split()\n    word_vectors = [model[word] for word in words if word in model]\n    #if not word_vectors:\n    #    return None\n    #text_vector = sum(word_vectors) / len(word_vectors)\n    words_list = [word for word in words if word in model]\n    return words_list, word_vectors"},"cell_type":"code","id":"2fd1a750-0ed8-4ab8-9d01-24d6c9c30594","execution_count":23,"outputs":[]},{"source":"# Text to be converted to word vectors\ntext = \"This is a sample sentence.\"\n\n# Convert text to word vector\nvector = text_to_vector(text, glove_model)\nprint(vector[0], \"\\n\")\n\nprint(\"is: \\n\", vector[1][0],\"\\n\")\nprint(\"a: \\n\", vector[1][1],\"\\n\")\nprint(\"sample: \\n\", vector[1][2])\n","metadata":{"executionCancelledAt":null,"executionTime":57,"lastExecutedAt":1697418746842,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Text to be converted to word vectors\ntext = \"This is a sample sentence.\"\n\n# Convert text to word vector\nvector = text_to_vector(text, glove_model)\nprint(vector[0], \"\\n\")\n\nprint(\"is: \\n\", vector[1][0],\"\\n\")\nprint(\"a: \\n\", vector[1][1],\"\\n\")\nprint(\"sample: \\n\", vector[1][2])\n","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"e679f978-bb6b-433a-8f6d-0de2e7e86a0c","execution_count":24,"outputs":[]},{"source":"glove_model.similar_by_vector(vector[1][2], topn=1)","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1697418746893,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"glove_model.similar_by_vector(vector[1][2], topn=1)"},"cell_type":"code","id":"9ec047b3-1532-4ffd-a1cf-858083f21abd","execution_count":25,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}