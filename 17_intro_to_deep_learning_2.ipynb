{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632a1cd3-68cc-4f93-8901-fc6c85ef46d6",
   "metadata": {},
   "source": [
    "# 5.2 Introduction to Deep Learning, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c572f-d5ad-481f-b921-9935b409b72d",
   "metadata": {},
   "source": [
    "## 5.2.1 Multiclass Multiclassification Problem\n",
    "\n",
    "* input has 4 samples and 5 features\n",
    "* target contains 3 classes\n",
    "* one hidden layer, two linear neural networks\n",
    "* use the softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff4a9d6-73f8-4f48-a480-c2532d7b3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# input data with 4  samples and 5 features\n",
    "input = torch.tensor([\n",
    "    [ 0.7550,  0.2580, -0.0376, -0.5695,  0.0454],\n",
    "    [-0.6897,  1.4822,  0.7860, -2.0889, -0.4481],\n",
    "    [ 0.4464,  0.7335, -0.9837,  1.7818, -0.9048],\n",
    "    [-0.7997,  1.1157, -0.4644,  1.1144, -0.6903]\n",
    "])\n",
    "\n",
    "# target with class number per observation\n",
    "mc_target = torch.tensor([2,2,1,0])\n",
    "\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54c68cb8-ce9c-4488-a14b-f0609ddfeec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_activation: \n",
      " tensor([[ 0.4926, -0.1480,  0.2304],\n",
      "        [ 0.8385,  0.4200,  0.9452],\n",
      "        [ 0.1639, -0.0355, -0.1235],\n",
      "        [ 0.3077,  0.4059,  0.1616]], grad_fn=<AddmmBackward0>)\n",
      "output: \n",
      " tensor([[0.4355, 0.2295, 0.3350],\n",
      "        [0.3609, 0.2375, 0.4016],\n",
      "        [0.3892, 0.3188, 0.2920],\n",
      "        [0.3370, 0.3718, 0.2912]], grad_fn=<SoftmaxBackward0>)\n",
      "prediction: \n",
      " tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1]])\n",
      "target: \n",
      " tensor([2, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# simple neural network model with 1 hidden layer, two linear networks\n",
    "torch.manual_seed(11052023)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(5,4),\n",
    "    nn.Linear(4,3))\n",
    "\n",
    "# compute the pre-activation output\n",
    "mc_pre_activation = net(input)\n",
    "\n",
    "# apply the softmax activation function\n",
    "softmax = nn.Softmax(dim = -1)\n",
    "mc_output = softmax(mc_pre_activation)\n",
    "\n",
    "# predict the class per observation\n",
    "mc_prediction = torch.argmax(mc_output,dim=-1).view(-1,1)\n",
    "\n",
    "print(f\"pre_activation: \\n {mc_pre_activation}\")\n",
    "print(f\"output: \\n {mc_output}\")\n",
    "print(\"prediction: \\n\", mc_prediction)\n",
    "print(\"target: \\n\", mc_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bdf669a0-40cd-40ef-8b9e-6c7634fa19a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 0, 1]])\n",
      "tensor([[0, 2, 0, 1]])\n",
      "tensor([0, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "print(mc_prediction.view(1, 4))\n",
    "print(mc_prediction.view(1,-1))\n",
    "print(mc_prediction.resize_(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e1184370-fb38-4f9f-a525-6b6e6694cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:\n",
      " tensor([[0.4355, 0.2295, 0.3350],\n",
      "        [0.3609, 0.2375, 0.4016],\n",
      "        [0.3892, 0.3188, 0.2920],\n",
      "        [0.3370, 0.3718, 0.2912]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "target:\n",
      " tensor([2, 2, 1, 0])\n",
      "\n",
      " loss: 1.08565354347229\n"
     ]
    }
   ],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()\n",
    "print(f\"output:\\n {mc_output}\")\n",
    "print(f\"\\ntarget:\\n {mc_target}\")\n",
    "print(f\"\\n loss: {ce_loss(mc_output, mc_target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ab6e4-db07-4bf7-a6a0-9d4312a866b7",
   "metadata": {},
   "source": [
    "#### On Cross Entropy Loss\n",
    "\n",
    "$$\\text{CrossEntropyLoss} = -\\sum{ \\text{one-hot-target} \\times\\log({\\text{probas})}}$$\n",
    "\n",
    "where $\\text{probas} = \\text{softmax}(\\text{output})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "15065973-dfd9-44d3-a4c9-c32edecab608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probas:\n",
      " tensor([[0.3679, 0.2994, 0.3327],\n",
      "        [0.3418, 0.3021, 0.3560],\n",
      "        [0.3522, 0.3283, 0.3196],\n",
      "        [0.3344, 0.3462, 0.3194]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "target: tensor([2, 2, 1, 0])\n",
      "\n",
      "one_hot_target:\n",
      " tensor([[0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "\n",
      " CrossEntropyLoss = 1.08565354347229\n"
     ]
    }
   ],
   "source": [
    "probas = softmax(mc_output)\n",
    "one_hot_target = F.one_hot(mc_target.clone().detach(), mc_output.shape[1])\n",
    "print(f\"probas:\\n {probas}\")\n",
    "print(f\"\\ntarget: {mc_target}\")\n",
    "print(f\"\\none_hot_target:\\n {one_hot_target}\")\n",
    "print(f\"\\n CrossEntropyLoss = {-torch.sum(one_hot_target * torch.log(probas))/4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d52fe-565f-4cb0-be34-3cb6ce545e03",
   "metadata": {},
   "source": [
    "## 5.1.2 Regression Problem\n",
    "\n",
    "* input has 4 observations and 5 features\n",
    "* target is a numerical value.\n",
    "* one hidden layer, two linear neural networks\n",
    "* use the sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d042fe2b-7434-4ea2-8757-f37ab9adbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data with 4  observations and 5 features\n",
    "input = torch.tensor([\n",
    "    [ 0.7550,  0.2580, -0.0376, -0.5695,  0.0454],\n",
    "    [-0.6897,  1.4822,  0.7860, -2.0889, -0.4481],\n",
    "    [ 0.4464,  0.7335, -0.9837,  1.7818, -0.9048],\n",
    "    [-0.7997,  1.1157, -0.4644,  1.1144, -0.6903]\n",
    "])\n",
    "reg_target = torch.tensor(\n",
    "    [[3.1000],\n",
    "     [2.3000],\n",
    "     [1.2000],\n",
    "     [0.4000]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8e4ad12-af62-4a64-9cfd-3added887f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " tensor([[-0.2958],\n",
      "        [ 0.0502],\n",
      "        [-0.6245],\n",
      "        [-0.4806]], grad_fn=<AddmmBackward0>)\n",
      "target: \n",
      " tensor([[3.1000],\n",
      "        [2.3000],\n",
      "        [1.2000],\n",
      "        [0.4000]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# simple neural network model with 1 hidden layer\n",
    "torch.manual_seed(11052023)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(5,4),\n",
    "    nn.Linear(4,1))\n",
    "\n",
    "# compute the output\n",
    "reg_output = net(input)\n",
    "\n",
    "print(\"output: \\n\", reg_output)\n",
    "print(\"target: \\n\", reg_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb332529-ddb1-4774-ae9f-d7538e26ceba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1743, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "reg_loss = mse_loss(reg_output, reg_target)\n",
    "reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952c10c-7953-4e39-943e-4c0c1bc68e1c",
   "metadata": {},
   "source": [
    "### 5.1.3 Binary Classification Problem  as a Regression Problem\n",
    "\n",
    "* input has 4 observations and 5 features\n",
    "* target contains `0` or `1`\n",
    "* two hidden layers\n",
    "* use the sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00dcaadb-c4ec-4863-9e43-20796af15977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: \n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "target: \n",
      " tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# input data with 4  observations and 5 features\n",
    "input = torch.tensor([\n",
    "    [ 0.7550,  0.2580, -0.0376, -0.5695,  0.0454],\n",
    "    [-0.6897,  1.4822,  0.7860, -2.0889, -0.4481],\n",
    "    [ 0.4464,  0.7335, -0.9837,  1.7818, -0.9048],\n",
    "    [-0.7997,  1.1157, -0.4644,  1.1144, -0.6903]\n",
    "])\n",
    "bc_target = torch.tensor([0, 0, 1, 0]).view(-1,1)\n",
    "  \n",
    "\n",
    "# simple neural network model with 2 hidden layers\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(5,4),\n",
    "    nn.Linear(4,1))\n",
    "\n",
    "# compute the pre-activation output\n",
    "bc_pre_activation = net(input)\n",
    "\n",
    "# apply the softmax activation function\n",
    "#sigmoid = nn.Sigmoid()\n",
    "bc_output = sigmoid(bc_pre_activation)\n",
    "\n",
    "# predict the class per observation\n",
    "bc_prediction = torch.FloatTensor((bc_output > 0.5).float())\n",
    "\n",
    "print(\"prediction: \\n\", bc_prediction)\n",
    "print(\"target: \\n\", bc_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ddfee0c3-3790-48aa-8e23-04777615db22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7500)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_mse_loss = nn.MSELoss()\n",
    "bin_mse_loss = mse_loss(bc_prediction, bc_target)\n",
    "bin_mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68171d17-ac38-4966-8737-71ea65cbfe84",
   "metadata": {},
   "source": [
    "## 5.3 Forward Pass\n",
    "\n",
    "Generating a prediction  from the neural network models is is called **running a forward pass** through the network.\n",
    "\n",
    "![](images/neural_network_v3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c047e-4fc6-480d-92a1-0c9d19360c73",
   "metadata": {},
   "source": [
    "*  Input is `X` of shape  `(n, m)`.\n",
    "\n",
    "*  Number of classes is `num_classes = k`.\n",
    "  \n",
    "*  First neural network: `nn.Linear(m, p)`.\n",
    "    *  weights: $w_0 = $`0.weight.t()` of shape `(m, p)`\n",
    "      \n",
    "    *  bias: $b_0 = $0.bias of shape `(1, p)`\n",
    "      \n",
    "*  Second neural network: `nn.Linear(p, k)`.\n",
    "    *  weights: $w_1 = $`1.weight.t()` of shape `(p, k)`\n",
    "      \n",
    "    *  bias: $b_1 = $1.bias of shape `(1, k)`\n",
    "\n",
    "*  Softmax activation function of a variable $z$ computes the elements of its $i$th row of $z$.\n",
    "    At the $i$th row,\n",
    "   \n",
    "\\begin{align*}\n",
    "   S(x_i) &= S([x_{i0},x_{i1},\\ldots,x_{i(m-1)}])\\\\\n",
    "          &= \\left[{S(x_{i0}), S(x_{i1}), \\ldots, S(x_{i(m-1)})}\\right]\n",
    "\\end{align*}\n",
    "such that $ 0 \\leq S(x_{ij}) = \\dfrac{e^{x_{ij}}}{e^{x_{i0}}+e^{x_{i1}} + \\ldots +e^{x_{i(m-1)}}} \\leq 1$ \n",
    "\n",
    "for each $j = 0,1, \\ldots, m-1$ \n",
    "\n",
    "and $1 = \\displaystyle\\sum_{j=1}^{m-1} S(x_{ij})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df35b1a6-088d-46cf-9d29-ae8a15b190a4",
   "metadata": {},
   "source": [
    "**Forward pass**\n",
    "\n",
    "$$\\left(z_0 = X\\circ w_0 + b_0\\right)\\;  \\Longrightarrow \\left(z_1 = z_0 \\circ w_1 + b_1\\right) \\; \\Longrightarrow S(z_1) \\;\\Longrightarrow Loss(\\hat{y}, y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69576fd-f353-4d12-a19e-c0d7d42edcf1",
   "metadata": {},
   "source": [
    "**For the first neural network: (hidden layer)** \n",
    "$z_0 = \\begin{bmatrix} z_{0}^{(0)} \\\\\n",
    "                 z_{1}^{(0)}  \\\\\n",
    "                \\end{bmatrix}$ such that\n",
    " \n",
    " \\begin{align*} \n",
    " \\begin{bmatrix} z_{0}^{(0)} \\\\\n",
    "                 z_{1}^{(0)}  \\\\            \n",
    " \\end{bmatrix} &= \n",
    " \\begin{bmatrix} x_{0}^{(0)} & x_{1}^{(0)}  \\\\\n",
    "  \\end{bmatrix} \\circ \n",
    "  \\begin{bmatrix} w_{00}^{(0)} & w_{01}^{(0)}  \\\\\n",
    "                  w_{10}^{(0)} & w_{11}^{(0)}  \\\\\n",
    "  \\end{bmatrix} +\n",
    "  \\begin{bmatrix} b_{0}^{(0)} \\\\\n",
    "                  b_{1}^{(0)} \\\\\n",
    "  \\end{bmatrix} \\\\ \n",
    "  \\begin{bmatrix} z_{0}^{(0)} \\\\\n",
    "                  z_{1}^{(0)} \\\\      \n",
    "  \\end{bmatrix} &= \n",
    " \\begin{bmatrix} \n",
    "      x_{0}^{(0)} w_{00}^{(0)} + x_{1}^{(0)} w_{10}^{(0)} + b_{0}^{(0)} \\\\\n",
    "      x_{0}^{(0)} w_{01}^{(0)} + x_{1}^{(0)} w_{11}^{(0)} + b_{1}^{(0)} \\\\ \n",
    "   \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus,\n",
    "\n",
    "$z_{0}^{(0)} = x_{0}^{(0)} w_{00}^{(0)} + x_{1}^{(0)} w_{10}^{(0)} + b_{0}^{(0)}$ and\n",
    " \n",
    "$z_{1}^{(0)} = x_{0}^{(0)} w_{01}^{(0)} + x_{1}^{(0)} w_{11}^{(0)} + b_{1}^{(0)}$.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fedc2a-3ab0-4f83-aa90-9ae24d0f17eb",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "**For the second neural network: (pre-activation (output layer)** \n",
    " $z_1=\\begin{bmatrix} z_{0}^{(1)} \\\\\n",
    "                 z_{1}^{(1)}  \\\\            \n",
    " \\end{bmatrix}$ such that\n",
    " \n",
    "\\begin{align*} \n",
    " \\begin{bmatrix} z_{0}^{(1)} \\\\\n",
    "                 z_{1}^{(1)}  \\\\            \n",
    " \\end{bmatrix} &= \n",
    " \\begin{bmatrix} z_{0}^{(0)} & z_{1}^{(0)}  \\\\\n",
    "  \\end{bmatrix} \\circ \n",
    "  \\begin{bmatrix} w_{00}^{(1)} & w_{01}^{(1)}  \\\\\n",
    "                  w_{10}^{(1)} & w_{11}^{(1)}  \\\\\n",
    "  \\end{bmatrix} +\n",
    "  \\begin{bmatrix} b_{0}^{(1)} \\\\\n",
    "                  b_{1}^{(1)} \\\\\n",
    "  \\end{bmatrix} \\\\\n",
    "  \\begin{bmatrix} z_{0}^{(1)} \\\\\n",
    "                  z_{1}^{(1)} \\\\            \n",
    " \\end{bmatrix} &=\n",
    "  \\begin{bmatrix} \n",
    "      z_{0}^{(0)} w_{00}^{(1)} + z_{1}^{(0)} w_{10}^{(1)} + b_{0}^{(1)} \\\\\n",
    "      z_{0}^{(0)} w_{01}^{(1)} + z_{1}^{(0)} w_{11}^{(1)} + b_{1}^{(1)} \\\\ \n",
    "   \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus,\n",
    "\n",
    "$z_{0}^{(1)} = z_{0}^{(0)} w_{00}^{(1)} + z_{1}^{(0)} w_{10}^{(1)} + b_{0}^{(1)}$ and\n",
    "\n",
    "$z_{1}^{(1)} = z_{0}^{(0)} w_{01}^{(1)} + z_{1}^{(0)} w_{11}^{(1)} + b_{1}^{(1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123154f-5190-49a3-b5cb-f0ff1ef3fd81",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "**Prediction using the Softmax activation function**: \n",
    " $\\hat{y} = S(z_1)$ such that\n",
    " \n",
    "$\\hat{y}= \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}_0 \\\\\n",
    "\\hat{y}_1\n",
    "\\end{bmatrix}$ with  \n",
    "\n",
    "$\\hat{y_0} = \\dfrac{e^{z_0^{(1)}}}{e^{z_0^{(1)}}+e^{z_1^{(1)}}}$ and \n",
    "\n",
    "$\\hat{y_1} = \\dfrac{e^{z_1^{(1)}}}{e^{z_0^{(1)}}+e^{z_1^{(1)}}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c5e23-64f7-496b-901c-7c2485c04f2f",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "**For the loss function (cross entropy for multiclass classification)**:\n",
    "\n",
    "$\\text{Loss}(\\hat{y}, y)$ where \n",
    "\n",
    "$\\hat{y}$ is the prediction and \n",
    "\n",
    "$y$ is the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27253d24-21e3-4dc5-a40b-2520626a2cf9",
   "metadata": {},
   "source": [
    "### 5.4 Backward pass\n",
    "\n",
    "A **backward pass**, or **backpropagation**, is the process by which layer weights and biases are updated during training. \n",
    "\n",
    "All this is part of something called a **\"training loop\"**. \n",
    "\n",
    "---\n",
    "\n",
    "This involves \n",
    "\n",
    "**propagating data forward**, **comparing outputs (predictions) to true values**, \n",
    "\n",
    "then **propagating backwards to improve each layer's weights and biases** using some handy math.\n",
    "\n",
    "---\n",
    "\n",
    "We repeat several times until the model is tuned with meaningful weights and biases. \n",
    "\n",
    "So during training, the backward pass is the complementary step to the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f079c82-a755-4217-8300-80bd55118946",
   "metadata": {},
   "source": [
    "## 5.4.1 Loss functions to evaluate model predictions\n",
    "\n",
    "* In the created neural networks, the model is trained by the inputs and then predictions are returned as outputs. \n",
    "\n",
    "*  We'll now assess the differences between actual values and those predicted by the network using the loss function\n",
    "    * the loss function tells us how good our model is at making predictions during training. \n",
    "    * the inputs of the loss function are  a model prediction as `y_hat`, and true label, or ground truth as `y`\n",
    "    * the output is a floating number.\n",
    " \n",
    "* Our goal in the backward pass is to minimize the loss. This can be done by updating the values of the weights and biases. Weights and biases are the trainable parameters of the model.\n",
    "\n",
    "**Visualizing a loss function with a minimum value**\n",
    "\n",
    "![](images/minimizing_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f31e5d-2f11-4cc5-9571-fffb568a0e57",
   "metadata": {},
   "source": [
    "This means that we will be using loss functions that are differentiable with respect to the weights and biases.\n",
    "\n",
    "Note that the forward pass is $$\\left(z_1 = X\\circ w_0 + b_0\\right)\\;  \\Longrightarrow \\left(z_1 = z_0 \\circ w_1 + b_1\\right) \\; \\Longrightarrow \\left( \\hat{y} = S(z_1) \\right)\\; \\Longrightarrow \\;\\text{loss} = \\text{Loss}(\\hat{y}, y)$$\n",
    "\n",
    "To update the weights and biases:\n",
    "\n",
    "*  *Updating the weight* : `weight = weight - lr * weight_grad`\n",
    "*  *Updating thew bias* :   `bias = bias - lr * bias_grad`\n",
    "\n",
    "where `lr` is the learning rate and `weight_grad` and `bias_grad` are the gradients of the weights and biases, respectively.\n",
    "\n",
    "\n",
    "In our previous neural networks, we have 12 trainable parameters:\n",
    "8 weights and 4 biases.\n",
    "\n",
    "![](images/neural_network_v3.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Since the loss function is $\\text{Loss}\\left(\\hat{y}, y\\right)$ and\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y} &= S(z_1) \\\\\n",
    "\\hat{y_0} &= \\dfrac{e^{z_0^{1}}}{e^{z_0^{(1)}}+e^{z_1^{(1)}}}\\\\\n",
    "\\hat{y_1} &= \\dfrac{e^{z_1^{1}}}{e^{z_0^{(1)}}+e^{z_1^{(1)}}}\\\\\n",
    "z_{0}^{(1)} &= z_{0}^{(0)} w_{00}^{(1)} + z_{1}^{(0)} w_{10}^{(1)} + b_{0}^{(1)}\\\\\n",
    "z_{1}^{(1)} &= z_{0}^{(0)} w_{01}^{(1)} + z_{1}^{(0)} w_{11}^{(1)} + b_{1}^{(1)}\\\\\n",
    "z_{0}^{(0)} &= x_{0}^{(0)} w_{00}^{(0)} + x_{1}^{(0)} w_{10}^{(0)} + b_{0}^{(0)}\\\\\n",
    "z_{1}^{(0)} &= x_{0}^{(0)} w_{01}^{(0)} + x_{1}^{(0)} w_{11}^{(0)} + b_{1}^{(0)}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The gradients in the 2nd Linear network:\n",
    "\n",
    "**(1) Gradient with respect to $w_{00}^{(1)}$**:\n",
    "\n",
    "$\\dfrac{\\partial{\\text{Loss}}}{\\partial{w_{00}^{(1)}}} = \n",
    "\\left(\\dfrac{\\partial{\\text{loss}}}{\\partial{\\hat{y}_0}}\\right)\n",
    "\\left(\\dfrac{\\partial{\\hat{y}_0}}{\\partial{z_0^{(1)}}}\\right)\n",
    "\\left(\\dfrac{\\partial{z_0^{(1)}}}{\\partial{w_{00}^{(1)}}}\\right)= \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}}}\\right) \\cdot\n",
    " S(z_0^{(1)})\\cdot S(z_1^{(1)}) \\cdot z_{0}^{(0)}$\n",
    "\n",
    "**(2) Gradient with respect to $w_{01}^{(1)}$**:\n",
    "\n",
    "$\\dfrac{\\partial{\\text{Loss}}}{\\partial{w_{01}^{(1)}}} = \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}_1}}\\right)\n",
    "\\left(\\dfrac{\\partial{\\hat{y}_1}}{\\partial{z_1^{(1)}}}\\right)\n",
    "\\left(\\dfrac{\\partial{z_1^{(1)}}}{\\partial{w_{01}^{(1)}}}\\right)= \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}}}\\right) \\cdot\n",
    " S(z_0^{(1)})\\cdot S(z_1^{(1)}) \\cdot z_{0}^{(0)}$\n",
    "\n",
    "**(3) Gradient with respect to $w_{10}^{(1)}$**:\n",
    "\n",
    " $\\dfrac{\\partial{\\text{Loss}}}{\\partial{w_{10}^{(1)}}} = \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}_0}}\\right)\n",
    "\\left(\\dfrac{\\partial{\\hat{y}_0}}{\\partial{z_0^{(1)}}}\\right)\n",
    "\\left(\\dfrac{\\partial{z_0^{(1)}}}{\\partial{w_{10}^{(1)}}}\\right)= \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}}}\\right) \\cdot\n",
    " S(z_0^{(1)})\\cdot S(z_1^{(1)}) \\cdot z_{1}^{(0)}$\n",
    "\n",
    "\n",
    "**(4) Gradient with respect to $w_{11}^{(1)}$**:\n",
    "\n",
    " $\\dfrac{\\partial{\\text{Loss}}}{\\partial{w_{11}^{(1)}}} = \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}_1}}\\right)\n",
    "\\left(\\dfrac{\\partial{\\hat{y}_1}}{\\partial{z_1^{(1)}}}\\right)\n",
    "\\left(\\dfrac{\\partial{z_1^{(1)}}}{\\partial{w_{11}^{(1)}}}\\right)= \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}}}\\right) \\cdot\n",
    " S(z_0^{(1)})\\cdot S(z_1^{(1)}) \\cdot z_{1}^{(0)}$\n",
    "\n",
    "**(5) Gradient with respect to $b_{0}^{(1)}$**:\n",
    "\n",
    "$\\dfrac{\\partial{\\text{Loss}}}{\\partial{b_0^{(1)}}} = \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}_0}}\\right)\n",
    "\\left(\\dfrac{\\partial{\\hat{y}_0}}{\\partial{z_0^{(1)}}}\\right)\n",
    "\\left(\\dfrac{\\partial{z_0^{(1)}}}{\\partial{b_{0}^{(1)}}}\\right) =\n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}}}\\right) \\cdot\n",
    " S(z_0^{(1)})\\cdot S(z_1^{(1)})$\n",
    "\n",
    "**(6) Gradient with respect to $b_{1}^{(1)}$**:\n",
    "\n",
    "$\\dfrac{\\partial{\\text{Loss}}}{\\partial{b_1^{(1)}}} = \n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}_1}}\\right)\n",
    "\\left(\\dfrac{\\partial{\\hat{y}_1}}{\\partial{z_1^{(1)}}}\\right)\n",
    "\\left(\\dfrac{\\partial{z_1^{(1)}}}{\\partial{b_{1}^{(1)}}}\\right) =\n",
    "\\left(\\dfrac{\\partial{\\text{Loss}}}{\\partial{\\hat{y}}}\\right) \\cdot\n",
    " S(z_0^{(1)})\\cdot S(z_1^{(1)})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535df9b3-92d0-469a-9c3f-a070b6eb8d01",
   "metadata": {},
   "source": [
    "The gradient of a weight is $\\dfrac{\\partial{\\text{loss}}}{\\partial{\\text{weight}}}$\n",
    "which is `weight.grad` in PyTorch.\n",
    "\n",
    "Also, The gradient of a bias in PyTorch  is $\\dfrac{\\partial{\\text{loss}}}{\\partial{\\text{bias}}}$\n",
    "which is `bias.grad` in PyTorch.\n",
    "\n",
    "The gradients for all the tensors that require gradient in the computational graph can be computed by calling `loss.backward()`. \n",
    "\n",
    "`Loss.backward()` uses the chain rule to propagate the gradients from the output tensor (usually the loss) to the input tensors (usually the model parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0fc00-7e12-4bb5-b49c-5166c454b32d",
   "metadata": {},
   "source": [
    "To update the weights and biases:\n",
    "\n",
    "*  *Updating the weight* : `weight = weight - lr * weight_grad`\n",
    "*  *Updating thew bias* :   `bias = bias - lr * bias_grad`\n",
    "\n",
    "\n",
    " where\n",
    " * $\\text{weight\\_grad} = \\dfrac{\\partial\\,{\\text{loss}}}{\\partial{w}}$\n",
    " * $\\text{bias\\_grad} = \\dfrac{\\partial\\,{\\text{loss}}}{\\partial{b}}$\n",
    " * `lr` is the **learning rate**.\n",
    "\n",
    " In deep learning the learning  rate is a hyperparameter that determines the step size at which a neural network's model parameters are updated during training. \n",
    "It controls how much the model learns from each new data point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268d519-373e-4e37-ac40-2a95937ba881",
   "metadata": {},
   "source": [
    "### 5.4.3 Loss functions for classification and regression\n",
    "\n",
    "**Binary cross entropy loss function (BCE)** for binary classification problem. In PyTorch we use (`nn.BCELoss()`)\n",
    "\n",
    "*  it measures the difference between the predicted probabilities, `y_hat` and the true labels `y`. \n",
    "\n",
    "*  The formula for binary cross entropy loss is:\n",
    "    \n",
    "    $$L(y,p) = - [y \\cdot \\log{p} + (1-y)\\log(1-p)]$$\n",
    "    \n",
    "where \n",
    "\n",
    "$L(y, p)$ is the binary cross-entropy loss, \n",
    "\n",
    "$y$ is the true binary label ($0$ or $1$), and \n",
    "\n",
    "$p$ is the predicted probability that the instance belongs to class 1 \n",
    "with\n",
    "\n",
    "*   `p = y_hat` if `y_hat > 0.5` and \n",
    "*  `1-p = y_hat` if `y_hat <= 0.5`.    \n",
    "    \n",
    "Its derivative with respect to $p$ is\n",
    "\n",
    "$$\\frac{dL}{dp} = -\\dfrac{y}{p} + \\dfrac{1 - y}{1 - p}$$\n",
    "\n",
    "\n",
    "The **BCE** is usually combined with **sigmoid** activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbaac3a-f3c2-48fd-8e8f-d234b46516cc",
   "metadata": {},
   "source": [
    "In PyTorch, the **cross-entropy loss function** is commonly used for multi-class classification problems and is typically referred to as \"CrossEntropyLoss.\" (`nn.CrossEntropyLoss()`)\n",
    "\n",
    "This loss function combines the `softmax function` (to convert raw scores into class probabilities) and the negative log-likelihood loss.\n",
    "\n",
    "\n",
    "The formula forthe  **cross-entropy loss function**  in one observation is given by:\n",
    "\n",
    "$$L(y, p) = - \\sum_{i}^{n}  y_i  \\cdot \\log{p_i}$$\n",
    "\n",
    "where $L(y, p)$ is the cross-entropy loss function,\n",
    "\n",
    "$n$ is the number of classes\n",
    "\n",
    "$y$  is the true multiclass label, which is a vector of one-hot encoded labels, where only the true class is $1$, and all other entries are $0$.\n",
    "\n",
    "$p$  represents the predicted probability distribution over classes.\n",
    "\n",
    "The `nn.CrossEntropyLoss` function automatically applies the **softmax** function to the output tensor and computes the **cross-entropy loss** between the predicted class probabilities and the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5a9ba-5eda-41c9-89d8-4bc986f78fea",
   "metadata": {},
   "source": [
    "n PyTorch, for regression tasks where we want to predict continuous values rather than class labels, we typically use mean squared error (MSE) or mean absolute error (MAE) loss functions.\n",
    "\n",
    "**Mean Squared Error (MSE) Loss (L2 Loss)**: `nn.MSELoss()`.\n",
    "\n",
    "This loss function measures the average of the squared differences between the predicted values and the true target values.\n",
    "Mean Absolute Error (MAE) Loss (L1 Loss):  nn.L1Loss().\n",
    "\n",
    "This loss function measures the average of the absolute differences between the predicted values and the true target value\n",
    ". It is also suitable for regression tasks, particularly iweou want your model to be less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d2cf4318-1a3b-4251-944c-46c08ba40c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# torch.manual_seed(12345)\n",
    "input_tensor = torch.randn(1,9)\n",
    "target = torch.tensor([[1., 0.]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(9,2),\n",
    "    nn.Sigmoid() # Sigmoid activation function                 \n",
    ")\n",
    "\n",
    "prediction = model(input_tensor)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(prediction, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dae02946-299c-4197-98e1-4dcc9ae16be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original weights:\n",
      " tensor([[-0.1853,  0.2824, -0.1396, -0.0936,  0.2140, -0.1216,  0.0378, -0.1440,\n",
      "         -0.2662],\n",
      "        [-0.0854, -0.2092,  0.1421,  0.3006, -0.0994,  0.2449,  0.0683,  0.0028,\n",
      "          0.0767]])\n",
      "updated weights:\n",
      "  tensor([[-0.1842,  0.2830, -0.1408, -0.0938,  0.2137, -0.1167,  0.0390, -0.1414,\n",
      "         -0.2651],\n",
      "        [-0.0864, -0.2097,  0.1432,  0.3007, -0.0992,  0.2405,  0.0672,  0.0005,\n",
      "          0.0757]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "print(f\"original weights:\\n {model[0].weight.data}\")\n",
    "print(f\"updated weights:\\n  {model[0].weight - lr * model[0].weight.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cebe89-367b-4a1e-8afd-911a4dca802a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In PyTorch, for regression tasks where we want to predict continuous values rather than class labels, we typically use **mean squared error (MSE)** or **mean absolute error (MAE) loss functions**. \n",
    "\n",
    "*   **Mean Squared Error (MSE) Loss (L2 Loss)**:  `nn.MSELoss()`.\n",
    "\n",
    "    *   This loss function measures the average of the squared differences between the predicted values and the true target values. \n",
    "\n",
    "*   **Mean Absolute Error (MAE) Loss (L1 Loss)**: ` nn.L1Loss()`.\n",
    "    *    \n",
    "This loss function measures the average of the absolute differences between the predicted values and the true target value\n",
    "    *    . It is also suitable for regression tasks, particularly iweou want your model to be less sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d9cb8-a897-4416-971e-08ec6c8bc1b8",
   "metadata": {},
   "source": [
    "### 5.4.4 The need for an optimizer\n",
    "\n",
    "*  Some functions have one minimum and one only, called the \"global\" minimum. These functions are \"convex\". \n",
    "*  Some \"non-convex\" functions have more than one \"local\" minimum.\n",
    "\n",
    "![](images/convex_and_non_convex_functions.png)\n",
    "\n",
    "At a local minimum, the function value is lowest compared to nearby points, but points further away may be even lower. \n",
    "\n",
    "When minimizing loss functions, our goal is to find the global minimum of the non-convex function, here, when x is approximately one.\n",
    "\n",
    "Loss functions used in deep learning are not convex! \n",
    "\n",
    "To find global minima of non-convex functions, we use a mechanism called \"gradient descent\" which is called \"optimizers\" in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c9abacd5-977e-4ec0-941c-f1d2a714ceaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first prediction =  tensor([[0., 1.]])\n",
      "final prediction =  tensor([[1., 0.]])\n",
      "target =  tensor([[1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# torch.manual_seed(908) # for 01\n",
    "# torch.manual_seed(1) # for 11\n",
    "torch.manual_seed(2)  # for 10\n",
    "#torch.manual_seed(3)  # for 11\n",
    "#torch.manual_seed(1000)  # for 00\n",
    "\n",
    "input_tensor = torch.randn(1,9)\n",
    "target = torch.tensor([[1., 0.]])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(9,2),\n",
    "    nn.Sigmoid() # Sigmoid activation function                 \n",
    ")\n",
    "\n",
    "first_prediction = model(input_tensor)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(first_prediction, target)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1, momentum=1)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "#loss.backward(retain_graph=True)\n",
    "loss.backward()\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "final_prediction = model(input_tensor)\n",
    "\n",
    "print(\"first prediction = \", (first_prediction > 0.5).float())\n",
    "print(\"final prediction = \", (final_prediction > 0.5).float())\n",
    "print(\"target = \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f18a40-b764-4771-8809-82e8dc68ce58",
   "metadata": {},
   "source": [
    "## 5.5 Writing a training loop\n",
    "\n",
    "In scikit-learn, the whole training loop is contained in the `.fit()` method. \n",
    "\n",
    "In PyTorch, however, we have to implement the loop manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfbb2d8-55ce-4b27-9b66-c3e00371158a",
   "metadata": {},
   "source": [
    "### 5.5.1 Training a neural network\n",
    "\n",
    "1)  Create a model\n",
    "    \n",
    "2)  Choose a loss function\n",
    "\n",
    "3)  Create a dataset\n",
    "\n",
    "4)  Define an optimizer\n",
    "\n",
    "5)  Run a training loop, where for each sample of the dataset, we repeat:\n",
    "\n",
    "    *   Calculating loss (forward pass)\n",
    "     \n",
    "    *   Calculating local gradients\n",
    "     \n",
    "    *   Updating model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "42e37146-d3aa-4465-8f71-4495fea257d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a026c-9e49-420f-8990-de730a93e43f",
   "metadata": {},
   "source": [
    "### 5.5.2 Before the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dc8f130c-765f-4709-ae81-7d4ab0471d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "salaries = pd.read_csv(\"datasets/ds_salaries.csv\", index_col = \"Unnamed: 0\")\n",
    "target_df = salaries[\"salary_in_usd\"]\n",
    "features_df = salaries[[\"experience_level\", \"employment_type\", \"remote_ratio\", \"company_size\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a8ddd73e-ad04-479c-a71e-4d16da5fb37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['work_year', 'experience_level', 'employment_type', 'job_title',\n",
      "       'salary', 'salary_currency', 'salary_in_usd', 'employee_residence',\n",
      "       'remote_ratio', 'company_location', 'company_size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(salaries.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ac62d910-dff9-422a-b211-e144062fc68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>70000</td>\n",
       "      <td>EUR</td>\n",
       "      <td>79833</td>\n",
       "      <td>DE</td>\n",
       "      <td>0</td>\n",
       "      <td>DE</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Scientist</td>\n",
       "      <td>260000</td>\n",
       "      <td>USD</td>\n",
       "      <td>260000</td>\n",
       "      <td>JP</td>\n",
       "      <td>0</td>\n",
       "      <td>JP</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>85000</td>\n",
       "      <td>GBP</td>\n",
       "      <td>109024</td>\n",
       "      <td>GB</td>\n",
       "      <td>50</td>\n",
       "      <td>GB</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Product Data Analyst</td>\n",
       "      <td>20000</td>\n",
       "      <td>USD</td>\n",
       "      <td>20000</td>\n",
       "      <td>HN</td>\n",
       "      <td>0</td>\n",
       "      <td>HN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>150000</td>\n",
       "      <td>USD</td>\n",
       "      <td>150000</td>\n",
       "      <td>US</td>\n",
       "      <td>50</td>\n",
       "      <td>US</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_year experience_level employment_type                   job_title  \\\n",
       "0       2020               MI              FT              Data Scientist   \n",
       "1       2020               SE              FT  Machine Learning Scientist   \n",
       "2       2020               SE              FT           Big Data Engineer   \n",
       "3       2020               MI              FT        Product Data Analyst   \n",
       "4       2020               SE              FT   Machine Learning Engineer   \n",
       "\n",
       "   salary salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
       "0   70000             EUR          79833                 DE             0   \n",
       "1  260000             USD         260000                 JP             0   \n",
       "2   85000             GBP         109024                 GB            50   \n",
       "3   20000             USD          20000                 HN             0   \n",
       "4  150000             USD         150000                 US            50   \n",
       "\n",
       "  company_location company_size  \n",
       "0               DE            L  \n",
       "1               JP            S  \n",
       "2               GB            M  \n",
       "3               HN            S  \n",
       "4               US            L  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0637caf0-1183-47c8-a2c1-0fe288127096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 4)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming features' type to category \n",
    "features_df.loc[:, \"company_size\"] = features_df[\"company_size\"].astype(\"category\").cat.codes\n",
    "features_df.loc[:, \"experience_level\"] = features_df[\"experience_level\"].astype(\"category\").cat.codes\n",
    "features_df.loc[:, \"employment_type\"] = features_df[\"employment_type\"].astype(\"category\").cat.codes\n",
    "features = features_df.to_numpy(dtype='float32')\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fb551a62-e53e-4696-8468-e5a59b531eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experience_level employment_type  remote_ratio company_size\n",
       "0                2               2             0            0\n",
       "1                3               2             0            2\n",
       "2                3               2            50            1\n",
       "3                2               2             0            2\n",
       "4                3               2            50            0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "09e992cf-e0da-4a1f-a682-ee41f1c5d126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.,  0.,  0.],\n",
       "       [ 3.,  2.,  0.,  2.],\n",
       "       [ 3.,  2., 50.,  1.],\n",
       "       [ 2.,  2.,  0.,  2.],\n",
       "       [ 3.,  2., 50.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a4ddf685-145b-4475-bdb9-68269cd7d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c728551-791f-4e1e-8447-cc230a190394",
   "metadata": {},
   "source": [
    "`MinMaxScaler()` is a function in scikit-learn that transforms features by scaling each feature to a given range, such as between 0 and 1. \n",
    "\n",
    "It is often used as an alternative to zero mean, unit variance scaling, which is also known as standardization. \n",
    "\n",
    "`MinMaxScaler()` can be useful for data that has outliers or different scales, as it preserves the shape of the original distribution and does not reduce the importance of small variations.\n",
    "\n",
    "The main idea behind normalization/standardization is always the same. \n",
    "\n",
    "Variables that are measured at different scales do not contribute equally to the model fitting & model learned function and might end up creating a bias. \n",
    "\n",
    "Thus, to deal with this potential problem feature-wise normalization such as `MinMax Scaling` is usually used prior to model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "944abe30-fc1b-46e8-90b4-4887ea91fa03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       607.000000\n",
       "mean     112297.869852\n",
       "std       70957.259411\n",
       "min        2859.000000\n",
       "25%       62726.000000\n",
       "50%      101570.000000\n",
       "75%      150000.000000\n",
       "max      600000.000000\n",
       "Name: salary_in_usd, dtype: float64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bb753819-1507-481f-b6f4-4013734084d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 1)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "target = scaler.fit_transform(target_df.values.reshape(-1, 1))\n",
    "target = target.astype(np.float32)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b16a8b99-1a99-46e7-9562-044a0a127b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    607.000000\n",
       "mean       0.183271\n",
       "std        0.118828\n",
       "min        0.000000\n",
       "25%        0.100256\n",
       "50%        0.165306\n",
       "75%        0.246409\n",
       "max        1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(target.reshape(607,)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9868b-c135-4561-8431-61a4c96187b9",
   "metadata": {},
   "source": [
    "## 6. PyTorch's `dataset` and `dataloader`\n",
    "\n",
    "PyTorch `dataset` and `dataloader` are two data primitives that allow us to use pre-loaded datasets as well as our own data for training deep learning models. \n",
    "\n",
    "A `dataset` is an object that stores the **samples** and their corresponding **labels**, and a `dataloader` is an object that wraps an **iterable** around the dataset to enable easy access to the samples in **batches**. \n",
    "\n",
    "The batch size parameter in `dataloader` in pytorch is the number of samples that will be loaded into memory at each iteration. \n",
    "\n",
    "This is controlled by the `batch_size` argument in the dataloader. \n",
    "\n",
    "By default, this is set to `1`. \n",
    "\n",
    "There are a few reasons why we might want to change the batch size:\n",
    "\n",
    "*   If we are training on a GPU, we can increase the batch size to make better use of the GPU’s processing power.\r\n",
    "*  \n",
    "Ifwer’re training a deep neural network,weu may need to increase the batch size to avoid overfitting onyour training data.\n",
    "*  \r\n",
    "Iweou have a lot of data and wanweur training to run fasterweou can increase the batch size so that more data is processed per training iteratio\n",
    "\n",
    "In deep learning, **epoch** and **batch size** are two key parameters that control the training of a model.\n",
    "\n",
    "*Epoch** is the number of times the training dataset is passed through the neural network during training. \n",
    "One epoch is one complete pass through the training dataset.\n",
    "\n",
    "**Batch size** is the number of training examples used in one iteration. The size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset. The higher the batch size, the more memory space you’ll needn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8215f20a-3c2a-4762-92af-dbffb9f5811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features), torch.tensor(target))\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = nn.Sequential(nn.Linear(4, 2),                       \n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "# Create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2134c7-cb65-404a-9906-a4dfd2ec9708",
   "metadata": {},
   "source": [
    "### 6.1 The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2002cc3e-e1eb-418c-9bc0-995f1b46c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            feature, target = data\n",
    "            pred = model(feature)\n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item() * feature.size(0)\n",
    "            total_samples += feature.size(0)\n",
    "            \n",
    "            # Print ground truth and predicted salaries\n",
    "            for i in range(len(target)):\n",
    "                print(\"Ground truth salary: {:.3f}. Predicted salary: {:.3f}\".format(target[i].item(), pred[i].item()))\n",
    "    \n",
    "    average_loss = total_loss / total_samples\n",
    "    print(\"Average Loss: {:.4f}\".format(average_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "50bda77f-2495-4276-917c-61adce82da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth salary: 0.148. Predicted salary: 0.088\n",
      "Ground truth salary: 0.253. Predicted salary: 0.208\n",
      "Ground truth salary: 0.280. Predicted salary: 0.153\n",
      "Ground truth salary: 0.205. Predicted salary: 0.154\n",
      "Ground truth salary: 0.179. Predicted salary: 0.153\n",
      "Ground truth salary: 0.213. Predicted salary: 0.165\n",
      "Ground truth salary: 0.093. Predicted salary: 0.175\n",
      "Ground truth salary: 0.213. Predicted salary: 0.208\n",
      "Ground truth salary: 0.288. Predicted salary: 0.208\n",
      "Ground truth salary: 0.096. Predicted salary: 0.262\n",
      "Ground truth salary: 0.145. Predicted salary: 0.175\n",
      "Ground truth salary: 0.330. Predicted salary: 0.153\n",
      "Ground truth salary: 0.224. Predicted salary: 0.208\n",
      "Ground truth salary: 0.167. Predicted salary: 0.185\n",
      "Ground truth salary: 0.364. Predicted salary: 0.229\n",
      "Ground truth salary: 0.087. Predicted salary: 0.219\n",
      "Ground truth salary: 0.104. Predicted salary: 0.133\n",
      "Ground truth salary: 0.339. Predicted salary: 0.251\n",
      "Ground truth salary: 0.330. Predicted salary: 0.153\n",
      "Ground truth salary: 0.117. Predicted salary: 0.197\n",
      "Ground truth salary: 0.132. Predicted salary: 0.208\n",
      "Ground truth salary: 0.208. Predicted salary: 0.207\n",
      "Ground truth salary: 0.305. Predicted salary: 0.208\n",
      "Ground truth salary: 0.149. Predicted salary: 0.176\n",
      "Ground truth salary: 0.168. Predicted salary: 0.231\n",
      "Ground truth salary: 0.121. Predicted salary: 0.197\n",
      "Ground truth salary: 0.257. Predicted salary: 0.208\n",
      "Ground truth salary: 0.313. Predicted salary: 0.231\n",
      "Ground truth salary: 0.253. Predicted salary: 0.121\n",
      "Ground truth salary: 0.263. Predicted salary: 0.208\n",
      "Ground truth salary: 0.022. Predicted salary: 0.088\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.129. Predicted salary: 0.153\n",
      "Ground truth salary: 0.121. Predicted salary: 0.153\n",
      "Ground truth salary: 0.129. Predicted salary: 0.197\n",
      "Ground truth salary: 0.102. Predicted salary: 0.220\n",
      "Ground truth salary: 0.192. Predicted salary: 0.251\n",
      "Ground truth salary: 0.380. Predicted salary: 0.208\n",
      "Ground truth salary: 0.168. Predicted salary: 0.153\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.404. Predicted salary: 0.208\n",
      "Ground truth salary: 0.275. Predicted salary: 0.208\n",
      "Ground truth salary: 0.288. Predicted salary: 0.208\n",
      "Ground truth salary: 0.205. Predicted salary: 0.252\n",
      "Ground truth salary: 0.178. Predicted salary: 0.208\n",
      "Ground truth salary: 0.183. Predicted salary: 0.153\n",
      "Ground truth salary: 0.221. Predicted salary: 0.229\n",
      "Ground truth salary: 0.025. Predicted salary: 0.242\n",
      "Ground truth salary: 0.217. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.153\n",
      "Ground truth salary: 0.144. Predicted salary: 0.185\n",
      "Ground truth salary: 0.227. Predicted salary: 0.208\n",
      "Ground truth salary: 0.246. Predicted salary: 0.176\n",
      "Ground truth salary: 0.196. Predicted salary: 0.231\n",
      "Ground truth salary: 0.116. Predicted salary: 0.175\n",
      "Ground truth salary: 0.056. Predicted salary: 0.175\n",
      "Ground truth salary: 0.272. Predicted salary: 0.208\n",
      "Ground truth salary: 0.186. Predicted salary: 0.231\n",
      "Ground truth salary: 0.221. Predicted salary: 0.153\n",
      "Ground truth salary: 0.424. Predicted salary: 0.231\n",
      "Ground truth salary: 0.092. Predicted salary: 0.111\n",
      "Ground truth salary: 0.305. Predicted salary: 0.251\n",
      "Ground truth salary: 0.162. Predicted salary: 0.230\n",
      "Ground truth salary: 0.179. Predicted salary: 0.132\n",
      "Ground truth salary: 0.124. Predicted salary: 0.252\n",
      "Ground truth salary: 0.131. Predicted salary: 0.155\n",
      "Ground truth salary: 0.247. Predicted salary: 0.208\n",
      "Ground truth salary: 0.061. Predicted salary: 0.176\n",
      "Ground truth salary: 0.126. Predicted salary: 0.176\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.268. Predicted salary: 0.208\n",
      "Ground truth salary: 0.211. Predicted salary: 0.208\n",
      "Ground truth salary: 0.149. Predicted salary: 0.219\n",
      "Ground truth salary: 0.129. Predicted salary: 0.142\n",
      "Ground truth salary: 0.079. Predicted salary: 0.111\n",
      "Ground truth salary: 0.251. Predicted salary: 0.197\n",
      "Ground truth salary: 0.230. Predicted salary: 0.185\n",
      "Ground truth salary: 0.221. Predicted salary: 0.143\n",
      "Ground truth salary: 0.225. Predicted salary: 0.208\n",
      "Ground truth salary: 0.148. Predicted salary: 0.274\n",
      "Ground truth salary: 0.163. Predicted salary: 0.207\n",
      "Ground truth salary: 0.052. Predicted salary: 0.219\n",
      "Ground truth salary: 0.357. Predicted salary: 0.143\n",
      "Ground truth salary: 0.089. Predicted salary: 0.176\n",
      "Ground truth salary: 0.127. Predicted salary: 0.176\n",
      "Ground truth salary: 0.330. Predicted salary: 0.185\n",
      "Ground truth salary: 0.201. Predicted salary: 0.208\n",
      "Ground truth salary: 0.178. Predicted salary: 0.208\n",
      "Ground truth salary: 0.156. Predicted salary: 0.207\n",
      "Ground truth salary: 0.129. Predicted salary: 0.208\n",
      "Ground truth salary: 0.263. Predicted salary: 0.229\n",
      "Ground truth salary: 0.217. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.221. Predicted salary: 0.176\n",
      "Ground truth salary: 0.297. Predicted salary: 0.208\n",
      "Ground truth salary: 0.109. Predicted salary: 0.176\n",
      "Ground truth salary: 0.078. Predicted salary: 0.177\n",
      "Ground truth salary: 0.341. Predicted salary: 0.219\n",
      "Ground truth salary: 0.389. Predicted salary: 0.121\n",
      "Ground truth salary: 0.083. Predicted salary: 0.111\n",
      "Ground truth salary: 0.068. Predicted salary: 0.219\n",
      "Ground truth salary: 0.078. Predicted salary: 0.176\n",
      "Ground truth salary: 0.292. Predicted salary: 0.185\n",
      "Ground truth salary: 0.173. Predicted salary: 0.153\n",
      "Ground truth salary: 0.037. Predicted salary: 0.133\n",
      "Ground truth salary: 0.223. Predicted salary: 0.251\n",
      "Ground truth salary: 0.160. Predicted salary: 0.219\n",
      "Ground truth salary: 0.246. Predicted salary: 0.207\n",
      "Ground truth salary: 0.213. Predicted salary: 0.208\n",
      "Ground truth salary: 0.138. Predicted salary: 0.187\n",
      "Ground truth salary: 0.066. Predicted salary: 0.110\n",
      "Ground truth salary: 0.144. Predicted salary: 0.175\n",
      "Ground truth salary: 0.184. Predicted salary: 0.208\n",
      "Ground truth salary: 0.287. Predicted salary: 0.185\n",
      "Ground truth salary: 0.232. Predicted salary: 0.219\n",
      "Ground truth salary: 0.045. Predicted salary: 0.110\n",
      "Ground truth salary: 0.072. Predicted salary: 0.155\n",
      "Ground truth salary: 0.086. Predicted salary: 0.251\n",
      "Ground truth salary: 0.704. Predicted salary: 0.175\n",
      "Ground truth salary: 0.057. Predicted salary: 0.110\n",
      "Ground truth salary: 0.184. Predicted salary: 0.208\n",
      "Ground truth salary: 0.255. Predicted salary: 0.208\n",
      "Ground truth salary: 0.370. Predicted salary: 0.143\n",
      "Ground truth salary: 0.081. Predicted salary: 0.175\n",
      "Ground truth salary: 0.217. Predicted salary: 0.251\n",
      "Ground truth salary: 0.297. Predicted salary: 0.251\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.176\n",
      "Ground truth salary: 0.238. Predicted salary: 0.208\n",
      "Ground truth salary: 0.233. Predicted salary: 0.164\n",
      "Ground truth salary: 0.184. Predicted salary: 0.175\n",
      "Ground truth salary: 0.226. Predicted salary: 0.134\n",
      "Ground truth salary: 0.322. Predicted salary: 0.208\n",
      "Ground truth salary: 0.272. Predicted salary: 0.208\n",
      "Ground truth salary: 0.081. Predicted salary: 0.177\n",
      "Ground truth salary: 0.032. Predicted salary: 0.124\n",
      "Ground truth salary: 0.124. Predicted salary: 0.134\n",
      "Ground truth salary: 0.146. Predicted salary: 0.176\n",
      "Ground truth salary: 0.246. Predicted salary: 0.153\n",
      "Ground truth salary: 0.151. Predicted salary: 0.153\n",
      "Ground truth salary: 0.197. Predicted salary: 0.208\n",
      "Ground truth salary: 0.152. Predicted salary: 0.208\n",
      "Ground truth salary: 0.079. Predicted salary: 0.219\n",
      "Ground truth salary: 0.179. Predicted salary: 0.153\n",
      "Ground truth salary: 0.399. Predicted salary: 0.176\n",
      "Ground truth salary: 0.095. Predicted salary: 0.198\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.104. Predicted salary: 0.134\n",
      "Ground truth salary: 0.129. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.098. Predicted salary: 0.197\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.125. Predicted salary: 0.230\n",
      "Ground truth salary: 0.012. Predicted salary: 0.134\n",
      "Ground truth salary: 0.749. Predicted salary: 0.219\n",
      "Ground truth salary: 0.196. Predicted salary: 0.088\n",
      "Ground truth salary: 0.288. Predicted salary: 0.143\n",
      "Ground truth salary: 0.246. Predicted salary: 0.176\n",
      "Ground truth salary: 0.092. Predicted salary: 0.110\n",
      "Ground truth salary: 0.272. Predicted salary: 0.251\n",
      "Ground truth salary: 0.227. Predicted salary: 0.176\n",
      "Ground truth salary: 0.173. Predicted salary: 0.208\n",
      "Ground truth salary: 0.072. Predicted salary: 0.219\n",
      "Ground truth salary: 0.180. Predicted salary: 0.208\n",
      "Ground truth salary: 0.002. Predicted salary: 0.176\n",
      "Ground truth salary: 0.179. Predicted salary: 0.153\n",
      "Ground truth salary: 0.313. Predicted salary: 0.251\n",
      "Ground truth salary: 0.251. Predicted salary: 0.185\n",
      "Ground truth salary: 0.221. Predicted salary: 0.208\n",
      "Ground truth salary: 0.026. Predicted salary: 0.176\n",
      "Ground truth salary: 0.330. Predicted salary: 0.185\n",
      "Ground truth salary: 0.330. Predicted salary: 0.208\n",
      "Ground truth salary: 0.246. Predicted salary: 0.208\n",
      "Ground truth salary: 0.055. Predicted salary: 0.175\n",
      "Ground truth salary: 0.238. Predicted salary: 0.208\n",
      "Ground truth salary: 0.083. Predicted salary: 0.176\n",
      "Ground truth salary: 0.214. Predicted salary: 0.176\n",
      "Ground truth salary: 0.015. Predicted salary: 0.228\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.162. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.111\n",
      "Ground truth salary: 0.171. Predicted salary: 0.153\n",
      "Ground truth salary: 0.142. Predicted salary: 0.176\n",
      "Ground truth salary: 0.261. Predicted salary: 0.176\n",
      "Ground truth salary: 0.276. Predicted salary: 0.143\n",
      "Ground truth salary: 0.082. Predicted salary: 0.154\n",
      "Ground truth salary: 0.251. Predicted salary: 0.208\n",
      "Ground truth salary: 0.213. Predicted salary: 0.219\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.364. Predicted salary: 0.208\n",
      "Ground truth salary: 0.029. Predicted salary: 0.134\n",
      "Ground truth salary: 0.240. Predicted salary: 0.208\n",
      "Ground truth salary: 0.263. Predicted salary: 0.153\n",
      "Ground truth salary: 0.189. Predicted salary: 0.251\n",
      "Ground truth salary: 0.035. Predicted salary: 0.208\n",
      "Ground truth salary: 0.005. Predicted salary: 0.153\n",
      "Ground truth salary: 0.345. Predicted salary: 0.185\n",
      "Ground truth salary: 0.092. Predicted salary: 0.242\n",
      "Ground truth salary: 0.213. Predicted salary: 0.176\n",
      "Ground truth salary: 0.129. Predicted salary: 0.088\n",
      "Ground truth salary: 0.207. Predicted salary: 0.219\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.216. Predicted salary: 0.251\n",
      "Ground truth salary: 0.255. Predicted salary: 0.208\n",
      "Ground truth salary: 0.247. Predicted salary: 0.208\n",
      "Ground truth salary: 0.026. Predicted salary: 0.111\n",
      "Ground truth salary: 0.272. Predicted salary: 0.208\n",
      "Ground truth salary: 0.002. Predicted salary: 0.154\n",
      "Ground truth salary: 0.124. Predicted salary: 0.176\n",
      "Ground truth salary: 0.050. Predicted salary: 0.176\n",
      "Ground truth salary: 0.137. Predicted salary: 0.208\n",
      "Ground truth salary: 0.027. Predicted salary: 0.251\n",
      "Ground truth salary: 0.104. Predicted salary: 0.231\n",
      "Ground truth salary: 0.144. Predicted salary: 0.176\n",
      "Ground truth salary: 0.105. Predicted salary: 0.251\n",
      "Ground truth salary: 0.188. Predicted salary: 0.175\n",
      "Ground truth salary: 0.330. Predicted salary: 0.153\n",
      "Ground truth salary: 0.087. Predicted salary: 0.088\n",
      "Ground truth salary: 0.243. Predicted salary: 0.208\n",
      "Ground truth salary: 0.243. Predicted salary: 0.208\n",
      "Ground truth salary: 0.166. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.134\n",
      "Ground truth salary: 0.151. Predicted salary: 0.219\n",
      "Ground truth salary: 0.081. Predicted salary: 0.175\n",
      "Ground truth salary: 0.213. Predicted salary: 0.208\n",
      "Ground truth salary: 0.074. Predicted salary: 0.198\n",
      "Ground truth salary: 0.414. Predicted salary: 0.110\n",
      "Ground truth salary: 0.037. Predicted salary: 0.208\n",
      "Ground truth salary: 0.078. Predicted salary: 0.133\n",
      "Ground truth salary: 0.015. Predicted salary: 0.080\n",
      "Ground truth salary: 0.114. Predicted salary: 0.175\n",
      "Ground truth salary: 0.094. Predicted salary: 0.133\n",
      "Ground truth salary: 0.221. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.050. Predicted salary: 0.198\n",
      "Ground truth salary: 0.147. Predicted salary: 0.208\n",
      "Ground truth salary: 0.078. Predicted salary: 0.176\n",
      "Ground truth salary: 0.196. Predicted salary: 0.252\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.177\n",
      "Ground truth salary: 0.171. Predicted salary: 0.176\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.142. Predicted salary: 0.231\n",
      "Ground truth salary: 0.074. Predicted salary: 0.176\n",
      "Ground truth salary: 0.330. Predicted salary: 0.185\n",
      "Ground truth salary: 0.224. Predicted salary: 0.208\n",
      "Ground truth salary: 0.142. Predicted salary: 0.153\n",
      "Ground truth salary: 0.115. Predicted salary: 0.231\n",
      "Ground truth salary: 0.113. Predicted salary: 0.175\n",
      "Ground truth salary: 0.043. Predicted salary: 0.175\n",
      "Ground truth salary: 0.033. Predicted salary: 0.197\n",
      "Ground truth salary: 0.184. Predicted salary: 0.229\n",
      "Ground truth salary: 0.171. Predicted salary: 0.208\n",
      "Ground truth salary: 0.287. Predicted salary: 0.185\n",
      "Ground truth salary: 0.129. Predicted salary: 0.111\n",
      "Ground truth salary: 0.043. Predicted salary: 0.153\n",
      "Ground truth salary: 0.314. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.110\n",
      "Ground truth salary: 0.018. Predicted salary: 0.088\n",
      "Ground truth salary: 0.083. Predicted salary: 0.219\n",
      "Ground truth salary: 0.063. Predicted salary: 0.207\n",
      "Ground truth salary: 0.163. Predicted salary: 0.132\n",
      "Ground truth salary: 0.236. Predicted salary: 0.185\n",
      "Ground truth salary: 1.000. Predicted salary: 0.121\n",
      "Ground truth salary: 0.058. Predicted salary: 0.175\n",
      "Ground truth salary: 0.138. Predicted salary: 0.153\n",
      "Ground truth salary: 0.206. Predicted salary: 0.208\n",
      "Ground truth salary: 0.347. Predicted salary: 0.208\n",
      "Ground truth salary: 0.290. Predicted salary: 0.208\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.155\n",
      "Ground truth salary: 0.155. Predicted salary: 0.251\n",
      "Ground truth salary: 0.099. Predicted salary: 0.207\n",
      "Ground truth salary: 0.447. Predicted salary: 0.215\n",
      "Ground truth salary: 0.038. Predicted salary: 0.198\n",
      "Ground truth salary: 0.213. Predicted salary: 0.143\n",
      "Ground truth salary: 0.213. Predicted salary: 0.175\n",
      "Ground truth salary: 0.305. Predicted salary: 0.185\n",
      "Ground truth salary: 0.048. Predicted salary: 0.153\n",
      "Ground truth salary: 0.263. Predicted salary: 0.208\n",
      "Ground truth salary: 0.074. Predicted salary: 0.175\n",
      "Ground truth salary: 0.104. Predicted salary: 0.197\n",
      "Ground truth salary: 0.132. Predicted salary: 0.251\n",
      "Ground truth salary: 0.000. Predicted salary: 0.242\n",
      "Ground truth salary: 0.071. Predicted salary: 0.153\n",
      "Ground truth salary: 0.058. Predicted salary: 0.088\n",
      "Ground truth salary: 0.102. Predicted salary: 0.110\n",
      "Ground truth salary: 0.029. Predicted salary: 0.242\n",
      "Ground truth salary: 0.364. Predicted salary: 0.208\n",
      "Ground truth salary: 0.171. Predicted salary: 0.270\n",
      "Ground truth salary: 0.254. Predicted salary: 0.185\n",
      "Ground truth salary: 0.121. Predicted salary: 0.219\n",
      "Ground truth salary: 0.138. Predicted salary: 0.219\n",
      "Ground truth salary: 0.270. Predicted salary: 0.251\n",
      "Ground truth salary: 0.012. Predicted salary: 0.111\n",
      "Ground truth salary: 0.039. Predicted salary: 0.197\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.110. Predicted salary: 0.185\n",
      "Ground truth salary: 0.086. Predicted salary: 0.175\n",
      "Ground truth salary: 0.087. Predicted salary: 0.144\n",
      "Ground truth salary: 0.349. Predicted salary: 0.208\n",
      "Ground truth salary: 0.106. Predicted salary: 0.220\n",
      "Ground truth salary: 0.673. Predicted salary: 0.185\n",
      "Ground truth salary: 0.043. Predicted salary: 0.111\n",
      "Ground truth salary: 0.102. Predicted salary: 0.176\n",
      "Ground truth salary: 0.125. Predicted salary: 0.220\n",
      "Ground truth salary: 0.191. Predicted salary: 0.175\n",
      "Ground truth salary: 0.079. Predicted salary: 0.208\n",
      "Ground truth salary: 0.310. Predicted salary: 0.185\n",
      "Ground truth salary: 0.005. Predicted salary: 0.132\n",
      "Ground truth salary: 0.171. Predicted salary: 0.134\n",
      "Ground truth salary: 0.238. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.111\n",
      "Ground truth salary: 0.225. Predicted salary: 0.208\n",
      "Ground truth salary: 0.100. Predicted salary: 0.185\n",
      "Ground truth salary: 0.345. Predicted salary: 0.208\n",
      "Ground truth salary: 0.230. Predicted salary: 0.251\n",
      "Ground truth salary: 0.154. Predicted salary: 0.207\n",
      "Ground truth salary: 0.099. Predicted salary: 0.197\n",
      "Ground truth salary: 0.102. Predicted salary: 0.219\n",
      "Ground truth salary: 0.339. Predicted salary: 0.251\n",
      "Ground truth salary: 0.141. Predicted salary: 0.153\n",
      "Ground truth salary: 0.072. Predicted salary: 0.219\n",
      "Ground truth salary: 0.196. Predicted salary: 0.208\n",
      "Ground truth salary: 0.133. Predicted salary: 0.153\n",
      "Ground truth salary: 0.072. Predicted salary: 0.231\n",
      "Ground truth salary: 0.250. Predicted salary: 0.185\n",
      "Ground truth salary: 0.236. Predicted salary: 0.207\n",
      "Ground truth salary: 0.100. Predicted salary: 0.155\n",
      "Ground truth salary: 0.318. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.208\n",
      "Ground truth salary: 0.345. Predicted salary: 0.185\n",
      "Ground truth salary: 0.152. Predicted salary: 0.207\n",
      "Ground truth salary: 0.029. Predicted salary: 0.207\n",
      "Ground truth salary: 0.112. Predicted salary: 0.175\n",
      "Ground truth salary: 0.094. Predicted salary: 0.088\n",
      "Ground truth salary: 0.124. Predicted salary: 0.198\n",
      "Ground truth salary: 0.029. Predicted salary: 0.198\n",
      "Ground truth salary: 0.133. Predicted salary: 0.198\n",
      "Ground truth salary: 0.124. Predicted salary: 0.132\n",
      "Ground truth salary: 0.090. Predicted salary: 0.220\n",
      "Ground truth salary: 0.339. Predicted salary: 0.229\n",
      "Ground truth salary: 0.015. Predicted salary: 0.219\n",
      "Ground truth salary: 0.330. Predicted salary: 0.176\n",
      "Ground truth salary: 0.096. Predicted salary: 0.197\n",
      "Ground truth salary: 0.372. Predicted salary: 0.185\n",
      "Ground truth salary: 0.094. Predicted salary: 0.176\n",
      "Ground truth salary: 0.173. Predicted salary: 0.219\n",
      "Ground truth salary: 0.011. Predicted salary: 0.134\n",
      "Ground truth salary: 0.098. Predicted salary: 0.208\n",
      "Ground truth salary: 0.352. Predicted salary: 0.208\n",
      "Ground truth salary: 0.075. Predicted salary: 0.207\n",
      "Ground truth salary: 0.286. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.134\n",
      "Ground truth salary: 0.115. Predicted salary: 0.208\n",
      "Ground truth salary: 0.060. Predicted salary: 0.176\n",
      "Ground truth salary: 0.037. Predicted salary: 0.175\n",
      "Ground truth salary: 0.086. Predicted salary: 0.207\n",
      "Ground truth salary: 0.124. Predicted salary: 0.176\n",
      "Ground truth salary: 0.163. Predicted salary: 0.150\n",
      "Ground truth salary: 0.063. Predicted salary: 0.176\n",
      "Ground truth salary: 0.354. Predicted salary: 0.208\n",
      "Ground truth salary: 0.213. Predicted salary: 0.208\n",
      "Ground truth salary: 0.105. Predicted salary: 0.219\n",
      "Ground truth salary: 0.205. Predicted salary: 0.134\n",
      "Ground truth salary: 0.246. Predicted salary: 0.251\n",
      "Ground truth salary: 0.166. Predicted salary: 0.219\n",
      "Ground truth salary: 0.200. Predicted salary: 0.197\n",
      "Ground truth salary: 0.025. Predicted salary: 0.134\n",
      "Ground truth salary: 0.079. Predicted salary: 0.153\n",
      "Ground truth salary: 0.441. Predicted salary: 0.208\n",
      "Ground truth salary: 0.380. Predicted salary: 0.142\n",
      "Ground truth salary: 0.431. Predicted salary: 0.208\n",
      "Ground truth salary: 0.083. Predicted salary: 0.088\n",
      "Ground truth salary: 0.196. Predicted salary: 0.198\n",
      "Ground truth salary: 0.160. Predicted salary: 0.176\n",
      "Ground truth salary: 0.184. Predicted salary: 0.208\n",
      "Ground truth salary: 0.161. Predicted salary: 0.251\n",
      "Ground truth salary: 0.142. Predicted salary: 0.176\n",
      "Ground truth salary: 0.055. Predicted salary: 0.153\n",
      "Ground truth salary: 0.297. Predicted salary: 0.208\n",
      "Ground truth salary: 0.062. Predicted salary: 0.154\n",
      "Ground truth salary: 0.183. Predicted salary: 0.153\n",
      "Ground truth salary: 0.051. Predicted salary: 0.274\n",
      "Ground truth salary: 0.260. Predicted salary: 0.185\n",
      "Ground truth salary: 0.112. Predicted salary: 0.111\n",
      "Ground truth salary: 0.163. Predicted salary: 0.208\n",
      "Ground truth salary: 0.161. Predicted salary: 0.251\n",
      "Ground truth salary: 0.241. Predicted salary: 0.175\n",
      "Ground truth salary: 0.147. Predicted salary: 0.175\n",
      "Ground truth salary: 0.221. Predicted salary: 0.208\n",
      "Ground truth salary: 0.147. Predicted salary: 0.175\n",
      "Ground truth salary: 0.092. Predicted salary: 0.242\n",
      "Ground truth salary: 0.230. Predicted salary: 0.229\n",
      "Ground truth salary: 0.087. Predicted salary: 0.155\n",
      "Ground truth salary: 0.201. Predicted salary: 0.208\n",
      "Ground truth salary: 0.046. Predicted salary: 0.088\n",
      "Ground truth salary: 0.142. Predicted salary: 0.176\n",
      "Ground truth salary: 0.163. Predicted salary: 0.176\n",
      "Ground truth salary: 0.178. Predicted salary: 0.230\n",
      "Ground truth salary: 0.280. Predicted salary: 0.176\n",
      "Ground truth salary: 0.111. Predicted salary: 0.208\n",
      "Ground truth salary: 0.129. Predicted salary: 0.231\n",
      "Ground truth salary: 0.005. Predicted salary: 0.133\n",
      "Ground truth salary: 0.277. Predicted salary: 0.274\n",
      "Ground truth salary: 0.087. Predicted salary: 0.219\n",
      "Ground truth salary: 0.246. Predicted salary: 0.134\n",
      "Ground truth salary: 0.272. Predicted salary: 0.251\n",
      "Ground truth salary: 0.317. Predicted salary: 0.208\n",
      "Ground truth salary: 0.414. Predicted salary: 0.164\n",
      "Ground truth salary: 0.028. Predicted salary: 0.153\n",
      "Ground truth salary: 0.128. Predicted salary: 0.185\n",
      "Ground truth salary: 0.100. Predicted salary: 0.220\n",
      "Ground truth salary: 0.031. Predicted salary: 0.133\n",
      "Ground truth salary: 0.061. Predicted salary: 0.219\n",
      "Ground truth salary: 0.263. Predicted salary: 0.153\n",
      "Ground truth salary: 0.280. Predicted salary: 0.208\n",
      "Ground truth salary: 0.171. Predicted salary: 0.208\n",
      "Ground truth salary: 0.154. Predicted salary: 0.207\n",
      "Ground truth salary: 0.156. Predicted salary: 0.198\n",
      "Ground truth salary: 0.112. Predicted salary: 0.088\n",
      "Ground truth salary: 0.309. Predicted salary: 0.153\n",
      "Ground truth salary: 0.305. Predicted salary: 0.207\n",
      "Ground truth salary: 0.140. Predicted salary: 0.133\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.196. Predicted salary: 0.207\n",
      "Ground truth salary: 0.005. Predicted salary: 0.177\n",
      "Ground truth salary: 0.303. Predicted salary: 0.185\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.263. Predicted salary: 0.231\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.380. Predicted salary: 0.208\n",
      "Ground truth salary: 0.113. Predicted salary: 0.251\n",
      "Ground truth salary: 0.072. Predicted salary: 0.111\n",
      "Ground truth salary: 0.076. Predicted salary: 0.198\n",
      "Ground truth salary: 0.185. Predicted salary: 0.208\n",
      "Ground truth salary: 0.258. Predicted salary: 0.153\n",
      "Ground truth salary: 0.263. Predicted salary: 0.252\n",
      "Ground truth salary: 0.079. Predicted salary: 0.231\n",
      "Ground truth salary: 0.431. Predicted salary: 0.274\n",
      "Ground truth salary: 0.196. Predicted salary: 0.229\n",
      "Ground truth salary: 0.127. Predicted salary: 0.251\n",
      "Ground truth salary: 0.275. Predicted salary: 0.176\n",
      "Ground truth salary: 0.211. Predicted salary: 0.251\n",
      "Ground truth salary: 0.692. Predicted salary: 0.228\n",
      "Ground truth salary: 0.032. Predicted salary: 0.133\n",
      "Ground truth salary: 0.152. Predicted salary: 0.208\n",
      "Ground truth salary: 0.121. Predicted salary: 0.198\n",
      "Ground truth salary: 0.128. Predicted salary: 0.142\n",
      "Ground truth salary: 0.177. Predicted salary: 0.251\n",
      "Ground truth salary: 0.246. Predicted salary: 0.208\n",
      "Ground truth salary: 0.372. Predicted salary: 0.088\n",
      "Ground truth salary: 0.300. Predicted salary: 0.251\n",
      "Ground truth salary: 0.196. Predicted salary: 0.219\n",
      "Ground truth salary: 0.246. Predicted salary: 0.121\n",
      "Ground truth salary: 0.106. Predicted salary: 0.176\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.127. Predicted salary: 0.208\n",
      "Ground truth salary: 0.201. Predicted salary: 0.208\n",
      "Ground truth salary: 0.280. Predicted salary: 0.185\n",
      "Ground truth salary: 0.066. Predicted salary: 0.220\n",
      "Ground truth salary: 0.685. Predicted salary: 0.185\n",
      "Ground truth salary: 0.138. Predicted salary: 0.134\n",
      "Ground truth salary: 0.112. Predicted salary: 0.166\n",
      "Ground truth salary: 0.079. Predicted salary: 0.176\n",
      "Ground truth salary: 0.171. Predicted salary: 0.251\n",
      "Ground truth salary: 0.119. Predicted salary: 0.220\n",
      "Ground truth salary: 0.096. Predicted salary: 0.134\n",
      "Ground truth salary: 0.116. Predicted salary: 0.219\n",
      "Ground truth salary: 0.129. Predicted salary: 0.251\n",
      "Ground truth salary: 0.457. Predicted salary: 0.229\n",
      "Ground truth salary: 0.182. Predicted salary: 0.219\n",
      "Ground truth salary: 0.127. Predicted salary: 0.176\n",
      "Ground truth salary: 0.096. Predicted salary: 0.208\n",
      "Ground truth salary: 0.112. Predicted salary: 0.122\n",
      "Ground truth salary: 0.106. Predicted salary: 0.229\n",
      "Ground truth salary: 0.134. Predicted salary: 0.219\n",
      "Ground truth salary: 0.397. Predicted salary: 0.229\n",
      "Ground truth salary: 0.192. Predicted salary: 0.219\n",
      "Ground truth salary: 0.042. Predicted salary: 0.176\n",
      "Ground truth salary: 0.178. Predicted salary: 0.175\n",
      "Ground truth salary: 0.005. Predicted salary: 0.198\n",
      "Ground truth salary: 0.221. Predicted salary: 0.153\n",
      "Ground truth salary: 0.246. Predicted salary: 0.185\n",
      "Ground truth salary: 0.538. Predicted salary: 0.143\n",
      "Ground truth salary: 0.189. Predicted salary: 0.251\n",
      "Ground truth salary: 0.098. Predicted salary: 0.208\n",
      "Ground truth salary: 0.181. Predicted salary: 0.208\n",
      "Ground truth salary: 0.188. Predicted salary: 0.231\n",
      "Ground truth salary: 0.272. Predicted salary: 0.208\n",
      "Ground truth salary: 0.163. Predicted salary: 0.153\n",
      "Ground truth salary: 0.255. Predicted salary: 0.208\n",
      "Ground truth salary: 0.265. Predicted salary: 0.208\n",
      "Ground truth salary: 0.111. Predicted salary: 0.176\n",
      "Ground truth salary: 0.013. Predicted salary: 0.102\n",
      "Ground truth salary: 0.356. Predicted salary: 0.229\n",
      "Ground truth salary: 0.004. Predicted salary: 0.166\n",
      "Ground truth salary: 0.254. Predicted salary: 0.185\n",
      "Ground truth salary: 0.097. Predicted salary: 0.207\n",
      "Ground truth salary: 0.193. Predicted salary: 0.166\n",
      "Ground truth salary: 0.297. Predicted salary: 0.229\n",
      "Ground truth salary: 0.203. Predicted salary: 0.219\n",
      "Ground truth salary: 0.364. Predicted salary: 0.208\n",
      "Ground truth salary: 0.069. Predicted salary: 0.176\n",
      "Ground truth salary: 0.179. Predicted salary: 0.143\n",
      "Ground truth salary: 0.146. Predicted salary: 0.134\n",
      "Ground truth salary: 0.164. Predicted salary: 0.185\n",
      "Ground truth salary: 0.129. Predicted salary: 0.111\n",
      "Ground truth salary: 0.188. Predicted salary: 0.208\n",
      "Ground truth salary: 0.172. Predicted salary: 0.208\n",
      "Ground truth salary: 0.127. Predicted salary: 0.219\n",
      "Ground truth salary: 0.142. Predicted salary: 0.251\n",
      "Ground truth salary: 0.196. Predicted salary: 0.229\n",
      "Ground truth salary: 0.191. Predicted salary: 0.207\n",
      "Ground truth salary: 0.246. Predicted salary: 0.208\n",
      "Ground truth salary: 0.146. Predicted salary: 0.219\n",
      "Ground truth salary: 0.065. Predicted salary: 0.134\n",
      "Ground truth salary: 0.062. Predicted salary: 0.088\n",
      "Ground truth salary: 0.069. Predicted salary: 0.176\n",
      "Ground truth salary: 0.063. Predicted salary: 0.197\n",
      "Ground truth salary: 0.230. Predicted salary: 0.229\n",
      "Ground truth salary: 0.193. Predicted salary: 0.176\n",
      "Ground truth salary: 0.029. Predicted salary: 0.228\n",
      "Ground truth salary: 0.184. Predicted salary: 0.208\n",
      "Ground truth salary: 0.221. Predicted salary: 0.153\n",
      "Ground truth salary: 0.084. Predicted salary: 0.207\n",
      "Ground truth salary: 0.100. Predicted salary: 0.197\n",
      "Ground truth salary: 0.161. Predicted salary: 0.219\n",
      "Ground truth salary: 0.189. Predicted salary: 0.208\n",
      "Ground truth salary: 0.117. Predicted salary: 0.088\n",
      "Ground truth salary: 0.313. Predicted salary: 0.251\n",
      "Ground truth salary: 0.364. Predicted salary: 0.251\n",
      "Ground truth salary: 0.330. Predicted salary: 0.208\n",
      "Ground truth salary: 0.330. Predicted salary: 0.208\n",
      "Ground truth salary: 0.318. Predicted salary: 0.208\n",
      "Ground truth salary: 0.211. Predicted salary: 0.208\n",
      "Ground truth salary: 0.127. Predicted salary: 0.176\n",
      "Ground truth salary: 0.096. Predicted salary: 0.185\n",
      "Ground truth salary: 0.163. Predicted salary: 0.102\n",
      "Ground truth salary: 0.161. Predicted salary: 0.208\n",
      "Ground truth salary: 0.236. Predicted salary: 0.208\n",
      "Ground truth salary: 0.230. Predicted salary: 0.153\n",
      "Ground truth salary: 0.272. Predicted salary: 0.251\n",
      "Ground truth salary: 0.263. Predicted salary: 0.251\n",
      "Ground truth salary: 0.263. Predicted salary: 0.208\n",
      "Ground truth salary: 0.389. Predicted salary: 0.185\n",
      "Ground truth salary: 0.036. Predicted salary: 0.185\n",
      "Ground truth salary: 0.539. Predicted salary: 0.121\n",
      "Ground truth salary: 0.133. Predicted salary: 0.110\n",
      "Ground truth salary: 0.011. Predicted salary: 0.242\n",
      "Ground truth salary: 0.188. Predicted salary: 0.208\n",
      "Ground truth salary: 0.134. Predicted salary: 0.230\n",
      "Ground truth salary: 0.159. Predicted salary: 0.219\n",
      "Ground truth salary: 0.248. Predicted salary: 0.153\n",
      "Ground truth salary: 0.160. Predicted salary: 0.219\n",
      "Ground truth salary: 0.190. Predicted salary: 0.208\n",
      "Ground truth salary: 0.272. Predicted salary: 0.185\n",
      "Ground truth salary: 0.102. Predicted salary: 0.207\n",
      "Ground truth salary: 0.087. Predicted salary: 0.185\n",
      "Ground truth salary: 0.263. Predicted salary: 0.176\n",
      "Ground truth salary: 0.106. Predicted salary: 0.176\n",
      "Ground truth salary: 0.272. Predicted salary: 0.185\n",
      "Ground truth salary: 0.062. Predicted salary: 0.153\n",
      "Ground truth salary: 0.116. Predicted salary: 0.088\n",
      "Ground truth salary: 0.165. Predicted salary: 0.208\n",
      "Ground truth salary: 0.017. Predicted salary: 0.242\n",
      "Ground truth salary: 0.105. Predicted salary: 0.219\n",
      "Ground truth salary: 0.059. Predicted salary: 0.153\n",
      "Ground truth salary: 0.179. Predicted salary: 0.242\n",
      "Ground truth salary: 0.043. Predicted salary: 0.124\n",
      "Ground truth salary: 0.009. Predicted salary: 0.175\n",
      "Ground truth salary: 0.189. Predicted salary: 0.176\n",
      "Ground truth salary: 0.302. Predicted salary: 0.219\n",
      "Ground truth salary: 0.029. Predicted salary: 0.154\n",
      "Ground truth salary: 0.297. Predicted salary: 0.153\n",
      "Ground truth salary: 0.171. Predicted salary: 0.208\n",
      "Ground truth salary: 0.072. Predicted salary: 0.198\n",
      "Ground truth salary: 0.195. Predicted salary: 0.176\n",
      "Ground truth salary: 0.246. Predicted salary: 0.229\n",
      "Ground truth salary: 0.207. Predicted salary: 0.208\n",
      "Ground truth salary: 0.156. Predicted salary: 0.207\n",
      "Ground truth salary: 0.169. Predicted salary: 0.185\n",
      "Ground truth salary: 0.325. Predicted salary: 0.142\n",
      "Ground truth salary: 0.230. Predicted salary: 0.208\n",
      "Ground truth salary: 0.632. Predicted salary: 0.185\n",
      "Ground truth salary: 0.015. Predicted salary: 0.103\n",
      "Ground truth salary: 0.024. Predicted salary: 0.198\n",
      "Ground truth salary: 0.122. Predicted salary: 0.153\n",
      "Ground truth salary: 0.330. Predicted salary: 0.143\n",
      "Ground truth salary: 0.032. Predicted salary: 0.088\n",
      "Ground truth salary: 0.356. Predicted salary: 0.185\n",
      "Ground truth salary: 0.107. Predicted salary: 0.154\n",
      "Ground truth salary: 0.089. Predicted salary: 0.175\n",
      "Ground truth salary: 0.050. Predicted salary: 0.176\n",
      "Ground truth salary: 0.022. Predicted salary: 0.103\n",
      "Ground truth salary: 0.060. Predicted salary: 0.176\n",
      "Ground truth salary: 0.188. Predicted salary: 0.197\n",
      "Ground truth salary: 0.749. Predicted salary: 0.153\n",
      "Ground truth salary: 0.203. Predicted salary: 0.208\n",
      "Ground truth salary: 0.049. Predicted salary: 0.173\n",
      "Ground truth salary: 0.119. Predicted salary: 0.175\n",
      "Ground truth salary: 0.400. Predicted salary: 0.143\n",
      "Ground truth salary: 0.146. Predicted salary: 0.208\n",
      "Ground truth salary: 0.073. Predicted salary: 0.153\n",
      "Average Loss: 0.0141\n"
     ]
    }
   ],
   "source": [
    "# Combining the codes\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "torch.manual_seed(1) # for 01\n",
    "\n",
    "salaries = pd.read_csv(\"datasets/ds_salaries.csv\")\n",
    "target_df = salaries[\"salary_in_usd\"]\n",
    "features_df = salaries[[\"experience_level\", \"employment_type\", \"remote_ratio\", \"company_size\"]]\n",
    "features_df.loc[:, \"company_size\"] = features_df[\"company_size\"].astype(\"category\").cat.codes\n",
    "features_df.loc[:, \"experience_level\"] = features_df[\"experience_level\"].astype(\"category\").cat.codes\n",
    "features_df.loc[:, \"employment_type\"] = features_df[\"employment_type\"].astype(\"category\").cat.codes\n",
    "features = features_df.to_numpy(dtype='float32')\n",
    "scaler = MinMaxScaler()\n",
    "target = scaler.fit_transform(target_df.values.reshape(-1, 1))\n",
    "target = target.astype(np.float32)\n",
    "\n",
    "# Create the dataset and the dataloader\n",
    "dataset = TensorDataset(torch.tensor(features), torch.tensor(target))\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = nn.Sequential(nn.Linear(4, 2),                       \n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "# Create the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loop over the number of epochs and the dataloader\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Get feature and target from the data loader\n",
    "        feature, target = data\n",
    "        # Run a forward pass\n",
    "        pred = model(feature)\n",
    "        # Compute loss and gradients\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192426b8-82fc-4946-bee3-d560135b1ac1",
   "metadata": {},
   "source": [
    "The inverse function of `scaler.fit_transform` in `scikit-learn` is `scaler.inverse_transform`, which scales the data back to the original representation. \n",
    "\n",
    "You can use this function to transform the predicted values into the original scale of the data. \n",
    "\n",
    "For example, if you have used StandardScaler to scale your data before fitting a model, you can use `StandardScaler.inverse_transform` to undo the scaling on the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ed473643-e90d-4bc5-b1e8-2e1a5a13e7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2061, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.preprocessing import StandardScaler.inverse_transform\n",
    "# StandardScaler.inverse_transform(pred)\n",
    "torch.mean(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901da45-8ab3-4c42-ae99-e3c37bde2c78",
   "metadata": {},
   "source": [
    "## 7. Activation functions between layers\n",
    "\n",
    "![](images/sigmoid_function.png)\n",
    "\n",
    "\n",
    "Some of the limitations of the sigmoid function. \n",
    "\n",
    "*  Bounded between 0 and 1.\n",
    "\n",
    "*  Can be used anywhere in the network.\n",
    "\n",
    "**Gradients**\n",
    "\n",
    "*  Approach zero for low and high values of $x$\n",
    "\n",
    "*  It will cause the function to saturate. \n",
    "\n",
    "* This can lead to vanishing gradients during backpropagation.\n",
    "\n",
    "\n",
    "**This is also a problem for softmax**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b698917-f50e-47b4-a73f-e8b72b843827",
   "metadata": {},
   "source": [
    "### 5.6.1 Introduction ReLU, Leaky ReLU and ELU\n",
    "\n",
    "![](images/relu.png)\n",
    "\n",
    "**ReLU**  Rectified Linear Unit \n",
    "\n",
    "*  $f(x) = \\max(x,0)$\n",
    "\n",
    "*  for positive inputs, the output is equal to \n",
    "the input\n",
    "\n",
    "*  for strictly negative inputs, the output is   equal to  zero\n",
    "\n",
    "*  it oovercomes the vanishing gradients problem\n",
    "\n",
    "*  however, it is not differentiable at $x=0$\n",
    "\n",
    "*  The problem of “dead ReLU” or “dying ReLU” refers to a situation in neural networks where certain neurons using the Rectified Linear Unit (ReLU) activation function become inactive during training and never recover.\n",
    "\n",
    "   *  Such neurons always output zero and do not contribute to the learning process\n",
    "\r",
    "*  In pytorch, `relu = nn.ReLU()`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](images/leaky_relu.png)\n",
    "\n",
    "\n",
    "**Leaky ReLU**\n",
    "\n",
    "* For positive inputs, it behaves similarly to ReLU\n",
    "\n",
    "* For negative inputs, it multiplies the input by a small coefficient (defaulted to 0.01)\n",
    "\n",
    "* The gradients for negative inputs are never null.\n",
    "\n",
    "* In PyTorch, `leaky_relu = nn.LeakyReLU(negative_slope = 0.05)`\n",
    "\n",
    "\n",
    "\n",
    "![](images/elu_and_its_derivative.png) \n",
    "\n",
    "**ELU**  Exponential Linear Uni\n",
    "\n",
    "*  $f(x) = \\begin{cases}x & \\text{if $x > 0$}\\\\\n",
    "                       \\alpha(e^x-1) & \\text{if $x \\leq 0$}\\end{cases}$\n",
    "\n",
    "*  Tend to converge faster than ReLU (because mean ELU activations are closer to zero)\n",
    "\n",
    "*  Fully continuous\n",
    "\n",
    "*  Fully differentiable\n",
    "\n",
    "*  Does not have a vanishing gradients problem\n",
    "\n",
    "*  Does not have an exploding gradients problem\n",
    "\n",
    "*  Does not have a dead relu problem.\n",
    "\n",
    "*  In pytorch,  `F.elu(input, alpha=1.0, inplace=False)` which applies the ELU function to the input tensor with `alpha=1.0` and not operating `in-place`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ff397130-a212-41a7-8dbb-48405308cf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(1.5, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a197fa7b-03a5-4736-b622-b5f5bcf0b202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2., requires_grad=True)\n",
      "tensor(-0.0200, grad_fn=<LeakyReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "# Apply your ReLU function on input, and calculate gradients\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "input = torch.tensor(-2.0, requires_grad=True)\n",
    "\n",
    "output = leaky_relu(input)\n",
    "output.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "radient = input.grad\n",
    "\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d151fdc1-3981-4647-9d36-78cfbbea884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5410, -0.2934, -2.1788,  0.5684])\n",
      "tensor([ 1.5410, -0.2543, -0.8868,  0.5684])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "input = torch.randn(4) # create a random input tensor of size 4\n",
    "output = F.elu(input, alpha=1.0, inplace=False) #\n",
    "\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f78527-5ab4-4865-bc9c-8bd9410c2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.7 Counting the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ff3b6654-aad4-4a45-819b-12572c32b7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "for parameter in model.parameters():\n",
    "  total += parameter.numel()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5d70d-5268-4ec0-ad09-291b7e4bb739",
   "metadata": {},
   "source": [
    "# Create a neural network with exactly four linear layers and more than 120 parameters\n",
    "\n",
    "which takes n_features as inputs and outputs n_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "dd8059c7-b128-41e9-a4fa-e89f1e9b1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_capacity(model):\n",
    "  total = 0\n",
    "  for p in model.parameters():\n",
    "    total += p.numel()\n",
    "  return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "11cb460e-d158-419c-b883-9f558993f195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with more than 120 parameters\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, 4),\n",
    "    nn.Linear(4, n_classes)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3459917-14da-4a4a-b19c-a55f221a8916",
   "metadata": {},
   "source": [
    "Using the TensorDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "0323886b-d08e-41b5-ac35-c0ac042d5c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.5978, 0.0183, 0.0100, 0.3260, 0.4076, 0.9300, 0.3318, 0.9228],\n",
      "       dtype=torch.float64), tensor([0.5949], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "np_features = np.array(np.random.rand(12, 8))\n",
    "np_target = np.array(np.random.rand(12, 1))\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "torch_features = torch.tensor(np_features)\n",
    "torch_target = torch.tensor(np_target)\n",
    "\n",
    "# Create a TensorDataset from two tensors\n",
    "dataset = TensorDataset(torch_features, torch_target)\n",
    "\n",
    "# Return the last element of this dataset\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882a85e-67bf-4d97-bfe1-f6dd4f6a3abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
