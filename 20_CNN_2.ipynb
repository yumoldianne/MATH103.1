{"cells":[{"source":"#  Convolutional Neural Networks in Text Classification with PyTorch\n\nThe text classification problem can be approached in a number of ways with respect to encoding the text data to numerical values.\n\n1. Text is modeled as the *frequency of occurrence of words* in a given text with respect of these words in the complete corpus. Example: `CountVectorizer()` and `TfidfVectorizer()` in `scikit-learn`.\n2. Text is modeled as the *sequence of words or characters*. This type of approach is used mainly by the **Recurrent Neural Networks** (**RNN**).\n3. Text is modeled as a *distribution of words in a given space*. This is achieved through the use of the **Convolutional Neural Network** architecture.","metadata":{"editable":true,"slideshow":{"slide_type":""},"tags":[]},"id":"755d97c1-0bdf-4086-a519-9c206b87a085","cell_type":"markdown"},{"source":"## What is a semantic space? \n\nSemantic spaces are representations of natural language that are capable of capturing meaning.\n\nReference: https://en.wikipedia.org/wiki/Semantic_space.\n\nA *semantic space* is a way of representing the meaning of words using vectors, matrices, or other mathematical structures. \n\nThe idea: \n\n**\"Words that are similar in meaning will have similar or close vectors in the semantic space, while words that are different or unrelated will have distant or orthogonal vectors\".**\n\n\nSlogan: **\"You shall know a word by the company it keeps\"** (J.R. Firth).\n\n\nFor example, \n\n*   `fire and dog` are two words unrelated in their meaning, and in fact they are not often used in the same sentence. \n*   On the other hand, the words `dog and cat` are sometimes seen together, so they may share some aspect of meaning.\n\nMathematically,\n\n![](images/cos.png)\n","metadata":{},"id":"42031b4d-6848-465e-b60f-587c69fc01fc","cell_type":"markdown"},{"source":"## A common architecture for CNN in text classification\n\n\n*   each word in a document is represented as an *embedding vector*, \n*   a single convolutional layer with m filters is applied, producing an m-dimensional vector for each document ngram.\n*  The vectors are combined using max-pooling followed by a ReLU activation.\n*  The result is then passed to a linear layer for the final classification.\n\n","metadata":{},"id":"628f3487-da35-4a66-a0e2-82bb156cd1fb","cell_type":"markdown"},{"source":"### Word embedding\n\nIt is a special way of creating features that group together similar words. Word embeddings would create similar features for various shades of blue. Word embeddings have another interesting property: they are mathematical representations of words that obey intuitive rules. For example, in word embeddings, if we take the features for \"King\", subtract the features for \"man\", and add the features for \"woman\", we get a set of features that are very close to those of \"queen\".\n\n\nword embeddings are a lookup table in PyTorch. They are a way of representing words as dense vectors of real numbers, one per word in your vocabulary. \n\nWe can use the `torch.nn.Embedding` class or the `torch.nn.functional.embedding` function to create and retrieve word embeddings using indices. \n\nThese modules take an input tensor of indices and return the corresponding embeddings from a weight matrix that is learned during training. You can also specify some optional parameters, such as padding index, `max_norm`, `norm_type`, `scale_grad_by_freq`, and `sparse`, to customize the behavior of the embedding layer. \n\nSources:  \n\n*   [Embedding — PyTorch 2.1 documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n*   [torch.nn.functional.embedding — PyTorch 2.1 documentation](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)\n\n*   [What \"exactly\" happens inside embedding layer in pytorch?](https://stackoverflow.com/questions/58718612/what-exactly-happens-inside-embedding-layer-in-pytorch)\n*   [What kind of word embedding is used in the original transformer?](https://ai.stackexchange.com/questions/26235/what-kind-of-word-embedding-is-used-in-the-original-transformer)                                                                                                                                                                                                                                                                               ","metadata":{},"id":"970c9100-4a52-4c73-aa5c-ff4bcbe8e6a0","cell_type":"markdown"},{"source":"### A CNN Architecture\n\n![](images/waakss1l.png)\n\n\n___\n\n","metadata":{},"id":"efc7cbc7-c7a7-49db-8c95-dfe223f560b1","cell_type":"markdown"},{"source":"## Text processing pipeline\n\n","metadata":{},"id":"6e871ba8-6654-45de-b2f5-9d15f792bdc8","cell_type":"markdown"},{"source":"### Step 1. Preprocessing text data\n\n![](images/text_processing_pipeline_preprocessing.png)","metadata":{},"id":"654a1697-577c-4ed7-b3e5-4315564acf21","cell_type":"markdown"},{"source":"import nltk\nimport numpy as np\nimport pandas as pd\nimport re \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\n\nclass Preprocessing:\n    \n    def __init__(self):\n    \n        self.data = 'datasets/tweets.csv'\n\n        self.X_raw = None\n        self.y = None\n        self.X_cleaned = None\n        self.X_tokenized = None\n        self.X_stopwords_removed = None\n        self.X_lemmatized = None\n        #self.vocabular = None\n        #self.word2idx = None\n        #self.vector_size = None\n        #self.X_encoded = None\n        #self.X_padded = None\n\n    def load_data(self):\n        # Reads the raw csv file and split into\n        # features (X) and target (y)\n        \n        df = pd.read_csv(self.data)\n        #df.drop(['id','keyword','location'], axis=1, inplace=True)\n        \n        self.X_raw = df['text'].values\n        self.y = df['target'].values\n\n    def clean_text(self):\n        # Removes special symbols and just keep\n        # words in lower or upper form\n        \n        self.X_cleaned = [x.lower() for x in self.X_raw]\n        self.X_cleaned = [re.sub(r'[^\\w\\s]', '', x) for x in self.X_cleaned]\n        \n    def text_tokenized(self):\n        # Tokenizes each sentence by implementing the nltk tool\n        \n        self.X_tokenized = [word_tokenize(x) for x in self.X_cleaned]\n\n    def text_stopwords_removed(self):\n        ## Create a list of stopwords\n        \n        stop_words = set(stopwords.words(\"english\"))\n        no_stopwords = []\n        \n        for tokens in self.X_tokenized:\n            tokens = [token for token in tokens if token not in stop_words]\n            no_stopwords.append(tokens)\n            \n        self.X_stopwords_removed = no_stopwords\n\n    def text_lemmatized(self):\n    \n        lemmatizer = WordNetLemmatizer()\n\n        text_lemmas = []\n        for tokens in self.X_stopwords_removed:\n            lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"n\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"a\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"r\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"s\") for word in lemmas]\n            text_lemmas.append(lemmas)\n        \n        self.X_lemmatized = text_lemmas\n        \n    #preprocessing.load_data()\n    #preprocessing.clean_text()\n    #preprocessing.text_tokenized()\n    #preprocessing.text_stopwords_removed()\n    #preprocessing.text_lemmatized()","metadata":{"executionCancelledAt":null,"executionTime":633,"lastExecutedAt":1700292070399,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import nltk\nimport numpy as np\nimport pandas as pd\nimport re \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\n\nclass Preprocessing:\n    \n    def __init__(self):\n    \n        self.data = 'datasets/tweets.csv'\n\n        self.X_raw = None\n        self.y = None\n        self.X_cleaned = None\n        self.X_tokenized = None\n        self.X_stopwords_removed = None\n        self.X_lemmatized = None\n        #self.vocabular = None\n        #self.word2idx = None\n        #self.vector_size = None\n        #self.X_encoded = None\n        #self.X_padded = None\n\n    def load_data(self):\n        # Reads the raw csv file and split into\n        # features (X) and target (y)\n        \n        df = pd.read_csv(self.data)\n        #df.drop(['id','keyword','location'], axis=1, inplace=True)\n        \n        self.X_raw = df['text'].values\n        self.y = df['target'].values\n\n    def clean_text(self):\n        # Removes special symbols and just keep\n        # words in lower or upper form\n        \n        self.X_cleaned = [x.lower() for x in self.X_raw]\n        self.X_cleaned = [re.sub(r'[^\\w\\s]', '', x) for x in self.X_cleaned]\n        \n    def text_tokenized(self):\n        # Tokenizes each sentence by implementing the nltk tool\n        \n        self.X_tokenized = [word_tokenize(x) for x in self.X_cleaned]\n\n    def text_stopwords_removed(self):\n        ## Create a list of stopwords\n        \n        stop_words = set(stopwords.words(\"english\"))\n        no_stopwords = []\n        \n        for tokens in self.X_tokenized:\n            tokens = [token for token in tokens if token not in stop_words]\n            no_stopwords.append(tokens)\n            \n        self.X_stopwords_removed = no_stopwords\n\n    def text_lemmatized(self):\n    \n        lemmatizer = WordNetLemmatizer()\n\n        text_lemmas = []\n        for tokens in self.X_stopwords_removed:\n            lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"n\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"a\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"r\") for word in lemmas]\n            lemmas = [lemmatizer.lemmatize(word, pos=\"s\") for word in lemmas]\n            text_lemmas.append(lemmas)\n        \n        self.X_lemmatized = text_lemmas\n        \n    #preprocessing.load_data()\n    #preprocessing.clean_text()\n    #preprocessing.text_tokenized()\n    #preprocessing.text_stopwords_removed()\n    #preprocessing.text_lemmatized()","outputsMetadata":{"0":{"height":173,"type":"stream"}}},"id":"0397df49-f226-4d7c-acef-b430629ef8b1","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/repl/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /home/repl/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}]},{"source":"## Step 2. Encoding the cleaned, tokenized, stopwords-removed and lemmatized text data\n\n![](images/text_processing_pipeline_encoding.png)","metadata":{},"cell_type":"markdown","id":"cdc710b5-5b38-4c19-91f4-f82119d09abc"},{"source":"import copy\n\nclass Encoding:\n    \n    def __init__(self, lemmatized_texts, num_words):\n        \n        self.X_lemmatized = lemmatized_texts\n        self.num_words = num_words\n        self.vector_size = None\n        self.fdist = None\n        self.X_encoded_texts = None\n        self.text4encoding = None\n        self.X_padded_codes = None\n    \n    def text_encoding(self):\n\n        vocabulary = dict()\n        fdist = nltk.FreqDist()  \n        \n        for tokens in self.X_lemmatized:  \n            for word in tokens:\n                fdist[word] += 1\n        \n        self.fdist = fdist\n        common_words = fdist.most_common(self.num_words)\n\n        for idx, word in enumerate(common_words):\n            vocabulary[word[0]] = (idx+1)\n        \n        self.vocabulary = vocabulary\n      \n        encoded_texts = list()\n        texts4encoding = list()\n        \n        for tokens in self.X_lemmatized:\n            temp_codes = list()\n            temp_words = list()\n            \n            for word in tokens:\n                if word in self.vocabulary.keys():\n                    temp_codes.append(self.vocabulary[word])\n                    temp_words.append(word)\n                             \n            encoded_texts.append(temp_codes)\n            texts4encoding.append(temp_words)\n\n        self.vector_size = np.max([len(x) for x in encoded_texts])\n        self.X_encoded_texts = encoded_texts\n        self.texts4encoding = texts4encoding\n  \n    def codes_padding(self):\n        pad_idx = 0\n        padded_codes = list()\n        \n        codes_from_texts = copy.deepcopy(self.X_encoded_texts)\n        for encoded_text in codes_from_texts:\n            while len(encoded_text) < self.vector_size:\n                encoded_text.append(pad_idx)\n            padded_codes.append(encoded_text)\n\n        self.X_padded_codes = np.array(padded_codes)\n","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1700296985309,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import copy\n\nclass Encoding:\n    \n    def __init__(self, lemmatized_texts, num_words):\n        \n        self.X_lemmatized = lemmatized_texts\n        self.num_words = num_words\n        self.vector_size = None\n        self.fdist = None\n        self.X_encoded_texts = None\n        self.text4encoding = None\n        self.X_padded_codes = None\n    \n    def text_encoding(self):\n\n        vocabulary = dict()\n        fdist = nltk.FreqDist()  \n        \n        for tokens in self.X_lemmatized:  \n            for word in tokens:\n                fdist[word] += 1\n        \n        self.fdist = fdist\n        common_words = fdist.most_common(self.num_words)\n\n        for idx, word in enumerate(common_words):\n            vocabulary[word[0]] = (idx+1)\n        \n        self.vocabulary = vocabulary\n      \n        encoded_texts = list()\n        texts4encoding = list()\n        \n        for tokens in self.X_lemmatized:\n            temp_codes = list()\n            temp_words = list()\n            \n            for word in tokens:\n                if word in self.vocabulary.keys():\n                    temp_codes.append(self.vocabulary[word])\n                    temp_words.append(word)\n                             \n            encoded_texts.append(temp_codes)\n            texts4encoding.append(temp_words)\n\n        self.vector_size = np.max([len(x) for x in encoded_texts])\n        self.X_encoded_texts = encoded_texts\n        self.texts4encoding = texts4encoding\n  \n    def codes_padding(self):\n        pad_idx = 0\n        padded_codes = list()\n        \n        codes_from_texts = copy.deepcopy(self.X_encoded_texts)\n        for encoded_text in codes_from_texts:\n            while len(encoded_text) < self.vector_size:\n                encoded_text.append(pad_idx)\n            padded_codes.append(encoded_text)\n\n        self.X_padded_codes = np.array(padded_codes)\n"},"cell_type":"code","id":"f4f64233-b2ac-474b-afd8-05c5f973fa12","execution_count":63,"outputs":[]},{"source":"## Step 3. Building Dataset and DataLoader \n![](images/text_processing_pipeline_dataset_dataloader.png)","metadata":{},"cell_type":"markdown","id":"720b1783-b784-4c9b-bc2e-689528cd36a9"},{"source":"from torch.utils.data import Dataset, DataLoader\n\nclass DatasetMapping(Dataset):\n\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n      \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1700296003158,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from torch.utils.data import Dataset, DataLoader\n\nclass DatasetMapping(Dataset):\n\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n      \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n"},"cell_type":"code","id":"4f0db97a-be34-443a-8e6a-e3f1669a762d","execution_count":45,"outputs":[]},{"source":"from sklearn.model_selection import train_test_split\n\nclass   DatasetLoading:\n    \n    def __init__(self, padded_codes, targets):\n        \n        self.X = padded_codes\n        self.y = targets\n        self.X_train = None\n        self.y_train = None\n        self.X_test = None\n        self.y_test = None\n        \n    def data_split(self):\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.20, random_state=20231116)    \n\n    def data_mapping(self):\n        \n        self.train = DatasetMapping(self.X_train, self.y_train)\n        self.test = DatasetMapping(self.X_test, self.y_test)\n\n    def data_loading(self):\n        self.loader_train = DataLoader(self.train, batch_size=params.batch_size)\n        self.loader_test = DataLoader(self.test, batch_size=params.batch_size)  ","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1700296072746,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.model_selection import train_test_split\n\nclass   DatasetLoading:\n    \n    def __init__(self, padded_codes, targets):\n        \n        self.X = padded_codes\n        self.y = targets\n        self.X_train = None\n        self.y_train = None\n        self.X_test = None\n        self.y_test = None\n        \n    def data_split(self):\n        \n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.20, random_state=20231116)    \n\n    def data_mapping(self):\n        \n        self.train = DatasetMapping(self.X_train, self.y_train)\n        self.test = DatasetMapping(self.X_test, self.y_test)\n\n    def data_loading(self):\n        self.loader_train = DataLoader(self.train, batch_size=params.batch_size)\n        self.loader_test = DataLoader(self.test, batch_size=params.batch_size)  "},"cell_type":"code","id":"2cbe4908-016f-4514-9e7b-ef54c17fd701","execution_count":46,"outputs":[]},{"source":"## Building, training and evaluating the cnn model\n\n![](images/text_classification_model.PNG)","metadata":{},"cell_type":"markdown","id":"fdd0b4a7-9587-43e6-a713-a76a53d99b89"},{"source":"### Step 4. Building the CNN model","metadata":{},"cell_type":"markdown","id":"3620cad2-d717-41be-b556-0e8f53f6b9e0"},{"source":"import torch\nimport torch.nn as nn\n\nclass TextClassificationCNN(nn.ModuleList):\n\n    def __init__(self, params):\n        super(TextClassificationCNN, self).__init__()\n        \n        # Parameters regarding text preprocessing\n        self.vector_size = params.vector_size\n        self.num_words = params.num_words\n        self.embedding_dim = params.embedding_dim\n      \n        # Dropout definition\n        self.dropout = nn.Dropout(params.dropout)\n       \n        # CNN parameters definition\n        # Kernel sizes\n        self.kernel_1 = 2\n        self.kernel_2 = 3\n        self.kernel_3 = 4\n        self.kernel_4 = 5\n      \n        # Output size for each convolution\n        self.out_size = params.out_size\n        # Number of strides for each convolution\n        self.stride = params.stride\n      \n        # Embedding layer definition\n        self.embedding = nn.Embedding(self.num_words + 1, self.embedding_dim, padding_idx=0)\n      \n        # Convolution layers definition\n        self.conv_1 = nn.Conv1d(self.vector_size, self.out_size, \n                                self.kernel_1, self.stride)\n        self.conv_2 = nn.Conv1d(self.vector_size, self.out_size, \n                                self.kernel_2, self.stride)\n        self.conv_3 = nn.Conv1d(self.vector_size, self.out_size, \n                                self.kernel_3, self.stride)\n        self.conv_4 = nn.Conv1d(self.vector_size, self.out_size, \n                                self.kernel_4, self.stride)\n      \n        \n        # Max pooling layers definition\n        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n\n        # Fully connected layer definition\n        #self.fc = nn.Linear(self.in_features_fc(), 1)\n        self.fc1 = nn.Linear(self.in_features_fc(), 64) \n        self.fc2 = nn.Linear(64, 1)  \n      \n    def in_features_fc(self):\n        '''Calculates the number of output features after Convolution + Max pooling\n        Convolved_Features = \n        ((embedding_dim + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n        Pooled_Features = \n        ((embedding_dim + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n      \n        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n        '''\n        # Calculate size of convolved/pooled features for convolution_1/max_pooling_1 features\n        out_conv_1 = ((self.embedding_dim - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n        out_conv_1 = math.floor(out_conv_1)\n        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n        out_pool_1 = math.floor(out_pool_1)\n      \n        # Calculate size of convolved/pooled features for convolution_2/max_pooling_2 features\n        out_conv_2 = ((self.embedding_dim - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n        out_conv_2 = math.floor(out_conv_2)\n        out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n        out_pool_2 = math.floor(out_pool_2)\n      \n        # Calculate size of convolved/pooled features for convolution_3/max_pooling_3 features\n        out_conv_3 = ((self.embedding_dim - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n        out_conv_3 = math.floor(out_conv_3)\n        out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n        out_pool_3 = math.floor(out_pool_3)\n      \n        # Calculate size of convolved/pooled features for convolution_4/max_pooling_4 features\n        out_conv_4 = ((self.embedding_dim - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n        out_conv_4 = math.floor(out_conv_4)\n        out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n        out_pool_4 = math.floor(out_pool_4)\n      \n        # Returns \"flattened\" vector (input for fully connected layer)\n        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n\n    def forward(self, x):\n\n        # Sequence of tokes is filterd through an embedding layer\n        x = self.embedding(x)\n      \n        # Convolution layer 1 is applied\n        x1 = self.conv_1(x)\n        x1 = torch.relu(x1)\n        x1 = self.pool_1(x1)\n      \n        # Convolution layer 2 is applied\n        x2 = self.conv_2(x)\n        x2 = torch.relu((x2))\n        x2 = self.pool_2(x2)\n   \n         # Convolution layer 3 is applied\n        x3 = self.conv_3(x)\n        x3 = torch.relu(x3)\n        x3 = self.pool_3(x3)\n      \n        # Convolution layer 4 is applied\n        x4 = self.conv_4(x)\n        x4 = torch.relu(x4)\n        x4 = self.pool_4(x4)\n      \n        # The output of each convolutional layer is concatenated into a unique vector\n        union = torch.cat((x1, x2, x3, x4), 2)\n        union = union.reshape(union.size(0), -1)\n\n        # The \"flattened\" vector is passed through a fully connected layer\n        #out = self.fc(union)\n        out1 = self.fc1(union)\n      \n        # Dropout is applied\t\t\n        out1 = self.dropout(out1)\n        # Activation function is applied\n        #out = torch.sigmoid(out)\n        out1 = torch.relu(out1)\n        \n        out2 = self.fc2(out1)\n        #out2 = self.dropout(out2)\n        out2 = torch.sigmoid(out2)\n      \n        return out2.squeeze()","metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1700306967865,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import torch\nimport torch.nn as nn\n\nclass TextClassificationCNN(nn.ModuleList):\n\n    def __init__(self, params):\n        super(TextClassificationCNN, self).__init__()\n        \n        # Parameters regarding text preprocessing\n        self.vector_size = params.vector_size\n        self.num_words = params.num_words\n        self.embedding_dim = params.embedding_dim\n      \n        # Dropout definition\n        self.dropout = nn.Dropout(params.dropout)\n       \n        # CNN parameters definition\n        # Kernel sizes\n        self.kernel_1 = 2\n        self.kernel_2 = 3\n        self.kernel_3 = 4\n        self.kernel_4 = 5\n      \n        # Output size for each convolution\n        self.out_size = params.out_size\n        # Number of strides for each convolution\n        self.stride = params.stride\n      \n        # Embedding layer definition\n        self.embedding = nn.Embedding(self.num_words + 1, self.embedding_dim, padding_idx=0)\n      \n        # Convolution layers definition\n        self.conv_1 = nn.Conv1d(self.vector_size, self.out_size, \n                                self.kernel_1, self.stride)\n        self.conv_2 = nn.Conv1d(self.vector_size, self.out_size, \n                                self.kernel_2, self.stride)\n        self.conv_3 = nn.Conv1d(self.vector_size, self.out_size, \n                                self.kernel_3, self.stride)\n        self.conv_4 = nn.Conv1d(self.vector_size, self.out_size, \n                                self.kernel_4, self.stride)\n      \n        \n        # Max pooling layers definition\n        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n\n        # Fully connected layer definition\n        #self.fc = nn.Linear(self.in_features_fc(), 1)\n        self.fc1 = nn.Linear(self.in_features_fc(), 64) \n        self.fc2 = nn.Linear(64, 1)  \n      \n    def in_features_fc(self):\n        '''Calculates the number of output features after Convolution + Max pooling\n        Convolved_Features = \n        ((embedding_dim + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n        Pooled_Features = \n        ((embedding_dim + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n      \n        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n        '''\n        # Calculate size of convolved/pooled features for convolution_1/max_pooling_1 features\n        out_conv_1 = ((self.embedding_dim - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n        out_conv_1 = math.floor(out_conv_1)\n        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n        out_pool_1 = math.floor(out_pool_1)\n      \n        # Calculate size of convolved/pooled features for convolution_2/max_pooling_2 features\n        out_conv_2 = ((self.embedding_dim - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n        out_conv_2 = math.floor(out_conv_2)\n        out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n        out_pool_2 = math.floor(out_pool_2)\n      \n        # Calculate size of convolved/pooled features for convolution_3/max_pooling_3 features\n        out_conv_3 = ((self.embedding_dim - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n        out_conv_3 = math.floor(out_conv_3)\n        out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n        out_pool_3 = math.floor(out_pool_3)\n      \n        # Calculate size of convolved/pooled features for convolution_4/max_pooling_4 features\n        out_conv_4 = ((self.embedding_dim - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n        out_conv_4 = math.floor(out_conv_4)\n        out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n        out_pool_4 = math.floor(out_pool_4)\n      \n        # Returns \"flattened\" vector (input for fully connected layer)\n        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n\n    def forward(self, x):\n\n        # Sequence of tokes is filterd through an embedding layer\n        x = self.embedding(x)\n      \n        # Convolution layer 1 is applied\n        x1 = self.conv_1(x)\n        x1 = torch.relu(x1)\n        x1 = self.pool_1(x1)\n      \n        # Convolution layer 2 is applied\n        x2 = self.conv_2(x)\n        x2 = torch.relu((x2))\n        x2 = self.pool_2(x2)\n   \n         # Convolution layer 3 is applied\n        x3 = self.conv_3(x)\n        x3 = torch.relu(x3)\n        x3 = self.pool_3(x3)\n      \n        # Convolution layer 4 is applied\n        x4 = self.conv_4(x)\n        x4 = torch.relu(x4)\n        x4 = self.pool_4(x4)\n      \n        # The output of each convolutional layer is concatenated into a unique vector\n        union = torch.cat((x1, x2, x3, x4), 2)\n        union = union.reshape(union.size(0), -1)\n\n        # The \"flattened\" vector is passed through a fully connected layer\n        #out = self.fc(union)\n        out1 = self.fc1(union)\n      \n        # Dropout is applied\t\t\n        out1 = self.dropout(out1)\n        # Activation function is applied\n        #out = torch.sigmoid(out)\n        out1 = torch.relu(out1)\n        \n        out2 = self.fc2(out1)\n        #out2 = self.dropout(out2)\n        out2 = torch.sigmoid(out2)\n      \n        return out2.squeeze()"},"cell_type":"code","id":"f221e701-673c-40cc-9a95-47144cd48b3d","execution_count":179,"outputs":[]},{"source":"from dataclasses import dataclass\n\n@dataclass\nclass Parameters:\n    # Preprocessing parameters\n    vector_size: int = 25   # standard length of each row vector in the input\n    num_words: int = 9300  # number of words in the vocabulary\n    test_size = 0.20         \n    random_state = 42\n   \n    # Model parameters\n    embedding_dim: int = 256\n    out_size: int = 32\n    stride: int = 2\n    #dilation: int = 2\n   \n    \n    # Training parameters\n    epochs: int = 50\n    batch_size: int = 128\n    learning_rate: float = 0.001\n    dropout: float = 0.05\n    \nparams=Parameters()","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1700306853454,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from dataclasses import dataclass\n\n@dataclass\nclass Parameters:\n    # Preprocessing parameters\n    vector_size: int = 25   # standard length of each row vector in the input\n    num_words: int = 9300  # number of words in the vocabulary\n    test_size = 0.20         \n    random_state = 42\n   \n    # Model parameters\n    embedding_dim: int = 256\n    out_size: int = 32\n    stride: int = 2\n    #dilation: int = 2\n   \n    \n    # Training parameters\n    epochs: int = 50\n    batch_size: int = 128\n    learning_rate: float = 0.001\n    dropout: float = 0.05\n    \nparams=Parameters()"},"cell_type":"code","id":"9ee1e20c-9c20-4318-b6fb-f55efe7b7c06","execution_count":177,"outputs":[]},{"source":"### Step 1. Preprocessing\ndata = Preprocessing()\ndata.load_data()\ndata.clean_text()\ndata.text_tokenized()\ndata.text_stopwords_removed()\ndata.text_lemmatized()\n\n### Step 2. Encoding\ncode = Encoding(data.X_lemmatized, params.num_words)\ncode.text_encoding()\ncode.codes_padding()\n\n### Step 3. Dataset and DataLoader\ndsl = DatasetLoading(code.X_padded_codes, data.y)\ndsl.data_split()\ndsl.data_mapping()\ndsl.data_loading()","metadata":{"executionCancelledAt":null,"executionTime":2100,"lastExecutedAt":1700304389910,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"### Step 1. Preprocessing\ndata = Preprocessing()\ndata.load_data()\ndata.clean_text()\ndata.text_tokenized()\ndata.text_stopwords_removed()\ndata.text_lemmatized()\n\n### Step 2. Encoding\ncode = Encoding(data.X_lemmatized, params.num_words)\ncode.text_encoding()\ncode.codes_padding()\n\n### Step 3. Dataset and DataLoader\ndsl = DatasetLoading(code.X_padded_codes, data.y)\ndsl.data_split()\ndsl.data_mapping()\ndsl.data_loading()"},"cell_type":"code","id":"0bb08db5-6b92-48ed-af43-70ba7246b0df","execution_count":135,"outputs":[]},{"source":"print(f\"lemmatized row max length = {max([len(x) for x in code.X_lemmatized])}\")\nprint(f\"row max length = {max([len(x) for x in code.texts4encoding])}\")\nprint(f\"vocab length = {len(code.vocabulary)}\")\nprint(f\"unique words length = {len(code.fdist)}\\n\")","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"cell_type":"code","id":"bfe695d9-251c-4051-ac35-33c2050bde16","execution_count":181,"outputs":[{"output_type":"stream","name":"stdout","text":"lemmatized row max length = 25\nrow max length = 25\nvocab length = 9300\nunique words length = 19999\n\n"}]},{"source":"### Step 4. Train and evaluate the CNN model","metadata":{},"cell_type":"markdown","id":"de8ad684-943b-4638-b3d2-dc0b21fbca8a"},{"source":"import math\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nloader_train = dsl.loader_train\nloader_test = dsl.loader_test\ny_train = dsl.y_train\ny_test = dsl.y_test\n\n\ncnn_model = TextClassificationCNN(params)\noptimizer = optim.RMSprop(cnn_model.parameters(), lr= params.learning_rate)\n\n# Starts training phase\nfor epoch in range(params.epochs):\n    \n    # Set model in training model\n    cnn_model.train()\n    train_predictions = []\n    \n     # Starts batch training\n    for x_batch, y_batch in loader_train:\n\n        y_batch = y_batch.type(torch.FloatTensor)\n            \n        # Feed the model\n        y_pred = cnn_model(x_batch)\n         \n        # Loss calculation\n        loss = F.binary_cross_entropy(y_pred, y_batch)\n         \n        # Clean gradientes\n        optimizer.zero_grad()\n         \n        # Gradients calculation\n        loss.backward()\n         \n        # Gradients update\n        optimizer.step()\n         \n        # Save predictions\n        train_predictions += list(y_pred.detach().numpy())\n        \n    # Metrics calculation for train accuracy\n     \n    true_positives = 0\n    true_negatives = 0\n    \n    for true, pred in zip(y_train, train_predictions):\n        if (pred >= 0.5) and (true == 1):\n            true_positives += 1\n        elif (pred < 0.5) and (true == 0):\n            true_negatives += 1\n        else:\n        \tpass\n    train_accuracy = (true_positives + true_negatives) / len(y_train)\n    \n    # Metrics calculation for test accuracy\n    \n    # Set the model in evaluation mode\n    cnn_model.eval()\n    test_predictions = []\n    \n    # Start evaluation phase\n    with torch.no_grad():\n        for x_batch, y_batch in loader_test:\n            y_pred = cnn_model(x_batch)\n            test_predictions += list(y_pred.detach().numpy())\n    \n    true_positives = 0\n    true_negatives = 0\n    \n    for true, pred in zip(y_test, test_predictions):\n        if (pred >= 0.5) and (true == 1):\n            true_positives += 1\n        elif (pred < 0.5) and (true == 0):\n            true_negatives += 1\n        else:\n        \tpass\n    test_accuracy = (true_positives + true_negatives) / len(y_test)\n     \n    \n    \n    print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuracy, test_accuracy))","metadata":{"executionCancelledAt":null,"executionTime":61407,"lastExecutedAt":1700307036128,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import math\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nloader_train = dsl.loader_train\nloader_test = dsl.loader_test\ny_train = dsl.y_train\ny_test = dsl.y_test\n\n\ncnn_model = TextClassificationCNN(params)\noptimizer = optim.RMSprop(cnn_model.parameters(), lr= params.learning_rate)\n\n# Starts training phase\nfor epoch in range(params.epochs):\n    \n    # Set model in training model\n    cnn_model.train()\n    train_predictions = []\n    \n     # Starts batch training\n    for x_batch, y_batch in loader_train:\n\n        y_batch = y_batch.type(torch.FloatTensor)\n            \n        # Feed the model\n        y_pred = cnn_model(x_batch)\n         \n        # Loss calculation\n        loss = F.binary_cross_entropy(y_pred, y_batch)\n         \n        # Clean gradientes\n        optimizer.zero_grad()\n         \n        # Gradients calculation\n        loss.backward()\n         \n        # Gradients update\n        optimizer.step()\n         \n        # Save predictions\n        train_predictions += list(y_pred.detach().numpy())\n        \n    # Metrics calculation for train accuracy\n     \n    true_positives = 0\n    true_negatives = 0\n    \n    for true, pred in zip(y_train, train_predictions):\n        if (pred >= 0.5) and (true == 1):\n            true_positives += 1\n        elif (pred < 0.5) and (true == 0):\n            true_negatives += 1\n        else:\n        \tpass\n    train_accuracy = (true_positives + true_negatives) / len(y_train)\n    \n    # Metrics calculation for test accuracy\n    \n    # Set the model in evaluation mode\n    cnn_model.eval()\n    test_predictions = []\n    \n    # Start evaluation phase\n    with torch.no_grad():\n        for x_batch, y_batch in loader_test:\n            y_pred = cnn_model(x_batch)\n            test_predictions += list(y_pred.detach().numpy())\n    \n    true_positives = 0\n    true_negatives = 0\n    \n    for true, pred in zip(y_test, test_predictions):\n        if (pred >= 0.5) and (true == 1):\n            true_positives += 1\n        elif (pred < 0.5) and (true == 0):\n            true_negatives += 1\n        else:\n        \tpass\n    test_accuracy = (true_positives + true_negatives) / len(y_test)\n     \n    \n    \n    print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuracy, test_accuracy))","outputsMetadata":{"0":{"height":565,"type":"stream"}}},"cell_type":"code","id":"513850d1-2855-427b-9581-eeba021a8d4f","execution_count":180,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 1, loss: 0.67441, Train accuracy: 0.54204, Test accuracy: 0.58634\nEpoch: 2, loss: 0.62419, Train accuracy: 0.59672, Test accuracy: 0.61589\nEpoch: 3, loss: 0.50018, Train accuracy: 0.70082, Test accuracy: 0.74064\nEpoch: 4, loss: 0.40677, Train accuracy: 0.79343, Test accuracy: 0.77150\nEpoch: 5, loss: 0.35273, Train accuracy: 0.84483, Test accuracy: 0.77610\nEpoch: 6, loss: 0.29666, Train accuracy: 0.87816, Test accuracy: 0.78398\nEpoch: 7, loss: 0.23659, Train accuracy: 0.89672, Test accuracy: 0.75968\nEpoch: 8, loss: 0.23768, Train accuracy: 0.91412, Test accuracy: 0.79383\nEpoch: 9, loss: 0.18433, Train accuracy: 0.92381, Test accuracy: 0.73342\nEpoch: 10, loss: 0.13944, Train accuracy: 0.94236, Test accuracy: 0.78923\nEpoch: 11, loss: 0.14775, Train accuracy: 0.95649, Test accuracy: 0.69337\nEpoch: 12, loss: 0.11419, Train accuracy: 0.95961, Test accuracy: 0.73014\nEpoch: 13, loss: 0.11690, Train accuracy: 0.95731, Test accuracy: 0.78398\nEpoch: 14, loss: 0.10645, Train accuracy: 0.96617, Test accuracy: 0.78004\nEpoch: 15, loss: 0.10984, Train accuracy: 0.97159, Test accuracy: 0.78595\nEpoch: 16, loss: 0.08637, Train accuracy: 0.97471, Test accuracy: 0.75312\nEpoch: 17, loss: 0.08128, Train accuracy: 0.97586, Test accuracy: 0.78135\nEpoch: 18, loss: 0.06761, Train accuracy: 0.97373, Test accuracy: 0.76888\nEpoch: 19, loss: 0.06841, Train accuracy: 0.97504, Test accuracy: 0.74721\nEpoch: 20, loss: 0.05469, Train accuracy: 0.97652, Test accuracy: 0.76494\nEpoch: 21, loss: 0.04918, Train accuracy: 0.97915, Test accuracy: 0.77216\nEpoch: 22, loss: 0.05197, Train accuracy: 0.97800, Test accuracy: 0.75968\nEpoch: 23, loss: 0.04240, Train accuracy: 0.98046, Test accuracy: 0.75246\nEpoch: 24, loss: 0.03908, Train accuracy: 0.97061, Test accuracy: 0.76231\nEpoch: 25, loss: 0.04748, Train accuracy: 0.98210, Test accuracy: 0.76100\nEpoch: 26, loss: 0.05407, Train accuracy: 0.98194, Test accuracy: 0.76297\nEpoch: 27, loss: 0.04566, Train accuracy: 0.98210, Test accuracy: 0.77413\nEpoch: 28, loss: 0.04250, Train accuracy: 0.98243, Test accuracy: 0.76625\nEpoch: 29, loss: 0.04323, Train accuracy: 0.98259, Test accuracy: 0.77282\nEpoch: 30, loss: 0.04425, Train accuracy: 0.98276, Test accuracy: 0.76297\nEpoch: 31, loss: 0.07309, Train accuracy: 0.98391, Test accuracy: 0.74196\nEpoch: 32, loss: 0.04219, Train accuracy: 0.98309, Test accuracy: 0.78201\nEpoch: 33, loss: 0.04056, Train accuracy: 0.98407, Test accuracy: 0.75246\nEpoch: 34, loss: 0.05196, Train accuracy: 0.98292, Test accuracy: 0.77938\nEpoch: 35, loss: 0.03541, Train accuracy: 0.98391, Test accuracy: 0.78267\nEpoch: 36, loss: 0.04081, Train accuracy: 0.98358, Test accuracy: 0.78398\nEpoch: 37, loss: 0.04161, Train accuracy: 0.98161, Test accuracy: 0.78135\nEpoch: 38, loss: 0.06607, Train accuracy: 0.98374, Test accuracy: 0.75443\nEpoch: 39, loss: 0.04562, Train accuracy: 0.98522, Test accuracy: 0.78595\nEpoch: 40, loss: 0.04035, Train accuracy: 0.98424, Test accuracy: 0.77741\nEpoch: 41, loss: 0.03742, Train accuracy: 0.98407, Test accuracy: 0.77873\nEpoch: 42, loss: 0.04507, Train accuracy: 0.98358, Test accuracy: 0.75181\nEpoch: 43, loss: 0.04346, Train accuracy: 0.98424, Test accuracy: 0.77741\nEpoch: 44, loss: 0.04018, Train accuracy: 0.98342, Test accuracy: 0.77479\nEpoch: 45, loss: 0.03794, Train accuracy: 0.98456, Test accuracy: 0.79383\nEpoch: 46, loss: 0.03440, Train accuracy: 0.98473, Test accuracy: 0.79317\nEpoch: 47, loss: 0.03687, Train accuracy: 0.98440, Test accuracy: 0.79251\nEpoch: 48, loss: 0.05546, Train accuracy: 0.98342, Test accuracy: 0.67039\nEpoch: 49, loss: 0.03463, Train accuracy: 0.98440, Test accuracy: 0.79186\nEpoch: 50, loss: 0.04181, Train accuracy: 0.98473, Test accuracy: 0.78661\n"}]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}